{"meta":{"title":"iVEGA","subtitle":"一个小确幸的地方","description":null,"author":"不负韶华不负己","url":"http://yoursite.com"},"pages":[{"title":"自我介绍","date":"2018-06-22T03:15:14.000Z","updated":"2018-06-22T03:15:14.000Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""},{"title":"分类目录","date":"2018-06-22T03:14:52.000Z","updated":"2018-06-22T03:14:52.000Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"标签种类","date":"2018-06-22T03:14:28.000Z","updated":"2018-06-22T03:14:28.000Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"版本比对之映射本体(二)","slug":"版本比对(二)","date":"2019-05-02T14:14:35.000Z","updated":"2019-05-04T16:17:05.780Z","comments":true,"path":"2019/05/02/版本比对(二)/","link":"","permalink":"http://yoursite.com/2019/05/02/版本比对(二)/","excerpt":"The article has been encrypted, please enter your password to view.","text":"输入密码，查看文章 Incorrect Password! No content to display! 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","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"实习","slug":"NLP/实习","permalink":"http://yoursite.com/categories/NLP/实习/"}],"tags":[{"name":"版本比对","slug":"版本比对","permalink":"http://yoursite.com/tags/版本比对/"},{"name":"本体映射","slug":"本体映射","permalink":"http://yoursite.com/tags/本体映射/"}]},{"title":"版本比对之功能实现(一)","slug":"版本比对(一)","date":"2019-05-02T10:26:16.000Z","updated":"2019-05-02T15:50:56.710Z","comments":true,"path":"2019/05/02/版本比对(一)/","link":"","permalink":"http://yoursite.com/2019/05/02/版本比对(一)/","excerpt":"这个版本比对,是我实习第一个任务,一番下来,能力也提高了不少,但还有很大的进步空间,顺便记录下自己提高的过程,当你全身心投入写代码的时候,发现还是比较有意思的.","text":"这个版本比对,是我实习第一个任务,一番下来,能力也提高了不少,但还有很大的进步空间,顺便记录下自己提高的过程,当你全身心投入写代码的时候,发现还是比较有意思的. 一.主要思想 二.主程序 三.参数file_content文本获取 四.子程序 4.1 获取改动部分 4.2 合并操作 4.2.1 合并列表相邻的操作 4.2.2 合并各个文本的操作 4.3 统一存放修改部分 4.4 修改部分排序 一.主要思想 版本比对主要步骤如下: 读取两个docx文件:使用DivideDoc方法初步读取全文信息,然后经过整理,得到全文的字符串; 找出不同部分:使用diffOnText(text)读取,操作方式总共有三种,删除,操作,不变; 进行两个层面的操作: 修改列表diffs的相邻元素合并–&gt;mergeContinueChangeSimple(diffs); 不同文本的修改部分合并–&gt;mergeContinueChange(diffs). 统一存放修改记录:这时候会添加绝对位置–&gt;findChangeLog(diffs); 基于绝对位置的修改记录排序:防止章节互换出现的问题—&gt;mergelog(change_log_l);二.主程序 1234567891011121314151617181920212223242526272829# 两个文本比对的主函数def compare_file_check(file1_content, file2_content): \"\"\" 主要从chapter_divide.DivideDoc类中的DivideDoc方法和getContent方法得到参数内容 :param file1_content: :param file2_content: :return: \"\"\" # 找出两个不同,有三种情况:0,-1,1 #(0, '全局信息湖南互联网稿件统项目技服务合合同编号...') # (-1, '建行北京润德支行营业部'), # (0, '开户账号：'), # (-1, '11001093901052506732'), diff = diffOnText(file1_content, file2_content) print(\"打印diff\") print(diff) # 对修改部分进行合并 if len(diff) &gt;= 2: #单单只是合并连续相同操作的的修改列表 diff = mergeContinueChangeSimple(diff) if len(diff) &gt;= 2: # 进一步合并各个文本的修改 diff = mergeContinueChange(diff) # 把所有的修改都记录在一个文件上,用text2_change_log保存 text2_change_log = findChangeLog(diff) # 把修改的记录按照绝对位置进行排序,防止章节互换后,位置有变化 text2_change_log=mergelog(text2_change_log) # 返回全部的修改 return text2_change_log 三.参数file_content文本获取通过getContent对初步的全文信息进行加工,让每部分都有对应的标题信息.12345678910111213141516def getContent(document_path,document_name): dv = reviewer.chapter_divide.DivideDoc(document_path + document_name) global_text = dv.getGlobalText() # 为全局信息添加标题--\"全局信息\" global_text.insert(0, \"全局信息\") # 获取标题及其内容 first_content = dv.getFirstContent() # 获取A文件合同的附件 first_appendix = dv.getAppendix() # 为合同的附件添加标题--\"附件\" first_appendix.insert(0, \"附件\") # 将A文件全文信息进行连接，以列表形式进行存储（二维列表） content_l = [] content_l.append(global_text) content_l.extend(first_content) content_l.append(first_appendix) 通过在class DivideDoc中的DivideDoc这个方法得到全文的初步信息.:1234567891011121314151617181920212223242526class DivideDoc:\"\"\" first_sencond_content:包含一级和二级标题的内容(有空行)，每个元素被封装为Paragraph类，供后续要素抽取使用 first_content:只包含一级标题（有空行），列表的第一个元素的一级标题 global_text:全局信息(有空行) appendix：附件信息\"\"\" def __init__(self,filename): self.filename=filename self.first_sencond_content, self.global_text, self.first_content,self.appendix=self.DivideDoc(self.filename) self.paragraphs=self.getParagraph(self.first_sencond_content) def DivideDoc(self,file_path): \"\"\" 对文档进行一级和二级标题解析 :return: \"\"\" absult_path = os.path.abspath(file_path) # print(absult_path) if absult_path.split(\".\")[-1] == \"doc\": self.transeDoc(absult_path) file_path = file_path.replace(\"doc\", \"docx\") all_info, doc_chapters,global_text,appendix,flag = self.divideDocByStyle(file_path) if not flag: doc_chapters, head2_pattern, flag, global_text,appendix = self.divideDocByRegular(file_path) all_info = self.divideHead2_2(doc_chapters, head2_pattern, flag) return all_info,global_text,doc_chapters,appendix 四.子程序4.1 获取改动部分1234567def diffOnText(strA, strB): dmp = dmp_module.diff_match_patch() dmp.Diff_Timeout = 0.0 A = strA B = strB diffs_ = dmp.diff_main(A, B) return diffs_ 4.2 合并操作4.2.1 合并列表相邻的操作12345678910111213141516171819202122232425262728def mergeContinueChangeSimple(diffs): \"\"\" :param diffs: 合并连续相同的操作:[-1,'的'],[-1,'问哦']--&gt;[-1,'的问哦'] :return: \"\"\" diffs_merge=[] # 第一个操作 pre_operation = diffs[0][0] # 修改的内容 pre_change_word = diffs[0][1] # 从第二个操作开始遍历,作为当前的操作 for x in range(1, len(diffs)): # 当前的操作 now_operation = diffs[x][0] # 如果当前操作与前一个操作相同,操作方式(本来相同,就没必要合并),内容进行合并 # 之后在此进行遍历 if now_operation == pre_operation: pre_change_word += diffs[x][1] else: # 若操作方式不同,则将前一个操作方式及其内容添加到diffs_merge这个新的操作列表当中 diffs_merge.append([pre_operation,pre_change_word]) # 使得当前操作更新为前一个操作(操作方式及其内容),之后在此进行遍历 pre_operation = now_operation pre_change_word = diffs[x][1] # 当遍历完成后,把前一个操作添加到合并列表当中 # (因为在循环中,都让当前的操作成为前一个操作) diffs_merge.append([pre_operation, pre_change_word]) return diffs_merge 4.2.2 合并各个文本的操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566def mergeContinueChange(diffs): \"\"\" 合并各个文本各自的操作 [0, '程序时，],[1, '允许'], [-1, '同意'], [1, '时'][0, '，乙方不'] ---&gt; [0, '程序时，],[1, '允许时'], [-1, '同意'],[0, '，乙方不'] :param diffs: 合并不同的修改 :return: \"\"\" # Construct the two texts which made up the diff originally. diffs_merge=[] text1 = \"\" text2 = \"\" delete_index = -1 insert_index = -1 # 对各个操作进行合并,对各个文本长度进行更新(重点) for x in range(0, len(diffs)): # 1.对各个操作进行合并,先扫描diffs中各个元素 # 1.1 对插入操作进行合并 if diffs[x][0] == dmp_module.diff_match_patch.DIFF_INSERT: lenth2 = len(text2) # [0]是操作方式的代码,[1]是操作内容 if len(diffs_merge) &gt;= 2 and diffs_merge[-2][0]== 1: # 文本2当前的位置+插入的内容=文本2的现有长度 # 1859 + 2 =1861 if insert_index + len(diffs_merge[-2][1]) == lenth2: # 允许+时 # diffs[x][0]已经是插入,因此diffs[x][1]时插入的内容 diffs_merge[-2][1] = diffs_merge[-2][1] + diffs[x][1] print(diffs_merge[-2][1]) else: diffs_merge.append([diffs[x][0],diffs[x][1]]) insert_index = lenth2 else: diffs_merge.append([diffs[x][0], diffs[x][1]]) insert_index = lenth2 # 1.2 对删除操作进行合并 if diffs[x][0] == dmp_module.diff_match_patch.DIFF_DELETE: lenth1 = len(text1) if len(diffs_merge) &gt;= 2 and diffs_merge[-2][0]== -1: # 文本1当前的位置+删除的文本=文本1的总长度 if delete_index + len(diffs_merge[-2][1]) == lenth1: diffs_merge[-2][1] = diffs_merge[-2][1]+diffs[x][1] else: diffs_merge.append([diffs[x][0],diffs[x][1]]) delete_index = lenth1 else: diffs_merge.append([diffs[x][0], diffs[x][1]]) delete_index = lenth1 # 1.3对不变操作直接添加即可 if diffs[x][0] == 0: diffs_merge.append([diffs[x][0], diffs[x][1]]) # Diff Match and Patch--dmp # 2.更新text1,text2长度 # 2.1 更新text1长度 if diffs[x][0] != dmp_module.diff_match_patch.DIFF_INSERT: # 其实dmp_module.diff_match_patch.DIFF_INSERT就是一个常量-&gt;1, # 弄成这样只是为了便于理解 text1 += diffs[x][1] # 2.2 更新text2长度: if diffs[x][0] != dmp_module.diff_match_patch.DIFF_DELETE: text2 += diffs[x][1] print(diffs) print(\"打印的diffs_merge\") print(diffs_merge) return diffs_merge 4.3 统一存放修改部分12345678910111213141516171819202122232425def findChangeLog(diffs): \"\"\" 将所有合并后的修改记录放在一个变量当中,方便保存在一个文件中 :param diffs: :return: \"\"\" text2 = \"\" text2_change_log = [] # 遍历各个修改部分 for x in range(0, len(diffs)): lenth2 = len(text2) # 1.记录操作 # 将文本1的删除操作记录在同一个变量当中.位置是文本2的对应的位置 if diffs[x][0] == dmp_module.diff_match_patch.DIFF_DELETE: text2_change_log.append([lenth2,-1,diffs[x][1]]) # 文本2的插入操作记录 if diffs[x][0] == dmp_module.diff_match_patch.DIFF_INSERT: text2_change_log.append([lenth2,1,diffs[x][1]]) # 2.只需更新文本B的长度 # 文本B的长度+=插入操作对应内容的长度 if diffs[x][0] != dmp_module.diff_match_patch.DIFF_DELETE: text2 += diffs[x][1] # 基于绝对位置进行排序 text2_change_log.sort(key=operator.itemgetter(0)) return text2_change_log 4.4 修改部分排序对修改部分排序,防止章节互换出现的问题12345678910111213141516171819202122232425262728293031323334def mergelog(change_log_l): \"\"\" :param: :rtype: object \"\"\" change_dict=&#123;&#125; merge_chang_l=[] midd_result=[] # [[4, -1, '新'], [11, -1, '抓取系']] # 1.遍历修改记录列表 for i in range(len(change_log_l)): # 把绝对位置和操作作为key,避免相同位置出现覆盖的情况 key=str(change_log_l[i][0])+\"$$\"+str(change_log_l[i][1]) # 2.基于绝对位置和操作内容作为key新建字典 # 设置好key,构建字典,操作内容当做value if key in change_dict: # key存在,添加对应的value change_dict[key]+=change_log_l[i][2] else: # key不存在就当场新建键值对,k-v change_dict[key]=change_log_l[i][2] # 3.之后对字典解析,得到新的合并列表 for i,value in change_dict.items(): midd_result=[] midd_result.append(int(i.split('$$')[0])) midd_result.append(int(i.split('$$')[1])) midd_result.append(value) merge_chang_l.append(midd_result) sorted(merge_chang_l,key=itemgetter(0)) print(\"change_log_l:\") print(change_log_l) print(merge_chang_l) return merge_chang_l 结果:","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"实习","slug":"NLP/实习","permalink":"http://yoursite.com/categories/NLP/实习/"}],"tags":[{"name":"版本比对","slug":"版本比对","permalink":"http://yoursite.com/tags/版本比对/"}]},{"title":"tf创建变量及其范围","slug":"tf创建变量及其范围","date":"2019-03-28T15:42:14.000Z","updated":"2019-03-28T15:50:01.000Z","comments":true,"path":"2019/03/28/tf创建变量及其范围/","link":"","permalink":"http://yoursite.com/2019/03/28/tf创建变量及其范围/","excerpt":"本文是实习期间第三篇，以后写博客，不一定每天都要写，但要做好总结，当完成一定工作的时候就可以找个晚上写博客，平常多问，其实很感谢我的师兄和师姐，技术功底很不错。本文就是对tensorflow的一些用法的总结。","text":"本文是实习期间第三篇，以后写博客，不一定每天都要写，但要做好总结，当完成一定工作的时候就可以找个晚上写博客，平常多问，其实很感谢我的师兄和师姐，技术功底很不错。本文就是对tensorflow的一些用法的总结。 一、tf.placeholder 与 tf.Variable区别 二、tf.Varivale与tf.get_variable 三、tf.get_variable与tf.variable_scope 四、参考文献 一、tf.placeholder 与 tf.Variable区别 tf.placeholder更多的时候是对于便于数据的输入；–因此在初始化的时候，可以不用赋初值，需要赋值的时候是在feed_dict方法当中 tf.Variable更多的是模型参数的存储； 二、tf.Varivale与tf.get_variable 变量可以通过tf.Varivale与tf.get_variable来创建。当tf.get_variable用于变量创建时，和tf.Variable的功能基本等价． tf.get_varialbe和tf.Variable最大的区别在于：tf.Variable的变量名是一个可选项，通过name=’v’的形式给出。但是tf.get_variable必须指定变量名，也就是name值12v = tf.get_variable('v', shape=[1], initializer=tf.constant_initializer(1.0))v = tf.Variable(tf.constant(1.0, shape=[1], name='v' 三、tf.get_variable与tf.variable_scope tf.get_variable更多用来获取共享变量；通常配合tf.variable_scope(&#39;foo&#39;, reuse=True)使用，如果reuse属性值为None或者False，则tf.get_variable不能在同一变量空间下使用，例如下面：12345# reuse=False或者不说明时，都会报错的情况：with tf.variable_scope('foo'): v = tf.get_variable('v',[1],initializer=tf.constant_initializer(1.0))with tf.variable_scope('foo'): v1 = tf.get_variable('v',[1]) 四、参考文献 tensorflow之tf.placeholder 与 tf.Variable区别对比","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"tensorflow","slug":"NLP/tensorflow","permalink":"http://yoursite.com/categories/NLP/tensorflow/"}],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://yoursite.com/tags/tensorflow/"},{"name":"变量创建","slug":"变量创建","permalink":"http://yoursite.com/tags/变量创建/"}]},{"title":"viterbi，约束搜索，贪心搜索","slug":"viterbi，约束搜索，贪心搜索","date":"2019-03-28T15:38:17.000Z","updated":"2019-03-28T15:55:10.510Z","comments":true,"path":"2019/03/28/viterbi，约束搜索，贪心搜索/","link":"","permalink":"http://yoursite.com/2019/03/28/viterbi，约束搜索，贪心搜索/","excerpt":"本文是实习的第二篇，本文主要是对三大搜索方法进行区别，viterbi是全局最优但条件独立，约束搜索是组合全局最优，贪心搜索是束宽为1的约束搜索。","text":"本文是实习的第二篇，本文主要是对三大搜索方法进行区别，viterbi是全局最优但条件独立，约束搜索是组合全局最优，贪心搜索是束宽为1的约束搜索。 参考：答案参考 viterbi有最优解是因为HMM每一步是条件独立的！！！独立！！！独立！！！重要的事说三遍！既然后面的概率和前面的没关系，那前面选最大的概率就行了！！！beam search时后面的概率依赖于前面所有的词，相当于n-gram是满的，viterbi的n-gram是2！！！ 因此 viterbi算法是全局最优，但是他是HMM的独立条件得到的，即后面的概率和前面的没关系，因此最后得出来概率也不一是全局最合适。 seq2seq中，beam search 是为了找出词表所构成的token组合路径。词表所构成的搜索空间是所有可能的输出token，数量非常非常庞大。 viterbi找的是全局最优，beam search是局部最优，当beam search的束宽等于１的时候，也就相当于贪心搜索了；贪心搜索就是每次只找当前阶段最有可能的单词，而不是最有可能的组合，因为他的束宽为１。","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"实习","slug":"NLP/实习","permalink":"http://yoursite.com/categories/NLP/实习/"}],"tags":[{"name":"viterbi，约束搜索，贪心搜索","slug":"viterbi，约束搜索，贪心搜索","permalink":"http://yoursite.com/tags/viterbi，约束搜索，贪心搜索/"}]},{"title":"生成模型","slug":"生成模型","date":"2019-03-28T15:31:44.000Z","updated":"2019-03-28T15:48:15.000Z","comments":true,"path":"2019/03/28/生成模型/","link":"","permalink":"http://yoursite.com/2019/03/28/生成模型/","excerpt":"本文是实习期间第一篇，主要是对生成模型有一个比较大概的了解，主要研究了两类模型：传统编码器与解码器模型；指针生成网络模型，之后还讲述了attention机制的一些好处。虽然实习要补上很多东西，但这是值得的，加油。","text":"本文是实习期间第一篇，主要是对生成模型有一个比较大概的了解，主要研究了两类模型：传统编码器与解码器模型；指针生成网络模型，之后还讲述了attention机制的一些好处。虽然实习要补上很多东西，但这是值得的，加油。 一、传统编码器-解码器 二、LSTM与指针生成网络 三、补充 四、说明 五、Pointer Networks场景 六、attention解决的问题 七、ROUGE 八、参考文献 一、传统编码器-解码器以RNN为例： 二、LSTM与指针生成网络 三、补充 四、说明 1.隐状态$H={h_1},{h_2},{h_3},{h_4},…{h_i}$在进行解码之前就已经 算出来了；2.训练阶段：目标序列Y是已经知道的，因此在我们只需要$y_{t_1}$和$s_{t_1}$就可以算出$s_t$3.交叉熵损失函数需要多理解下，到底怎么求的。4.在注意力模型中$Z^{(i)}$是解码器的 当前隐状态（因为当前时刻的隐状态不怎么好求）5.序列符号：我们通常会在样本的输入序列和输出序列后面分别附上一个特殊符号“”表示序列的终止，前面 加上 五、Pointer Networks场景 关键词：解决摘要描述错误；解决OOV；选取权重最高。 指针网络特别适合那些直接复制源文本的某些单词（单词表现问复制每个单词），因此特别适合专有名词比较多的情况。同时由于是直接复制，即从中选取概率最高的作为当前预测结果，因此在一定程度上解决了自动摘要失实的问题——》也就是 对细节把控性不好的问题，同时还解决了OOV问题，因为传统的sequence模型是基于词表的概率分布，只要不在词典的单词，最后预测的概率一律为0. 六、attention解决的问题 关键词：固定长度的向量表示；长序列；相关性权重更大； 传统编码器-解码器结构的LSTM/RNN模型，由于不论输入长短，编码器都将其编码成一个固定长度的向量表示，这使模型对于长输入序列的学习效果很差（解码效果很差）； attention机制在编码阶段，因此使得与解码器前一个状态$S_{t-1}$相关性更大的单词对应的隐状态$h_i$的权重更大，（同时也是由于采用了全局的信息，）然后模型输出时会选择性地专注考虑输入中的对应相关的信息。说明: 1.查询项Q 为解码器的隐藏状态，键项K和值项V均为编码器的隐藏状态2.由于attention的目的是做对齐，感觉上最好是用当前时刻的隐藏状态来计算，但实际上又无法获得当前时刻的隐藏状态。 七、ROUGE 关键词：人工摘要与自动摘要做对比；统计二者之间重叠的基本单元（n元语法，词序列和词对） ROUGE评价方法与pyramid，BLUE方法一起作为评价自动摘要质量的内部评价方法的三大中流砥柱。 主要思想： 由多个专家分别生成人工摘要，构成标准摘要集，将系统生成的自动摘要与人工生成的标准摘要相比较，通过统计二者之间重叠的基本单元（n元语法，词序列和词对）的数目，来评价摘要的质量。通过多专家人工摘要的对比，提高评价系统的稳定性和健壮性。 这个方法已经成为评价摘要技术的通用标准之一。 八、参考文献Pointer-network理论及tensorflow实战get to the point:summarization with pointer-generator networksseq2seq编码器与解码器 –学习笔记漫谈四种神经网络序列解码模型【附示例代码】深度学习中 的 Attention机制Pointer Networks简介及其应用10.11. 注意力机制","categories":[{"name":"NLP，实习","slug":"NLP，实习","permalink":"http://yoursite.com/categories/NLP，实习/"}],"tags":[{"name":"生成模型","slug":"生成模型","permalink":"http://yoursite.com/tags/生成模型/"},{"name":"编码器","slug":"编码器","permalink":"http://yoursite.com/tags/编码器/"},{"name":"指针网络","slug":"指针网络","permalink":"http://yoursite.com/tags/指针网络/"}]},{"title":"BERT模型解释","slug":"BERT模型解释","date":"2019-03-10T04:42:33.000Z","updated":"2019-03-10T05:02:11.000Z","comments":true,"path":"2019/03/10/BERT模型解释/","link":"","permalink":"http://yoursite.com/2019/03/10/BERT模型解释/","excerpt":"本文主要是对BERT模型进行说明,主要分成两个部分:输入部分和特征提取部分,其中输入部分创新点在同时将词向量,位置向量,分割向量三者相加,作为Transformer的输入,同时采用Mask模型来实现随机预测mask单词,并且对判断是否是下一句进行了说明.Transformer本质上是编码器-解码器类型.其最主要的就是attention机制,主要是self attention,同时还使用残差网络和正则化对输入数据进行修正.最后以X+Z的向量类型作为解码器的输入.","text":"本文主要是对BERT模型进行说明,主要分成两个部分:输入部分和特征提取部分,其中输入部分创新点在同时将词向量,位置向量,分割向量三者相加,作为Transformer的输入,同时采用Mask模型来实现随机预测mask单词,并且对判断是否是下一句进行了说明.Transformer本质上是编码器-解码器类型.其最主要的就是attention机制,主要是self attention,同时还使用残差网络和正则化对输入数据进行修正.最后以X+Z的向量类型作为解码器的输入. 一.主要思想 二.输入部分 三.主要任务 3.1 Mask预测 3.2 下句话的判断 四.特征提取部分-Transformer 4.1 残差网络 4.1.1 原因 4.1.2 思想 4.2 transferomer动态预测图 五.对EMLO和GPT的改进 六.启示 七.参考文献 一.主要思想 通过双向多层的transformer来提取特征,之后进行一系列的NLP任务.(问答,分类,阅读理解) 预训练模型:训练结果比较好的一组权重值对应的模型可以与其他人分享使用该模型. 二.输入部分（１）embedding层:将input数据转化为word embedding（２）embedding_processor层：word embedding + position embedding（和Transformer一致） +segment embedding（BERT模型创新）–三种向量相加;（３）Transformer层 说明:word embedding-单词本身的向量,记住Mask位置的词也有专门的词向量,例如:131;position embedding-通过Transformer机制当中的公式得到的位置向量segment embedding-表明该词所在的位置,如果是上句向量为0,下句向量为1–句子对形式,对于一个句子的输入，可以只用segment embedding A,–分割向量 三.主要任务预训练的主要任务:（１）对随机遮挡词Mask预测;（２）对是否是下句话的判断. 3.1 Mask预测 主要是采用随机Mask方式,遮挡15%的词并替换为[mask],然后对这15%的词做以下操作:123a) 80%的概率，遮挡词被替换为[mask]。⟶别人看不到我。b) 10%的概率，遮挡词被替换为随机词。⟶别人看走眼我。c) 10%的概率，遮挡词被替换为原来词。⟶ 别人能看到我。 例子: 说明:input是数据,label的正确结果 这样做的方法是为了降低了可视域的难度。并且也会复制原本完整的句子,之后再次随机mask句子的其他单词进行预测. 3.2 下句话的判断 说明:input是数据,label的正确结果 四.特征提取部分-Transformer 4.1 残差网络4.1.1 原因 虽然传统的标准化能够修正数据分布,防止梯度弥散和梯度爆炸,从而加快训练,但是随着网络深度的加深,虽然说模型训练的速度上去了,但是模型的性能却没有上去,反而会降低模型的性能,因此残差网络主要是提高模型的性能(准确率). 4.1.2 思想 其实就是将输入和隐藏层的输出(下一个层的输入,也就是某层的激活函数值)建立一个映射关系(对该层所有神经元都一样的),就相当于是一个补血的措施.因此常见为$H(X)=F(X)+x$,其中x为输入数据的向量(两项一般同构的,即相同维度). 4.2 transferomer动态预测图 五.对EMLO和GPT的改进（１）BERT用transformer方法取代了ELMo中用LSTM提取特征的方法（２）BERT解决了GPT的单层transformer问题,改为双层（３）BERT采用了Fine tuning方式 六.启示1.双向:transformer当中,编码器是双向提取特征的,解码器是单向进行解析的;2.深度:由于transformer中使用残差网络,因此可以构建深度更深的网络;3.非标注数据:由于我们判断当前句是否是下一句(句级标注),因此我们受到词级标注影响就会小很多.4.双向预测:由于使用双向Tramsformer,因此模型就会在更深的层当中知道词所在的位置,因此我们使用的是Mask方法进行标注. 七.参考文献 Bert模型浅析 Bert论文阅读 Transformer模型详解 Transformer","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"BERT","slug":"BERT","permalink":"http://yoursite.com/tags/BERT/"},{"name":"Transformer","slug":"Transformer","permalink":"http://yoursite.com/tags/Transformer/"},{"name":"残差网络","slug":"残差网络","permalink":"http://yoursite.com/tags/残差网络/"}]},{"title":"Tomas Mikolov三篇论文特色","slug":"Mikolov三篇论文","date":"2019-03-10T02:59:06.000Z","updated":"2019-03-10T03:03:00.000Z","comments":true,"path":"2019/03/10/Mikolov三篇论文/","link":"","permalink":"http://yoursite.com/2019/03/10/Mikolov三篇论文/","excerpt":"本文主要是对词向量的三篇经典论文进行了一个概述,从word2vec到doc2vec再到N-gram embeding技术,这三种重要的词向量的重要思想,有效地提升了词向量的表示水平.","text":"本文主要是对词向量的三篇经典论文进行了一个概述,从word2vec到doc2vec再到N-gram embeding技术,这三种重要的词向量的重要思想,有效地提升了词向量的表示水平. 一.概况 二.word2vec 三.doc2vec 四.n-gram embedding 五.参考文献 一.概况1、Efficient Estimation of Word Representation in Vector Space, 2013－主要是发明了word2vec 构建了分布式向量2、Distributed Representations of Sentences and Documents, 2014–发明doc2vec,doc可以为任何长度的(段落,文档,句子), 主要实现了word2vec可以训练任何长度的向量(段落,文档,句子),只不过需要添加对应的矩阵作为输入(段落矩阵,文档矩阵),之后根据段落的id提取出对应的段落表示.3、Enriching Word Vectors with Subword Information, 2016－N-gram embeding 主要是提升对罕见词或者未出现词的表示能力,主要是采用N-gram embeding技术.何为n-gram embedding技术,参考如下说明: 二.word2vec 主要是通过调整权重矩阵作为最终的分布式词向量. 三.doc2vec 在doc2vec的CBOW模型中,相对于word2vec增加了段落向量作为输入 训练阶段:新增了paragraph id，即训练语料中每个句子都有一个唯一的id。paragraph id和普通的word一样，也是先映射成一个向量，即paragraph vector。paragraph vector与word vector的维数虽一样，但是来自于两个不同的向量空间。在之后的计算里，paragraph vector和word vector累加或者连接起来，作为输出层softmax的输入。在一个句子或者文档的训练过程中，paragraph id保持不变，共享着同一个paragraph vector，相当于每次在预测单词的概率时，都利用了整个句子的语义。( 说明下,段落可以是句子,文档和整个段落,因此在这里段落向量就相当与是一个句子的向量) 预测阶段:给待预测的句子新分配一个paragraph id，词向量和输出层softmax的参数和训练阶段得到的参数保持不变，重新利用梯度下降法(SGD)训练待预测的句子。待收敛后，即得到待预测句子的paragraph vector。 四.n-gram embeddingn-gram向量:通过词缀的vector来表示词 假如unset这个单词,其词向量(分布式)已经有了,使用bigram规则:”un”,”ns”,”se”,”et”这4个bigram,然后求出对应的词向量,之后进行叠加得到一个单词的n-gram词向量 这样每个词汇就可以表示成一串字母n-gram，一个词的embedding表示为其所有n-gram的和。这样我们训练也从用中心词的embedding预测目标词，转变成用中心词的n-gram embedding预测目标词。(对中心词也是使用n-gram进行切割,切割完成之后,求出各个切割部分的向量,之后进行叠加,得到中心词的词向量表示.这就是所谓的”n-gram embedding”,之前基于词的训练就叫做embedding). 五.参考文献 Word2Vec 作者Tomas Mikolov 的三篇代表作 NLP+2vec︱认识多种多样的2vec向量化模型","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"word2vec","slug":"word2vec","permalink":"http://yoursite.com/tags/word2vec/"},{"name":"doc2vec","slug":"doc2vec","permalink":"http://yoursite.com/tags/doc2vec/"},{"name":"N-gram-embeding","slug":"N-gram-embeding","permalink":"http://yoursite.com/tags/N-gram-embeding/"}]},{"title":"CNN,N-GRAM和word2Vec","slug":"CNN,N-GRAM和word2Vec","date":"2019-03-10T02:09:38.000Z","updated":"2019-03-10T03:05:27.000Z","comments":true,"path":"2019/03/10/CNN,N-GRAM和word2Vec/","link":"","permalink":"http://yoursite.com/2019/03/10/CNN,N-GRAM和word2Vec/","excerpt":"本文主要对CNN和N-Gram这两种模型进行了说明,其中CNN主要是通过卷积核(滑动窗口)对数据进行特征提取,得到不同词组合的权重,实现低维的n-gram,而N-Gram则是对通过前面N-1个词来预测下一个词的方法,来预测不同词组合出现的概率.本质上CNN是一个超级的N-Gram.","text":"本文主要对CNN和N-Gram这两种模型进行了说明,其中CNN主要是通过卷积核(滑动窗口)对数据进行特征提取,得到不同词组合的权重,实现低维的n-gram,而N-Gram则是对通过前面N-1个词来预测下一个词的方法,来预测不同词组合出现的概率.本质上CNN是一个超级的N-Gram. 一.CNN 二.N-Gram 三.区别 四.特征提取 五.word2vec模型算法 六.参考文献 一.CNN CNN其主要适用范围是所卷积的对象具有局部关联性,在图像中好理解,因为图像中的像素一般都是具有关联的,因此在进行卷积的时候,一般都会提取出重点部位的信息(眼镜或者鼻子等);在文本中,主要是提取出关键的单词和词语,就是低维的词袋模型(通过卷积核实现特征提取和降维,得到不同词组合的权重). 二.N-Gram CNN是一个超级的N-Gram,CNN相当于在低维向量空间中实现了N-Gram。如果我们用一个滑动窗口（比如宽度是2）滑过一个句子，那么，我们便可以提取到句子中的所有2-gram。 N-Gram主要是统计局部信息的模型(就是基于句中前面几个单词预测下个单词的可能性–局部统计),主要是统计不同词之间的关系.表现为他们一起出现的概率. 三.区别 N-gram是直接统计不同的N个词之间组合在一起的概率， CNN是通过学习得到不同词组合的每个卷积核的权重，加权和来达到某种分类或者其他目的。如果N过大，用于统计的数据不够，就会导致模型的过拟合，而N太小则会导致欠拟合(模型的精度随着N-Gram模型当中的N增大而增大)。 四.特征提取特征提取主要提取数据中最有用的信息. 语法:词语之间的组合状况. 语义:代词的具体指向. 词频:词语的出现次数 词性:五.word2vec模型算法１．两个算法：（１）Skip-grams ：给定目标中心词预测上下文（２）CBOW(连续词袋模型) ： 通过上下文预测目标中心词２．两种训练方式:（１）Hierarchical softmax（２）Negative sampling六.参考文献 CNN为什么能使用在NLP? word2vec是如何得到词向量的？","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"CNN","slug":"CNN","permalink":"http://yoursite.com/tags/CNN/"},{"name":"N-GRAM","slug":"N-GRAM","permalink":"http://yoursite.com/tags/N-GRAM/"},{"name":"特征提取","slug":"特征提取","permalink":"http://yoursite.com/tags/特征提取/"}]},{"title":"模型性能指标与卷积各层概念","slug":"模型性能指标与卷积各层概念","date":"2019-03-09T12:17:38.000Z","updated":"2019-03-09T12:29:24.140Z","comments":true,"path":"2019/03/09/模型性能指标与卷积各层概念/","link":"","permalink":"http://yoursite.com/2019/03/09/模型性能指标与卷积各层概念/","excerpt":"本文主要是对一些简单的概念进行简单的描述,便于日后的复习,防止简单概念复杂化.","text":"本文主要是对一些简单的概念进行简单的描述,便于日后的复习,防止简单概念复杂化. 衡量模型好坏的指标主要有两个: 准确率（precision）:（你给出的结果有多少是正确的） 召回率（recall）:（正确的结果有多少被你给出了） 但是这两个通常是此消彼长的（trade off），很难兼得, ROC曲线:准确率与召回率之间的关系,在模型调参的时候得到的; AUC:ROC曲线下的面积大小.衡量模型好坏的指标,但是比较不容易计算; F1:主要是代替AUC,通过结合准确率与召回率这两大指标,也就是F-Measure/F-Score方法中α=1的情况,具体公式如下: 1234#该式子可以理解为几何平均的平方除以算术平均,加权调和平均F1 = 2 * (precision * recall) / (precision + recall)precision = true_positives / (true_positives + false_positives)recall = true_positives / (true_positives + false_negatives) 全连接层:–分类器:提取到特征进行整合,得到不同类别,之后使用softmax方法,得到概率分类; 卷积层:特征提取器:不全连接,参数共享; 池话层:特征选择,减少参数个数.","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"PR","slug":"PR","permalink":"http://yoursite.com/tags/PR/"},{"name":"AUC","slug":"AUC","permalink":"http://yoursite.com/tags/AUC/"},{"name":"ROC","slug":"ROC","permalink":"http://yoursite.com/tags/ROC/"},{"name":"F1","slug":"F1","permalink":"http://yoursite.com/tags/F1/"},{"name":"全连接层","slug":"全连接层","permalink":"http://yoursite.com/tags/全连接层/"}]},{"title":"BN与LN","slug":"BN与LN","date":"2019-03-09T11:55:46.000Z","updated":"2019-03-09T12:23:56.300Z","comments":true,"path":"2019/03/09/BN与LN/","link":"","permalink":"http://yoursite.com/2019/03/09/BN与LN/","excerpt":"本文主要讲述的是批量标准化与层次标准化的知识,其主要目的是为了减少梯度弥散和梯度爆炸,加快训练速度,增强模型鲁棒性.BN与LN的区别最主要区别就是基于不同量级上的均值和方差.","text":"本文主要讲述的是批量标准化与层次标准化的知识,其主要目的是为了减少梯度弥散和梯度爆炸,加快训练速度,增强模型鲁棒性.BN与LN的区别最主要区别就是基于不同量级上的均值和方差. 一.批量正则化(BN) 1.1 原因 1.2 基本概念 1.3 分类情况 1.4 优缺点 二.层次标准化 三.参考文献 一.批量正则化(BN)1.1 原因 主要是由于数据之间的差异过大,分布不均,容易导致梯度弥散和梯度爆炸(链式求导,误差),从而使得神经网络训练速度很慢. 1.2 基本概念 主要是对该层的输入进行标准化(数据点减去均值,除以标准差(方差的开方)).因为该标准化方法常用在mini-batch的数据上,因此叫做BN. 说明:(1)减去均值:使得数据中心点是在原点;(2)除以标准差:缩小数据之间的分布差异 1.3 分类情况 通常我们将其分为输入层的输入数据的标准化与隐藏层的输入数据标准化.最大的区别就是:（１）输入层的输入数据的标准化:均值可以为0和方差可以为1;（２）隐藏层的输入数据标准化:均值不可以为0,方差不能为1,这个主要是为了训练更好的非线性模型.各隐藏层的输入均值在靠近0的区域即处于激活函数的线性区域,不利于训练 1.4 优缺点 (1)规范数据,加快训练; (2)使得模型更加健壮,由于其将每层的输入数据拉回在正太分布上,主要是通过调整γ和β这些参数; (3)由于BN的均值和方差是基于整体训练数据的,因此BN的训练速度是很慢的.难以应用在RNN中,因此我们提出了针对最小不同批次的训练数据(前提:不影响分布形态),使用不同的方差与均值.加快训练速度.—也就是我们接下来讲述的LN–层次标准化 二.层次标准化具体公式如下: 上面公式中 H 是 一层神经元的个数。这里一层网络共享一个均值和方差，不同批次的训练样本对应不同的均值和方差，这是和 Batch normalization 的最大区别。因此该方法适用于RNN,但是对CNN(本来就可以并行计算)作用不大. 三.参考文献 优化深度神经网络（三）Batch Normalization Layer Normalization AI数学Batch-Normalization详细解析","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"BN","slug":"BN","permalink":"http://yoursite.com/tags/BN/"},{"name":"LN","slug":"LN","permalink":"http://yoursite.com/tags/LN/"},{"name":"标准化","slug":"标准化","permalink":"http://yoursite.com/tags/标准化/"}]},{"title":"attention机制通俗解释","slug":"attention机制","date":"2019-03-09T10:12:31.000Z","updated":"2019-03-15T09:13:12.970Z","comments":true,"path":"2019/03/09/attention机制/","link":"","permalink":"http://yoursite.com/2019/03/09/attention机制/","excerpt":"本文主要讲述的是attention机制的主要思想及其流程,思想较为简单,其主要应用在编码器-解码器当中(seq2seq模型居多),最初主要是解决seq2seq机器翻译的弊端而出现的,之后发现该机制可以运用到很多地方.由于本人对attention机制理解不够透彻,因此此次对其进行了新的梳理,便于日后的回忆.","text":"本文主要讲述的是attention机制的主要思想及其流程,思想较为简单,其主要应用在编码器-解码器当中(seq2seq模型居多),最初主要是解决seq2seq机器翻译的弊端而出现的,之后发现该机制可以运用到很多地方.由于本人对attention机制理解不够透彻,因此此次对其进行了新的梳理,便于日后的回忆. 一.主要思想 二.原因 三.权重参数求解 四.应用 五.优点 四.参考文献 一.主要思想 主要就是词对应编码$h_i$间加权之和. 二.原因 传统seq2seq模型(编码器-解码器),越往后的单词权重越大,就会导致在进行序列输出的时候,首先输入的单词就会倾向越往后的单词另一种形式(例如机器学习这个序列,可能最开始翻译成learning),这样就会使得效果不好,因此我们人为随机设置输入项中各个单词的权重(每个词在不同的时候有不同的权重),通过BP算法,不断调整–权重矩阵(各个单词在不同时刻的权重),使得监督学习的损失函数降到最小.因此接下来就涉及到权重参数的求解,我们打算将其融合在attention模型的求解过程当中. 说明:seq2seq模型是一个结构化序列生成另一个结构化序列的模型. 三.权重参数求解具体求法如下: 设定变量:首先我们随机设定一个隐变量$z_0$(与输入向量维度一样),之后将各个单词的隐藏层编码$h_i$作为编码器的输入, 相似度计算:进行相似度计算(具体函数可以自行定义),即求得各个$h_i$对应权重$α_{ij}$,具体方式如下图: 说明:不同的求权重方法(对齐模型),代表不同的attention模型,也就是产生权重的概率分布的方法不同.常见计算方式有点乘dot,內积 权重系数归一化:为了便于计算,将求得的权重系数归一化; 得到语义编码$C_i$:$C_i=\\sum_{i=1}^{n}{a}_{ij}*{h_i}$–作为解码器的输入: 得到$z_1$及输出:通过$C_1$与$z_0$结合得到$z_1$,之后通过F函数(人为定义)得到Machine这个输出. 循环往复即可.四.应用语音识别+看图说话:重点部分进行训练五.优点从全局性考虑,获得全局性的权重信息.四.参考文献 深度学习中的注意力模型（2017版） Attention本质剖析","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"attention","slug":"attention","permalink":"http://yoursite.com/tags/attention/"},{"name":"seq2seq","slug":"seq2seq","permalink":"http://yoursite.com/tags/seq2seq/"}]},{"title":"EMLO模型的简单理解","slug":"EMLO模型的简单理解","date":"2019-03-09T08:47:25.000Z","updated":"2019-03-09T08:59:50.900Z","comments":true,"path":"2019/03/09/EMLO模型的简单理解/","link":"","permalink":"http://yoursite.com/2019/03/09/EMLO模型的简单理解/","excerpt":"本文是对EMLO模型的一个简单表述,主要讲述了EMLO的单词表示的生成过程,之后对EMLO模型的优缺点进行了阐述.","text":"本文是对EMLO模型的一个简单表述,主要讲述了EMLO的单词表示的生成过程,之后对EMLO模型的优缺点进行了阐述. 一.简单描述 二.优缺点 三.参考文献 一.简单描述Deep contextualized word representation:深层语境化的单词表示–emlo(Embedding from Language Models)主要是根据当前词的上下文不同而生成对应词的词向量.其主要阶段有两个:构建预训练的模型;将新生成的wordembedding(三种向量加权得到的)作为下游任务的输入.第一层是单词最初的wordembedding,这个用另外的一个语言模型学到的;第二层是单词上下文的wordembedding,主要获取的是句法特征(词语搭配),这个是用双向LSTM获取而来的,最初需要进行随机初始化.第三层是单词上下文的wordembedding,主要获取的是语义特征(例如.it所指代的词具体是什么),也是双向LSTM获取而来的.最初需要进行随机初始化.最后使用一个权重矩阵(需要进行不断调整的),将三个向量进行加权(解决一词多义),得到一个下游任务的输入向量. 二.优缺点 解决了一词多义的问题,因为单词的输入是情景化生成; 训练速度较慢; 当数据集较少和较差的时候,EMLO模型得到的效果较好,数据数量和质量较为高的时候,EMLO模型效果与其他模型相比,效果不那么明显;三.参考文献 从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"EMLO","slug":"EMLO","permalink":"http://yoursite.com/tags/EMLO/"}]},{"title":"求职经验摘录(一)","slug":"求职经验","date":"2019-03-09T08:04:00.000Z","updated":"2019-03-09T08:21:13.090Z","comments":true,"path":"2019/03/09/求职经验/","link":"","permalink":"http://yoursite.com/2019/03/09/求职经验/","excerpt":"本文的一些求职经验,主要是从网上摘录的,说得还是比较中肯的,为此将其放在网上便于日后复习.","text":"本文的一些求职经验,主要是从网上摘录的,说得还是比较中肯的,为此将其放在网上便于日后复习. 一.总结 二.关键词 三.模型篇 四.训练篇 本文经验摘自:别求面经了！小夕手把手教你斩下NLP算法岗offer！（11.27更新） 一.总结 不止今年，将来的面试肯定更趋向于千人千面，毕竟AI行业也将越来越细化嘛。所以小夕精心准备了这篇“万能”文章给你们，希望大家将来都能收割到自己想要的offer～ 顶会paper、top比赛、大厂研究院实习都不是必须的，但是最好它们取或运算后为真。从小夕身边的小伙伴的情况看，只要取或运算后为真，一般BAT的核心研究部门或者核心业务部门最少也能拿一个offer。 如果很不幸成为了“三无人员”，那么一定要保证扎实的数学、NLP、coding能力和最少一个研究方向的专精，能够在面试时表现出超出简历描述的能力，这样也会非常打动面试官的，毕竟谁都想招一个潜力股嘛。 二.关键词 虽然实现NLP的方法基本离不开机器学习与神经网络，但是如果按照前面简历篇讲的准备简历内容，其实在NLP岗的面试中很少直接考察ML和NN的理论知识。那考察什么呢？当然是考察关键词呀！所以总结一下自己简历的关键词，然后展开复习吧！ 下面小夕拿自己举个例子。小夕简历上出现的NLP关键字如:问答、MRC、对话、匹配、词向量、迁移、分类、分词、POS、NER等下面是面试中考过的基础知识举例 trick：方向不match的面试官喜欢考察词向量和文本分类相关的知识 SGNS/cBoW、FastText、ELMo等（从词向量引出）下面是面试中考过的基础知识举例 trick：方向不match的面试官喜欢考察词向量和文本分类相关的知识 三.模型篇 SGNS/cBoW、FastText、ELMo等（从词向量引出） DSSM、DecAtt、ESIM等（从问答&amp;匹配引出） HAN、DPCNN等（从分类引出） BiDAF、DrQA、QANet等（从MRC引出） CoVe、InferSent等（从迁移引出） MM、N-shortest等（从分词引出） Bi-LSTM-CRF等（从NER引出） LDA等主题模型（从文本表示引出） 四.训练篇 point-wise、pair-wise和list-wise（匹配、ranking模型） 负采样、NCE 层级softmax方法，哈夫曼树的构建 不均衡问题的处理 KL散度与交叉熵loss函数 评价指标篇 F1-score PPL MRR、MAP","categories":[{"name":"行业","slug":"行业","permalink":"http://yoursite.com/categories/行业/"},{"name":"思考","slug":"行业/思考","permalink":"http://yoursite.com/categories/行业/思考/"}],"tags":[{"name":"关键词","slug":"关键词","permalink":"http://yoursite.com/tags/关键词/"},{"name":"求职","slug":"求职","permalink":"http://yoursite.com/tags/求职/"}]},{"title":"行业思考","slug":"行业思考","date":"2019-03-09T07:49:14.000Z","updated":"2019-03-09T08:19:02.450Z","comments":true,"path":"2019/03/09/行业思考/","link":"","permalink":"http://yoursite.com/2019/03/09/行业思考/","excerpt":"本文主要是从网上进行摘录的,讲的很不错,因此将其公布在个人网站上,便于自己回顾.","text":"本文主要是从网上进行摘录的,讲的很不错,因此将其公布在个人网站上,便于自己回顾. 本文摘自:迅雷创始人程浩：人工智能创业的6大核心问题 决定要去一家公司一定要从公司人员、社会评价、曾在职人员、学校评价（如果你是应届生的话）、同行评价这几个维度多方位去了解，可能现在的孩子都挺聪明的，但是之前我找工作的时候就没有考虑那么多，所以进去没多久就走了。 人们常说职业规划分为三、五、八、十年，如果让我给我的十年交上的答卷打个分数，我想是八十分吧，有所积累，有所成长，还有一些进步和提升的空间，在未来第十五年到来之前，我想自己要突破的更多的还是与市场大环境的与时俱进。关键点： 1. 对行业的洞察理解。要熟知行业痛点； 2. 产品和工程化能力。光在实验室里搞没意义； 3. 成本控制。不光能做出来的产品，还得便宜的做出来； 4. 供应链能力。不光能出货，还要能批量生产； 5. 营销能力。产品出来了，你得把东西卖出去。团队里有没有营销高手，能不能搞定最好的渠道是关键。 一句话讲，要做技术、产品、商业和数据四位一体的“全栈”，这就是“一纵”。这才是健康的商业模式。 最后再说一点，目前大多数AI创业公司都是技术专家主导，这很容易理解，因为现在技术还有壁垒，技术专家主导起码保证产品能做出来。不过未来随着技术门槛的降低，特别在“非关键应用”领域里，团队的核心主导，会慢慢过渡到产品经理和行业专家为主，因为他们离用户需求最近。“非关键应用”领域，懂需求比技术实现更重要。长期来看，人工智能创业和任何其他领域的创业一样，一定是综合实力的比拼！","categories":[{"name":"行业","slug":"行业","permalink":"http://yoursite.com/categories/行业/"},{"name":"思考","slug":"行业/思考","permalink":"http://yoursite.com/categories/行业/思考/"}],"tags":[{"name":"职业规划","slug":"职业规划","permalink":"http://yoursite.com/tags/职业规划/"},{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/tags/人工智能/"}]},{"title":"GLOVE,PPMI,SVD","slug":"GLOVE,PPMI,SVD","date":"2019-03-09T04:23:50.000Z","updated":"2019-03-09T07:41:23.740Z","comments":true,"path":"2019/03/09/GLOVE,PPMI,SVD/","link":"","permalink":"http://yoursite.com/2019/03/09/GLOVE,PPMI,SVD/","excerpt":"本文主要讲述GLOVE,PPMI,SVD三种向量模型的主要区别及其关系,这个是重点,需要重点掌握.","text":"本文主要讲述GLOVE,PPMI,SVD三种向量模型的主要区别及其关系,这个是重点,需要重点掌握. 一.GLOVE 二.PPMI 三.SVD 3.1 SVD与word2vec的联系与区别 3.2 区别 3.3 联系 3.3.1 one-hot与词袋表示 3.3.2 结论 四.词向量的改进 五.补充 六.参考文献 一.GLOVE GLOVE相对于Word2vec,最主要是增加了共现矩阵,该矩阵的主要作用,可以看以下这个例子: 比如plays和cat共现了1000次。我们希望cat的word embedding和plays的context embedding的内积能和log(1000)比较接近．再比如cat和train共现了5次，那么cat和train的word/context embedding的内积就会比较小（大约等于log(5)）。如果它们的内积大于这个数字(说明向量之间的距离比较近,不符合实际距离.因此需要将两者的距离远一点)，GloVe就会让cat和train的word/context embedding距离远一些。总的来说GloVe和word2vec的基本原理差不多，GloVe根据最终的目标（也就是共现矩阵），不断地去调整向量间的距离距离，让它们接近或者疏远。最后的结果是上下文相似的单词就会有相似的词向量. 二.PPMI PPMI,最基础的词袋模型,这里是用局部上下文中的单词去得到单词的表示，所以也叫bag-of-contexts. 每个单词的词向量的维度是词表的大小;比如要得到cat的单词向量。我们就统计cat和其他所有单词在局部上下文中的共现情况。假设cat和plays在语料中共现1000次，plays在cat的单词向量向中对应的维度是第55维，因此cat单词向量的第55维就是1000。其实上面说的bag-of-contexts就是GloVe使用的共现矩阵。 但是PPMI（positive pointwise mutual information）是一种加权机制。cat和plays的共现能提供一些有价值的信息，不过cat和the共现的信息并没有什么价值，因为the和所有单词都大量的共现。基于上述考虑，我们可以用PPMI降低/增加一些维度的权重。说出来大家可能不信，像这样简单基础的单词表示，在一些场景下都会比word2vec，GloVe等神经网络词向量模型效果更好。比如中国-北京=日本-东京这样的性质，PPMI可以比word2vec和GloVe做得更好。 有了PPMI这样稀疏的表示，很自然的想法是对其降维得到低维的表示。可以考虑用SVD对PPMI进行分解，得到低维的单词向量。 虽然通过PPMI方法,根据上下文分布来表示一个词，而不再是孤立地“独热”了，因此能够表示语义的相关性，但问题是它还是N维的，维度还是太大了，整个词向量表（共现矩阵）太稀疏。因此我们引用出了通过SVD来进行降维,提高泛化性.(维度越高,泛化性越差,因为泛化所涉及的因素就是等于维度的个数.) 三.SVD 实际上就是将n个m维度的向量,通过奇异值分解,降维得到n个r维的向量.因此该向量也称之为SVD向量. SVD得到了word的稠密（dense）矩阵，该矩阵具有很多良好的性质：语义相近的词在向量空间相近，甚至可以一定程度反映word间的线性关系。(高维提取语义(例如,it所代表的含义),低维提取语法(词的搭配)) 3.1 SVD与word2vec的联系与区别3.2 区别 word2vec svd 方法 通过前后词来预测当前词 通过前后词来预测前后词 输出 最后接的是softmax来预测概率 没有 效果 相同的语料情况下， Word2Vec的词向量质量似乎更胜一筹。 3.3 联系CBOW是SVD的一个特例.word2vec保存了语义和语法,但是没有考虑语序. 3.3.1 one-hot与词袋表示 词袋模型就是one-hot向量的之和.因为词向量求和得到句向量.具体例子如下: 回到我们用one hot的时代，我们是这样表示一个句子的。假如我们将“我”、“爱”、“科学”、“空间”、“不”、“错”六个词用下面的one hot编码(按照词来表示) 那么不考虑词序的话，“我爱空间科学”这个短语就可以用下面的向量（词袋）表示 然后我们要对这个向量进行降维,主要是用神经网络做分类，后面可以接一个全连接层，隐藏节点为3.也就是将降成3维. 其中w12之类的权重值则是对应词的词向量,(只不过该词有三个维度) 因此,我们可以看到:词向量求和得到句向量，实际上就是传统的词袋模型的等价物！ 3.3.2 结论 Word2Vec的一个CBOW方案是，将前后若干个词的词向量求和，然后接一个N维的全连接层，并做一个softmax来预测当前词的概率。本文的前半部分说了，这种词向量求和(one-hot * 权重矩阵)，等价于原来的词袋模型接一个全连接层（这个全连接层的参数就是词向量表），这样来看，Word2Vec也只是一个N维输入(某个单词的分布式表示,n维)，中间节点为n个(输出节点)，N维输出的三层神经网络罢了，所以从网络结构上来看，它跟自编码器等价，也就是跟SVD分解等价。 比如很多模型实际上可以写成矩阵乘法，然后矩阵乘法就相当于一层神经网络，既然神经网络，那是不是可以加多层？是不是可以加激活函数？是不是可以用SVD分解一下？ 四.词向量的改进 对于词向量的改进，尤其是在word2vec上的改进。一个主流的改进方向就是加入更丰富的上下文信息。比如加入依存树的信息，这样我们就能知道上下文单词和中心单词之间的联系，理论上能帮助我们得到更好的词向量。再比如不仅考虑局部上下文的信息，还考虑单词和文档共现的信息等等（全局上下文）。合理的使用更加丰富的上下文信息可以提升词向量的质量，或者是得到不同性质的词向量。FastText还多引入了ngram特征，来缓解词序问题，但总的来说，依旧是把特征向量求平均来得到句向量． 五.补充 词袋模型，即向量的稀疏表示。我们需要先对文档库中的每一个句子进行分词，去停用词，词根化。 纯Attention！单靠注意力就可以！RNN要逐步递归才能获得全局信息，因此一般要双向RNN才比较好；CNN事实上只能获取局部信息，是通过层叠来增大感受野；Attention的思路最为粗暴，它一步到位获取了全局信息！ Position Embedding，也就是“位置向量”，将每个位置编号，然后每个编号对应一个向量，通过结合位置向量和词向量，就给每个词都引入了一定的位置信息，这样Attention就可以分辨出不同位置的词了，Position Embedding,谷歌有一个公式:$$\\\\PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})\\\\PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$$ pos是位置，i是维度。因为对于任意固定的offset k，$PE_{pos+k}$能被表示为$PE_{pos}$的线性函数。即把id为pos的位置，映射成一个$d_{model}$维的向量，这个向量的第i个元素的数值是$PE_{(pos,i)}$.奇数位元素是cos,偶数位元素是sin. 以前在RNN、CNN模型中其实都出现过Position Embedding，但在那些模型中，Position Embedding是锦上添花的辅助手段，也就是“有它会更好、没它也就差一点点”的情况，因为RNN、CNN本身就能捕捉到位置信息。但是在这个纯Attention模型中，Position Embedding是位置信息的唯一来源，因此它是模型的核心成分之一，并非仅仅是简单的辅助手段。 六.参考文献 词向量发展史-共现矩阵-SVD-NNLM-Word2Vec-Glove-ELMo 十分钟带你看遍词向量模型 tensor-to-tensor理论篇","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"词向量","slug":"词向量","permalink":"http://yoursite.com/tags/词向量/"},{"name":"PPMI","slug":"PPMI","permalink":"http://yoursite.com/tags/PPMI/"},{"name":"SVD","slug":"SVD","permalink":"http://yoursite.com/tags/SVD/"},{"name":"GLOVE","slug":"GLOVE","permalink":"http://yoursite.com/tags/GLOVE/"}]},{"title":"EM算法推导","slug":"EM算法推导","date":"2019-03-07T08:13:11.000Z","updated":"2019-03-07T08:19:04.880Z","comments":true,"path":"2019/03/07/EM算法推导/","link":"","permalink":"http://yoursite.com/2019/03/07/EM算法推导/","excerpt":"本文主要讲述EM算法的来源和推导,有助于理解EM的内涵.","text":"本文主要讲述EM算法的来源和推导,有助于理解EM的内涵. 一.主要思想 二.推导过程 一.主要思想 EM算法主要是解决混合高斯模型中的参数难以求解,导致的模型的最大后验概率难以一次性求得,因此使用EM算法来进行参数θ(模型的参数)的迭代(其中运用到了Jasen不等式–凸函数定理.),其中使用隐变量z,来说明某个数据点处于哪种分布. 二.推导过程推导过程如下:","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"EM算法","slug":"EM算法","permalink":"http://yoursite.com/tags/EM算法/"},{"name":"高斯模型","slug":"高斯模型","permalink":"http://yoursite.com/tags/高斯模型/"},{"name":"高斯混合模型","slug":"高斯混合模型","permalink":"http://yoursite.com/tags/高斯混合模型/"},{"name":"Jasen不等式","slug":"Jasen不等式","permalink":"http://yoursite.com/tags/Jasen不等式/"}]},{"title":"数据平滑的几种算法","slug":"数据平滑","date":"2019-02-27T12:04:57.000Z","updated":"2019-02-27T12:44:17.860Z","comments":true,"path":"2019/02/27/数据平滑/","link":"","permalink":"http://yoursite.com/2019/02/27/数据平滑/","excerpt":"本文主要讲述了数据平滑的几大算法,主要分为折扣,差值,回退,及其综合的算法,折扣方法有三种:其中拉普拉斯主要是直接通过给未出现词增加频数,当数据集比较大的时候问题,但不可避免的给予某些未出现的词较大的概率空间;古德图灵方法,主要是通过高频次的词来判断低频次(概率)的词,使用对应的折扣率,求出当前低频次的词;绝对折扣指的是(当前组合的频次-不同组合(unitGram,biGram和trigram之类的)在不同训练集上频数之差).插值算法则是指使用当前输入的子集(可以同阶,低阶)概率连乘.回退则是使用父集的概率,综合算法则是涉及到当前输入的词频和,预测词的词频和及其组合个数的结合,其中涉及到D,这个就是之前的绝对折扣.","text":"本文主要讲述了数据平滑的几大算法,主要分为折扣,差值,回退,及其综合的算法,折扣方法有三种:其中拉普拉斯主要是直接通过给未出现词增加频数,当数据集比较大的时候问题,但不可避免的给予某些未出现的词较大的概率空间;古德图灵方法,主要是通过高频次的词来判断低频次(概率)的词,使用对应的折扣率,求出当前低频次的词;绝对折扣指的是(当前组合的频次-不同组合(unitGram,biGram和trigram之类的)在不同训练集上频数之差).插值算法则是指使用当前输入的子集(可以同阶,低阶)概率连乘.回退则是使用父集的概率,综合算法则是涉及到当前输入的词频和,预测词的词频和及其组合个数的结合,其中涉及到D,这个就是之前的绝对折扣. 一.折扣法 1.拉普拉斯平滑 1.1 方案 1.2 优缺点 2.古德-图灵方法 2.1 主要思想 2.2 优缺点 3.Absolute discounting 二.插值法 1.线性插值法 2.Witten-Bell算法 四.回退法 五.综合 5.1 主要内容 5.2 改进版 六.影响数据平滑的因素 七.参考文献 数据平滑主要是针对数据稀疏/零概率问题的问题,所谓稀疏问题指的是这个数据没集没远远完全覆盖到现实情况.导致很多已经发生的事情/实例概率为0(大量的零值概率存在),这种零值的概率估计会导致语言模型算法的失败，例如：概率值作为乘数会使结果为0，而且不能做log运算. 说明:零概率问题，就是在计算实例的概率时，如果某个量x，在观察样本库（训练集）中没有出现过，会导致整个实例的概率结果是0。在文本分类的问题中，当一个词语没有在训练样本中出现，该词语调概率为0，使用连乘计算文本出现概率时也为0。这是不合理的，不能因为一个事件没有观察到就武断的认为该事件的概率是0。接下来讲解一些数据平滑的算法. 一.折扣法1.拉普拉斯平滑 主要是对概率公式的上下两项进行变换,分子加一，分母加K，K代表类别数目.让未出现的词,规定至少出现一次 1.1 方案（１）加一平滑$P(某个类的概率)\\frac{该类的词频+1}{词频总数+类别个数}$（２）加m平滑 它的效果通常会比Add-one好，但m需要人为设定$P(某个类的概率)\\frac{该类的词频+m}{词频总数+类别个数}$ 1.2 优缺点（１）优点: 只有当数据预料比较大的时候,对分子和分母单独进行相加这部分的影响可以忽略.（２）缺点:（2.1）当数据预料比较小的时候,如果使用该平滑方法,对结果影响比较大;（2.2）这个只是将未出现的概率类别都进行了加1,这样就相当于未出现的类别的概率都是一样的,没有区分性,不利于分类. 2.古德-图灵方法2.1 主要思想 用频数高的类N(r+1)来预测频数低的类N(r),因此可以进一步预测频数为0的.具体式子如下: 2.2 优缺点（１）解决了o概率问题,使得频数低的类的发生概率能够用频数高的来推测;（２）频数低的类的个数即$N_r$,不等于0(分母不为0),也就是要想求得N_0得值,因此,(假设最高频数是4),你需要使得N_3,N_2,N_1,N_0的个数不能为0,也就是频数要连续,否则任何一个为0,将会导致$N_0$对应得概率P(θ=0)不成立.因此常在小频数范围内使用. 3.Absolute discounting 插值法使用的参数实际上没有特定的选择，如果将lamda参数根据上下文进行选择的话就会演变成Absolute discounting。对于这个算法的基本想法是，有钱的，每个人交固定的税D，建立一个基金，没有钱的根据自己的爸爸有多少前来分这个基金。比如对于bigram的模型来说，有如下公式。参数说明:（１）D为参数，可以通过测试优化设定,主要是根据不同频数的bigrams （例如： “chinese food”，“good boy”，“want to”等)在不同数据集上出现的次数之差,具体如下图:经过测试得到D大约为0.75.（２）$P_{Abs}$$$P_{AbsDiscount}(w_i|w_{i−1})=C(w_{i−1}w_i)−dC(w_{i−1})+λ(w_{i−1})P(w_i) $$C()代表该词的个数,P()代表该词的概率 二.插值法1.线性插值法 设想对于一个trigram的模型，我们要统计预料库中“”“I like chinese food”出现的次数，结果发现它没有出现(也就是所谓的4元语法的的次数为0)，则计数为0，在回退策略中们将会试着用低阶的gram来进行替代，也就是用“like chinese food”出现的次数来替代(即P(like|chinese food)代替P(I|like chinese food)。在使用插值的时候，我们把不同阶层的n-gram的模型线性叠加组合起来之后再使用(也就是计算P(I|like),P(like|chinese),P(chinese|I like),之类的,进行线性叠加,)，简单的如trigram的模型，按照如下的方式进行叠加： 参数可以凭借经验进行设定，也可以通过特定的算法来进行确定，比如EM算法。对于数据一般可以分为: traning set, development set, testing set. 那么P的概率使用training set进行训练得出，lamda参数使用development set使得下面的概率最大化：得到 2.Witten-Bell算法 Witten-Bell算法终于从 Laplace 算法跳了出来，有了质的突破。这个方法的基本思想是：如果测试过程中一个实例在训练语料库中未出现过，那么他就是一个新事物，也就是说，他是第一次出现。那么可以用在语料库中看到新实例（即第一次出现的实例）的概率来代替未出现实例的概率。N是总频数,T是对应频数的个数之和$$\\begin{matrix} N = 1 \\times 50+2 \\times 40+3 \\times 30+4 \\times 20+5 \\times 10 = 350 \\\\ T = 50+40+30+20+10 = 150 \\\\ \\end{matrix}$$ $$\\frac{T}{N+T}=\\frac{150}{350+150}=0.3$$ 上述式子作为:近似表示在语料库看到新词汇的概率. 四.回退法 又名:Katz回退,该回退模型，思路实际上是：如果你自己有钱，那么就自己出钱，如果你自己没有钱，那么就你爸爸出，如果你爸爸没有钱，就你爷爷出，举一个例子，当使用Trigram的时候，如果Count（trigram）满足条件就使用，否则使用Bigram，再不然就使用Unigram.因此,尽可能的使用高阶模型。但是有时候高级模型的计数结果可能为0，这时我们就转而使用低阶模型来避免稀疏数据的问题 它也被称为：Katz smoothing，具体的可以去查看相应的书籍。它的表达式为： 其中d，a和k分别为参数。k一般选择为0，也就是爸爸存在语料库即可,但是也可以选其它的值.（１）第一行就是:父句频数/字句频数,前提爸爸有;（２）第二行就:爸爸在语料库当中不出现,那么就计算你爷爷(从i-n+2开始了)–也就是当高阶信息不存在,就是用所谓的低阶信息; 五.综合5.1 主要内容 Kneser-Ney smoothing这种算法是目前一种标准的而且是非常先进的平滑算法，它其实相当于前面讲过的几种算法的综合。它的思想实际上是：有钱的人，每个人交一个固定的税D，大家一起建立一个基金，没有钱的呢(也就是这个句子不存在语料库当中)，根据自己的的爸爸的“交际的广泛”的程度来分了这个基金(就是biGram的组合数目.)。这里交际的广泛实际上是指它爸爸会有多少种不同的类型，类型越多，这说明越好。其定义式为：或另一种形式: $$P_{KN}(wi|w_{i−1})=max(C(w_{i−1}w_i)−d,0)C(w_{i−1})+λ(w_{i−1})Pcontinuation(w_i) $$$λ(w_{i−1})$是一个系数,$Pcontinuation(w_i)$表示的是,不同词$w_i$的bigram,在总biGram当中所占的比例. 其中max（c(X)-D,0）的意思是要保证最后的计数在减去一个D后不会变成一个负数，D一般大于0小于1。这个公式递归的进行，直到对于Unigram的时候停止。而$λ$是一个正则化的常量，用于之前已经分配的概率值（也就是从高频词汇中减去的准备分配给哪些未出现的低频词(语料库当中的)的概率值（分基金池里面的基金））。其表达是为：或说明下:冒号后面是对应的条件$P_KN$是在wi固定的情况下，不同词的unigram和bigram数目的比值，这里需要注意的是$P_KN$是一个分布，它是一个非负的值，求和的话为1。 该方法主要的作用就是预测横线的词,I used to eat Chinese food with __ instead of knife and fork.主要是根据横线上的各个候选词对应unigram和bigram数目的比值也就是$Pcontinuation(w_i)$来决定,而不是候选词在语料库的频数**来决定选谁.$$Pcontinuation(w_i)=\\frac{UnitGram–对应词的词频}{BiGram–组合个数}$$ 5.2 改进版 这一种方法是上一种方法的改进版，而且也是现在最优的方法。上一个方法，每一个有钱的人都交一个固定的锐，这个必然会出现问题，就像国家收税一样，你有100万和你有1个亿交税的量肯定不一样这样才是比较合理的，因此将上一种方法改进就是：有钱的每个人根据自己的收入不同交不同的税D，建立一个基金，没有钱的，根据自己的爸爸交际的广泛程度来分配基金。 这里D根据c来设定不同的值，比如c为unigram，则使用D1，c位bigram，则使用D2，如果是大于等于3阶的使用D3,其中D还是根据数据集测试的出来的 六.影响数据平滑的因素几个对数据平滑效果产生影响的因素：（1）修正的后备分布（2）绝对减值优于线性减值（3）对于较低的非零计数（就是出现次数较少的n元语法，就是罕见的情况），插值模型优于后备模型（4）增加算法的自由参数，通过留存数据优化这些参数。 七.参考文献自然语言处理中N-Gram模型的Smoothing算法自然语言处理之数据平滑方法古德-图灵估计Good-turning估计自然语言处理：盘点一下数据平滑算法关于数据平滑的一些理解 Statistical Language Model笔记+几个简单平滑算法","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"拉普拉斯","slug":"拉普拉斯","permalink":"http://yoursite.com/tags/拉普拉斯/"},{"name":"回退","slug":"回退","permalink":"http://yoursite.com/tags/回退/"},{"name":"差值","slug":"差值","permalink":"http://yoursite.com/tags/差值/"}]},{"title":"泰勒公式与Hessian矩阵","slug":"泰勒公式与Hessian矩阵","date":"2019-02-26T06:20:42.000Z","updated":"2019-02-26T06:29:52.490Z","comments":true,"path":"2019/02/26/泰勒公式与Hessian矩阵/","link":"","permalink":"http://yoursite.com/2019/02/26/泰勒公式与Hessian矩阵/","excerpt":"本文主要讲述了泰勒公式与Hessian矩阵,最后对导数部分的知识进行了补充,其中hessian矩阵是泰勒公式二阶导的系数,同时可以判断Jacobian矩阵所得到的临界点,主要是通过heissian矩阵的特征值正负情况来进行判断.","text":"本文主要讲述了泰勒公式与Hessian矩阵,最后对导数部分的知识进行了补充,其中hessian矩阵是泰勒公式二阶导的系数,同时可以判断Jacobian矩阵所得到的临界点,主要是通过heissian矩阵的特征值正负情况来进行判断. 一.泰勒公式原理 1.1 含义 1.2 与其他定理的联系 二.微分与hessian矩阵 三.hessian矩阵特征值与特征向量 四.用途 4.1 判断临界点 4.1.1 矩阵正定性判断 4.1.2 判断方法 五.缺点 六.补充 6.1 方向导数 6.2 偏微分与全微分 6.3 方向导数,偏导数 6.4 负梯度方向 七.参考文献 一.泰勒公式原理1.1 含义 泰勒公式一句话描述：就是用多项式函数去逼近某点所在的光滑函数(实际存在,但是我们计算不出来.)。 多项式个数越多和次数越高,越逼近该点所在的原函数. 因此,泰勒公式让我们可以通过一个点来窥视整个函数的发展，因为点的发展趋势蕴含在导数之中(因为导数作为多项式的系数,决定点所在的曲线,因为点存在,说明x之类的变量已经固定下来了)，而导数的发展趋势蕴含在二阶导数之中….. 1.2 与其他定理的联系 柯西定理&gt;泰勒公式/定理&gt;拉格朗日中值定理&gt;罗尔中值定理(本质上等价)–从右向左推广（１）拉格朗日中值定理:$f(x)=f(x0)＋f‘(ξ)(x-x0)$或$f(x)-f(x0)=f‘(ξ)(x-x0)$,从上面我们可以看出他是泰勒公式只达到1阶多项式的时候. 用一条线段把两个端点连上，它是这条曲线函数的弦。函数上一定有一点，什么点呢，它的切线与弦平行。（２）罗尔中值定理:实质就是说如果闭区间上的两个端点值相等，那么这个函数上一定有这样一点，什么点呢，它的导数值是零.如下图:（３）柯西定理:实质上就是说,链接两点之间的曲线存在一条切线平行于 两点的弦(直线)（４）洛比达法则是柯西定理在求极限时的一个应用. 二.微分与hessian矩阵 我们先根据一元函数微分的公式定义,求出一元函数的一阶导数(最基本): 从上面我们可以看出,f’(x)是一元函数的一阶导数,那么我们根据之前的公式求得二元函数的一阶导数的具体形式,首先我们需要先列出一元函数的一阶导数(一般形式):$f(x+\\Delta x,y+\\Delta y)-f(x,y)=f’(x,y)\\binom{\\Delta x}{\\Delta y}+o(\\rho )$ ,其中$\\rho =\\sqrt[]{(\\Delta x)^2+(\\Delta y)^2}$ 之后根据一元函数的一阶导数类推出一元函数的一阶导数(具体形式):通过对比,我们可以看到 之后我们使用类似对比的方法求出二元函数的二阶导数,只不过是基于 一阶导数的微分来求得的.（１）一阶导数的微分的一般形式:$(f’_x(x+\\Delta x,y+\\Delta y),f’_y(x+\\Delta x,y+\\Delta y))-(f’_x(x,y),f’_y(x,y))=f’’(x,y)\\binom{\\Delta x}{\\Delta y}+o(\\rho )$其中$\\rho =\\sqrt[]{(\\Delta x)^2+(\\Delta y)^2}$（２）一阶导数的微分的具体形式: 对比可得我们可以看到,二元函数的二阶导数,我们把这种多元函数的二阶导数定义为hessian矩阵.我们将其推广到一般形式,如下图:.我们可以结合一个例子来观察:从上面我们可以得知: 在多元函数的泰勒公式中,我们使用梯度向量/雅可比矩阵作为对应的一阶导数,使用hessian/黑塞矩阵作为二阶导数,这两个导数作为多元函数泰勒公式当中多项式的系数. 三.hessian矩阵特征值与特征向量（１）Hessian矩阵是判断曲面上某点的凹凸性.,不是局部（２）Hessian矩阵的特征值就是形容其在该点附近的凹凸性，特征值越大，凸性越强.（３）Hessian的特征向量表示从该点出发的某个方向，特征值表示对应特征向量方向坡度下降的快慢.（４）Hessian矩阵的特征值正负决定曲面上该点的凹凸性,是凹,凸还是马鞍.特征值大小决定曲面上该点的凹凸性程度(伸缩程度),例如椭圆长短轴,因为特征值大对应的特征向量,是椭圆上的长轴,反之,小的对应短轴.（５）Hessian矩阵的特征值相差很大,则会变成斜长型的椭圆,导致特征值大代表的特征向量的方向,在基于一阶导数的步长情况下,比**特征值小代表的特征向量的方向走的快**,很容易越过谷底,最终导致梯度下降轨迹成之之字型,出现锯齿现象，所以收敛速度会变慢.如果特征值相差不大(且都为正),那么该点的凹凸程度接近正圆,各个方向的步伐就会一致,加快梯度下降. 四.用途4.1 判断临界点 hessian矩阵判断雅可比矩阵/梯度向量求出的临界点是否是极值点,最值点或者鞍点. 主要是根据hessian矩阵的正定性来判断: 4.1.1 矩阵正定性判断（１）特征值全部大于0/说明顺序主子式(计算量比较大)都大于0–&gt;正定矩阵,对应行列式一定大于0（２）特征值全部小于0/说明顺序主子式都小于0–&gt;负定矩阵,行列式正负不确定 （２.１）当矩阵的阶数为奇数,行列式小于0;—&gt;奇数个负特征值相乘 （２.２）当矩阵的阶数为偶数,行列式大于0;—&gt;偶数个负特征值相乘（３）其余情况为不定矩阵.行列式可能小于0或者大于0.（４）某驻点上,若干方向不一致,就是鞍点,判断手段就是使用黑塞矩阵,判断其为不定矩阵(特征值有正有负,且特征值可以为0),黑塞矩阵的特征值有正负,说明对应的特征向量方向不同. 半正定矩阵： 所有特征值为非负。 半负定矩阵：所有特征值为非正。 不定矩阵：特征值有正有负4.1.2 判断方法关于优化,当求出（１）Hessian是正定–&gt;|H|&gt;0,极小值;–&gt;函数在该点图像椭圆漏斗形(凸函数)（２）Hessian是负定–&gt;|H|&lt;0,极大值;–&gt;函数图像呈现椭圆山峰形(凹函数)（３）Hessian是不定–&gt;|H|&lt;0,极小值/最小值;–&gt;马鞍形(例如:两侧是上坡,两侧是下坡,方向不一样,在某些方向往上曲，在其他方向往下曲)如下图: 驻点:一阶导为0的点鞍点:一阶导为0,非局部极值的点,鞍点是拐点的驻点(因为鞍点的一阶导为0),它的曲面在鞍点好像一个马鞍,在一幅等高线图里，一般来说，当两个等高线圈圈相交叉的地点，就是鞍点。例如，两座山中间的山口就是一个鞍点。拐点:曲线凹凸性发生改变的点,不以一阶导是否为0为判断,但可以作为分类,分为拐点的驻点与非驻点.如下图: 五.缺点 优化理论中的原始牛顿法及阻尼牛顿法的缺陷就在这里，因为Hessian是半正定的所以，不能保证每次都能收敛到极小值点,因为Hessian矩阵算法它等于0的时候，就没办法判断此点的是否是极值点 梯度的散度就是 Laplacian； 梯度的 Jacobian 就是 Hessian。 牛顿方向（考虑海森矩阵）才一般被认为是下降最快的方向，可以达到superlinear的收敛速度。梯度下降类的算法的收敛速度一般是linear甚至sublinear的（在某些带复杂约束的问题）。为什么在一般问题里梯度下降比牛顿类算法更常用呢？因为对于规模比较大的问题，Hessian计算是非常耗时的；同时对于很多对精度需求不那么高的问题，梯度下降的收敛速度已经足够了。 梯度下降法找到的不一定是最快下降的点。梯度下降由梯度方向，和步长决定，每次移动一点点。但是每一次移动都是往极值方向，所以能够保证收敛,它只是目标函数在当前的点的切平面（当然高维问题不能叫平面）上下降最快的方向。怎么找到合适的步长，使得每次下降最快，这是另外一个问题了。 六.补充6.1 方向导数 读者就已经发现偏导数的局限性了，原来我们学到的偏导数指的是多元函数沿坐标轴的变化率，但是我们往往很多时候要考虑多元函数沿任意方向的变化率，那么就引出了方向导数.$f_{y} (x,y)$指的是函数在x方向不变，函数值沿着y轴方向的变化率 偏导数$f_{x} (x_{0},y_{0} )$就是曲面被平面$y=y_{0}$所截得的曲面在点$M_{0}$处的切线$M_{0}T_{x}$对x轴的斜率 偏导数$f_{y} (x_{0},y_{0} )$就是曲面被平面$x=x_{0}$所截得的曲面在点$M_{0}$处的切线$M_{0}T_{y}$对y轴的斜率具体如下图: 6.2 偏微分与全微分 多元函数（以三元函数为例）u=f(x,y,z)如果可微，则全微分du=f1(x,y,z)dx+f2(x,y,z)dy+f3(x,y,z)dz，这里f1、f2、f3分别表示u对x、y、z的偏导数。f1(x,y,z)dx称为关于x的偏微分，f2(x,y,z)dy称为关于y的偏微分，f3(x,y,z)dz称为关于z的偏微分。 全微分符合叠加原理，即全微分等于各偏微分之和。 偏微分也可以作为偏增量的近似，例如：f(x+△x，y，z）-f(x,y,z)≈f1(x,y,z)dx 6.3 方向导数,偏导数不是没有关系，而是离不开的关系，缺少不了的关系。1、方向导数 directional derivative 中，二维平面上，必须有两个偏导数； 三维空间上的方向导数，必须有三个方向的偏导数；2、对三维空间而言，方向导数是沿着一个特定方向的导数； 这个导数，是三个偏导数在这个特殊方向上的投影之和。 方向导数是说，对于方程z=f(x,y)，当&lt;x,y&gt;从&lt;x0,y0&gt;沿指定方向增加单位向量长度时，f(x,y)增加多少。当指定方向与梯度不重合时，函数变化就没有沿梯度方向变化大在&lt;x0,y0&gt;的梯度正交于过该点的等高线 本来梯度就不是横空出世的，当我们有了这个需求（要求一个方向，此方向函数值变化最大），得到了一个方向，然后这个方向有了意义，我们给了它一个名称，叫做梯度 6.4 负梯度方向 投影 导致方向导数为负的 七.参考文献为什么梯度反方向是函数值下降最快的方向？梯度寻优解释为什么梯度方向是函数值增长最快而不是下降最快的方向？","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"Hessian矩阵","slug":"Hessian矩阵","permalink":"http://yoursite.com/tags/Hessian矩阵/"},{"name":"泰勒","slug":"泰勒","permalink":"http://yoursite.com/tags/泰勒/"},{"name":"中值定理","slug":"中值定理","permalink":"http://yoursite.com/tags/中值定理/"}]},{"title":"Jacobian矩阵解析","slug":"Jacobian矩阵解析","date":"2019-02-26T06:03:55.000Z","updated":"2019-02-26T06:07:52.000Z","comments":true,"path":"2019/02/26/Jacobian矩阵解析/","link":"","permalink":"http://yoursite.com/2019/02/26/Jacobian矩阵解析/","excerpt":"本文主要讲述了雅可比矩阵的基本形式,及其作用,其中最直接的作用是作为一阶偏导的系数,用在泰勒公式中,掌握好这个,对于理解凸函数优化有重要作用.","text":"本文主要讲述了雅可比矩阵的基本形式,及其作用,其中最直接的作用是作为一阶偏导的系数,用在泰勒公式中,掌握好这个,对于理解凸函数优化有重要作用. 一.形式 二.用途 2.1 泰勒公式 2.2 空间变换 2.3 积分变换 2.4 求出临界点 2.5 逆向回推函数图像 三.缺点 四.Numeric Method 一.形式 自变量$x ∈ ℝ^n$,函数 $f(x) ∈ ℝ^m$,因此可以实现$ℝ^n → ℝ^m$的空间转换 因此jacobian是函数值为向量的多变量函数/矢量函数的一阶求导,是梯度的泛化,因为梯度只是针对与函数值为标量的多变量函数/标量函数的一阶求导(也就是m = 1时，函数的雅克比矩阵).梯度如下图:函数值为u这个标量,多变量有$x,y,z$ 一个值为标量的多变量函数的梯度(一阶导)的雅克比矩阵(一阶导)就是二阶导数，也就是Hessian矩阵$H(f)(X) = J(\\nabla f)(X)$ 说明:竖向都是按照自变量的顺序来求导(记住),然后变量转置雅可比矩阵包含的是函数某点(任意一点,需要确定)处函数如何变化的信息,因为该点的各个参数的偏导数存储在雅可比矩阵中.雅可比矩阵有利于整体把握各个变量与函数间的变化关系 二.用途2.1 泰勒公式 结合泰勒公式(不管是1还是多元),使用雅可比矩阵作为1阶多项式的系数(因为在泰勒公式下,多项式的系数涉及到导数).实现在该点下,基于泰勒公式,用多项式模拟该点所在的函数(不可知).也就是模拟在该点处的最优线性逼近(线性,都是多项式相加) 2.2 空间变换 雅可比矩阵是作为空间转换的依据,可以实现$ℝ^n → ℝ^m$的空间转换. 2.3 积分变换假设变换的原因就是,x,y所对应的积分域不好求 因此,雅可比行列式就是积分变换的倍率,面积(dxdy与dudv)比例系数; 因此在更换的过程中,u,v要可微,这样才实现对x,y的偏导 说明:一元函数中可导与可微等价，它们与可积无关。多元函数可微必可导(多元中是偏导)，而反之不成立 2.4 求出临界点 使得雅可比矩阵一阶导为0的点,但不确定是不是极值点,鞍点还是最值点. 例如,在NN中当损失函数是简单函数的时候,可以令雅可比向量(向量也是矩阵)求出极值点. 当损失函数是复杂函数的时候,由于雅可比矩阵求取的是某点处函数的变化趋势(缺点),因此只能看到近处的情况,不能看到全局的情况,但是我们可以任意选取一点,之后求出该点上雅可比矩阵,然后根据该矩阵,确定下坡的方向,缓慢移动到新的点.之后再次更新雅可比矩阵,经历若干个点的雅可比矩阵的计算,可以比较快速的找到局部最优解 2.5 逆向回推函数图像常用于求出某点周围情况 根据各个点的雅可比矩阵的绝对值长度,之后设置一个阀值,将阀值以内的点连成一条线即可,只需要找到若干个点即可完成初步的图像还原. 三.缺点 由于雅可比矩阵求取的是某点处函数的变化趋势(缺点),因此只能看到近处的情况,不能看到全局,函数的变化情况 在复杂的函数中,求出所有点的雅可比矩阵计算量大,因此不能一次完成极值/最值的求解.主要原因如下: 在模型当中,有特别多的参数; 函数非常复杂,可能没有符合要求的函数范式,即使有,但是计算成本很大; 函数可能会出现断连; 即使知道函数的形式,但是函数过于复杂,会出现特别多的噪音(特别陡峭); 四.Numeric Method 渐进的方法—逐步求解–Numeric Method 思想:先看图: 找到某一个点对应的x的y值和下一个点对应x的y值,然后求出函数的差值和自变量的差值,然后根据导数定义,求这一段区间上的jacoian矩阵中各个偏导数的近似解(因为点很多,因此可以将小区间作为一个点,得到该点的jacobian).之后使用相应的步长进行更新,求出下一个点(小区间)所在的jacobian,循环往复,逐步推进.(得到近似解)–逐步移动点(模型参数作为点的各个维度),然后逐步移动到使得jacobian矩阵接近0的点,然后进行进一步的判断(黑塞矩阵)","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"Jacobian","slug":"Jacobian","permalink":"http://yoursite.com/tags/Jacobian/"}]},{"title":"FastText原理解析","slug":"FastText","date":"2019-02-26T05:49:57.000Z","updated":"2019-03-10T10:00:16.400Z","comments":true,"path":"2019/02/26/FastText/","link":"","permalink":"http://yoursite.com/2019/02/26/FastText/","excerpt":"本文主要讲述的是FastText模型的原理,其两大特点为:N-Gram特征词向量,层次化softmax.因此本节,主要讨论这两大特点,有时候看论文,在了解基本原理的前提下,了解该论文的特色之处,给予自己灵感.","text":"本文主要讲述的是FastText模型的原理,其两大特点为:N-Gram特征词向量,层次化softmax.因此本节,主要讨论这两大特点,有时候看论文,在了解基本原理的前提下,了解该论文的特色之处,给予自己灵感. 一.主要思想 二.模型架构 三.模型构建步骤 四.层次化softmax 4.1 原因 五.n-gram特征 5.1 基本概念 5.2 原因 5.3 hash存储 六. FastText与CBOW 6.1 不同点 6.2 相同点 七.使用场景 八.参考文献 一.主要思想将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。这中间涉及到两个技巧：字符级n-gram特征的引入以及分层Softmax分类。注:fastText工具不仅实现了文本分类，还实现了快速词向量训练工具 二.模型架构输入–&gt;隐藏层–&gt;层次化softmax输入层是单个文档中所有词的词向量和文档中各个单词的n-gram组合向量;隐藏层是各个向量的叠加平均;输出层则是使用多层次的softmax函数来进行分类,训练复杂度是log级别,但是预测的时候则还是O(n)级别这个要搞清楚. 说明:叠加词向量背后的思想就是传统的词袋法CBOW，即将文档看成一个由词构成的集合–因此叠加词向量就得到一个文档向量/句子向量.(因此在CBOW模型当中输入是文档中的各个单词,只不过在该模型中的形式是one-hot向量) 三.模型构建步骤首先定义几个常量：VOCAB_SIZE = 2000EMBEDDING_DIM =100MAX_WORDS = 500CLASS_NUM = 5VOCAB_SIZE表示词汇表大小，这里简单设置为2000；EMBEDDING_DIM表示经过embedding层输出，每个词被分布式表示的向量的维度，这里设置为100。比如对于“达观”这个词，会被一个长度为100的类似于[ 0.97860014, 5.93589592, 0.22342691, -3.83102846, -0.23053935, …]的实值向量来表示；MAX_WORDS表示一篇文档最多使用的词个数，因为文档可能长短不一（即词数不同），为了能feed到一个固定维度的神经网络，我们需要设置一个最大词数，对于词数少于这个阈值的文档，我们需要用“未知词”去填充。比如可以设置词汇表中索引为0的词为“未知词”，用0去填充少于阈值的部分；CLASS_NUM表示类别数，多分类问题，这里简单设置为5步骤如下: 添加输入层（embedding层）。Embedding层的输入是一批文档，每个文档由一个词汇索引序列构成。例如：[10, 30, 80, 1000] 可能表示“我 昨天 来到 达观数据”这个短文本，其中“我”、“昨天”、“来到”、“达观数据”在词汇表中的索引分别是10、30、80、1000；Embedding层将每个单词映射成EMBEDDING_DIM维的向量。于是：input_shape=(BATCH_SIZE, MAX_WORDS), output_shape=(BATCH_SIZE,MAX_WORDS, EMBEDDING_DIM)； 添加隐含层（投影层）。投影层对一个文档中所有单词的向量进行叠加平均。keras提供的GlobalAveragePooling1D类可以帮我们实现这个功能。这层的input_shape是Embedding层的output_shape，这层的output_shape=( BATCH_SIZE, EMBEDDING_DIM)； 添加输出层（softmax层）。真实的fastText这层是Hierarchical Softmax，因为keras原生并没有支持Hierarchical Softmax，所以这里用Softmax代替。这层指定了CLASS_NUM，对于一篇文档，输出层会产生CLASS_NUM个概率值，分别表示此文档属于当前类的可能性。这层的output_shape=(BATCH_SIZE, CLASS_NUM) 指定损失函数、优化器类型、评价指标，编译模型。损失函数我们设置为categorical_crossentropy，它就是我们上面所说的softmax回归的损失函数；优化器我们设置为SGD，表示随机梯度下降优化器；评价指标选择accuracy，表示精度。 四.层次化softmax4.1 原因主要是假如类别比较多,那么softmax的计算量就会很大,导数分类速度下降.因此我们需要使用层次化softmax来加快分类,为此我们需要先构建哈夫曼树,我们使用类别的频数作为哈夫曼树的权重,之后根据哈夫曼树的构建法则,两两合并,得到权值最小的哈夫曼树,最后就使得词频多(权重大)的类别所在的叶子节点的深度是最浅的,因此就可以加快速度,然后我们训练的时候,根据输入,我们就可以得到层次化softmax上的哈夫曼树中的叶子节点是哪个(也就是输入及其类别都确定了),因此之后我们只需要跟着哈夫曼树的最短路径来进行对应类的概率求解.具体路径如下:其中记号说明如下: ⟦x⟧是一个特殊的函数，如果下一步需要向左走其函数值定义为1，向右则取-1。在训练时，我们知道最终输出叶子结点，并且从根结点到叶子结点的每一步的路径也是确定的。 v’ 是每个内部结点（逻辑回归单元）对应的一个向量，这个向量可以在训练过程中学习和更新。 h 是网络中隐藏层的输出。因此，我们以隐藏层的输出、中间结点对应向量以及路径取向函数为输入，相乘后再经过sigmoid函数，得到每一步逻辑回归的输出值因此我们可以看到:层次化softmax只不过就是单单计算输入节点对应类别的概率即可,而不需像softmax函数一样,将全部类别的概率算出来,加之正确类别的概率往往站到总概率的很大部分,因此我们将层次化softmax作为softmax的近似.同层次化softmax是softmax的近似,因此只需要正确类别的概率(softmax则是需要计算全部类别的概率),因此该方法最终计算复杂度是O(log)级别,之前的是O(n)级别. 树的结构是根据类标的频数构造的霍夫曼树。K个不同的类标组成所有的叶子节点，K-1个内部节点作为内部参数，从根节点到某个叶子节点经过的节点和边形成一条路径， 五.n-gram特征5.1 基本概念n-gram的讲解之前有涉及,在文本特征提取中，常常能看到n-gram的身影。它是一种基于语言模型的算法，基本思想是将文本内容按照字节顺序进行大小为N的滑动窗口操作，最终形成长度为N的字节片段序列。看下面的例子：以当前词为中心,窗口大小为N我来到达观数据参观相应的bigram特征为：我来 来到 到达 达观 观数 数据 据参 参观相应的trigram特征为：我来到 来到达 到达观 达观数 观数据 数据参 据参观注意一点：n-gram中的gram根据粒度不同，有不同的含义。它可以是字粒度，也可以是词粒度的。上面所举的例子属于字粒度的n-gram，词粒度的n-gram看下面例子：我 来到 达观数据 参观相应的bigram特征为：我/来到 来到/达观数据 达观数据/参观相应的trigram特征为：我/来到/达观数据 来到/达观数据/参观n-gram产生的特征只是作为文本特征的候选集，你后面可能会采用信息熵、卡方统计、IDF等文本特征选择方式筛选出比较重要特征bi-gram基于字: il lo ov ve ey yo ou(这个是英文的)–我爱 爱你 你们(中文的)bi-gram基于词:ilove loveyou(这个是英文的)5.2 原因word2vec把语料库中的每个单词当成原子的，它会为每个单词生成一个向量。这忽略了单词内部的形态特征，比如：“apple” 和“apples”，“达观数据”和“达观”，这两个例子中，两个单词都有较多公共字符，即它们的内部形态类似，但是在传统的word2vec中，这种单词内部形态信息因为它们被转换成不同的id丢失了为了克服这个问题，fastText使用了字符级别的n-grams来表示一个单词。对于单词“apple”，假设n的取值为3，则它的trigram有“&lt;ap”, “app”, “ppl”, “ple”, “le&gt;”其中，&lt;表示前缀，&gt;表示后缀。于是，我们可以用这些trigram来表示“apple”这个单词，进一步，我们可以用这5个trigram的向量叠加来表示“apple”的词向量。这带来两点好处： 对于低频词生成的词向量效果会更好。因为它们的n-gram可以和其它词共享。 对于训练词库之外的单词，仍然可以构建它们的词向量。我们可以叠加它们的字符级n-gram向量。5.3 hash存储由于n-gram的分词方法,导致最终形成的词语组合很多,(文档越多,组合也越多),因此为了减小系统查找压力,我们使用Hash存储对应的N-Gram向量 六. FastText与CBOW6.1 不同点 FastText CBOW 输入 文档中的各个词的词向量形式和文档的N-Gram向量(单词的字符级别的n-gram向量作为额外的特征) 文档中各个词的One-hot向量 架构 输入层—&gt;隐藏层–&gt;多层次softmax 输入层—&gt;隐藏层—&gt;softmax层 输出 输出是文档的类别/类标 输出的是目标词汇 隐藏层 直接叠加平均向量 上下文单词向量叠加起来并经过一次矩阵乘法（线性变化）并应用激活函数 训练时间 较快 时间较长 6.2 相同点 都是三层结构; 都需要对文档中各个词向量进行叠加(词袋模型)七.使用场景在短文本分类任务、实体识别消歧任务、同义近义简称别名挖掘任务、推荐系统中的文本向量化特征提取等等。实践经验表明，fastText更适用于样本数量大、类别标签多的任务，一般能够得到很好的效果，大多数情况下强于传统的BOW + LR/SVM分类器。更重要的是，训练效率非常之高。八.参考文献fastText，智慧与美貌并重的文本分类及向量化工具FastText的demo实现及其评估fastText原理及实践fastText，智慧与美貌并重的文本分类及向量化工具N-gram特征，浅谈FastText文本分类利器解读--重点理解","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"FastText","slug":"FastText","permalink":"http://yoursite.com/tags/FastText/"},{"name":"层次化softmax","slug":"层次化softmax","permalink":"http://yoursite.com/tags/层次化softmax/"},{"name":"N-Gram","slug":"N-Gram","permalink":"http://yoursite.com/tags/N-Gram/"}]},{"title":"MLE,MAP,贝叶斯估计,N-Gram","slug":"MLE,MAP,贝叶斯估计,N-Gram","date":"2019-02-25T13:17:34.000Z","updated":"2019-02-26T05:47:06.280Z","comments":true,"path":"2019/02/25/MLE,MAP,贝叶斯估计,N-Gram/","link":"","permalink":"http://yoursite.com/2019/02/25/MLE,MAP,贝叶斯估计,N-Gram/","excerpt":"本文主要介绍了MLE,MAP与朴素贝叶斯,贝叶斯估计,N-Gram几种方法,一直以来,这些基本问题困扰着我,今天趁着有时间,赶紧把这些似是而非的问题表达清楚.MLE是先验分布为均匀分布的MAP,朴素贝叶斯是特征之间相互独立的MAP.贝叶斯估计则是需要求出某种具体分布,从而求出对应模型参数的期望$P(θ)$","text":"本文主要介绍了MLE,MAP与朴素贝叶斯,贝叶斯估计,N-Gram几种方法,一直以来,这些基本问题困扰着我,今天趁着有时间,赶紧把这些似是而非的问题表达清楚.MLE是先验分布为均匀分布的MAP,朴素贝叶斯是特征之间相互独立的MAP.贝叶斯估计则是需要求出某种具体分布,从而求出对应模型参数的期望$P(θ)$ 一.总结 二.MLE与MAP 2.1 不同点 2.2 联系 三.MLE 3.1 似然和概率 3.2 主要思想 3.3 一般求取步骤 3.4 例子 四.MAP与朴素贝叶斯 4.1 形式 4.2 朴素贝叶斯 五.贝叶斯估计 六.结论 七.N-Gram 7.1 参数空间过大 7.2 数据稀疏 7.2.1 Add-one 7.2.1 Add-K 7.3 用途 7.3.1 词性标注 7.3.2 分词器 八.参考文献 一.总结 最大似然估计,最大后验概率和贝叶斯估计都是模型已知,参数未知的情况,所谓模型已知:指的是数据服从某种具体的分布(里面隐藏着模型),但是模型的参数及其何种分布形式都是未知的,所以就叫做模型已知,参数未知.只不过三者求出模型参数的准确率是不一样的. 其中最大似然估计和最大后验概率都只需求出P(θ)等于多少即可,而不用真正求出模型参数θ具体服从某种分布,但是贝叶斯估计若想求出P(θ),则需要求出参数θ具体属于某种分布,之后求出模型参数的期望,也就是P(θ)这个模型参数的分布的均值,也就是在MAP当中的先验概率,说明一下:对于先验概率,我们可以理解为通常发生的概率,因此我们可以用均值/期望这一个指标来说明即可.正因为贝叶斯估计能够求出具体模型参数所属的分布,因此得到的模型参数的期望更接近实际情况,也就是更精确. 二.MLE与MAP2.1 不同点 MLE MAP 形式: $argmax P(x_1,x_2,…x_n \\theta)$ $\\large argmax P(\\theta x_1,x_2,…x_n)=\\frac{P(x_1,..x_n \\theta)P(\\theta)}{P(x_1,x_2…x_n)}$ 特点: 模型完全以数据为导向, 加入了对模型参数的先验分布P(θ) 模型参数分布是否已知: 不知 已知,就是P(θ),例如:假设模型参数服从高斯分布, 准确率 初期来看,数据量小的时候,准确率最差,并且容易将错误的点也作为训练模型的数据,数据增多之后,准确率会上升. 当数据量小的时候,由于已经设定了模型参数的分布,因此可以将样本当中不符合模型训练的点当成噪点,提高准确率,但是随着数据量的不断增大,模型的参数会越来越接近正确的分布,此时MAP也会接近MLE 使用场景 数据量比较大的时候 中小型的数据量 缺点 MLE 只考虑训练数据拟合程度没有考虑先验知识，把错误点也加入模型中，导致过拟合 2.2 联系 最大似然估计*先验分布=最大后验概率 MAP P(θ|x)=θ的后验概率=θ的先验概率$P(θ)$＊标准似然度（调整因子） MLE是先验分布是均匀分布P(Θ)=1的MAP 说明:（通常，先验概率能从数据中直接分析得到） 三.MLEMaximum Likehood Estimate:最大似然估计3.1 似然和概率以这个函数$P(A|B)$为例 A已知,B未知(前提条件未知–似然,结果已知),说明数据已知,参数未知,描述的是同一批数据在不同模型参数下的概率是多少–似然(参数未知)—本节讲的就是这个 B已知,A未知,说明数据未知,模型参数已知,进一步说明模型已知,因此这个指的是概率(模型参数已知),因此在固定的一个模型下,各个数据点的概率是多少(常见形式) 3.2 主要思想 找到模型参数θ,使得每条样本出现的概率最大! 具体来说假设样本为$x1,x2…..xn$,待估计的参数为θ. 那么要优化的目标为:$argmax P(x_1,x_2,…x_n|\\theta) \\tag 0$假设每个样本间独立同分布那么我们有:$argmax \\prod_{i=1}^n{P(x_i|\\theta)} \\tag 1$后面一般是取对数,把连乘转化成连加的形式更方便计算 3.3 一般求取步骤由上可知最大似然估计的一般求解过程：（1） 写出似然函数；（2） 对似然函数取对数，并整理；（3） 求导数 ；（4） 解似然方程MLE没有任何的先验知识，完全由样本决定，如果样本量比较少或者是不准确的话，则结果没有意义。 3.4 例子 我们拿这枚硬币抛了10次，得到的数据（$x_0$）是：反正正正正反正正正反。我们想求的正面概率θ是模型参数，而抛硬币模型我们可以假设是二项分布(也就是每次抛硬币服从二项分布)。 那么，出现实验结果$x_0$（即反正正正正反正正正反）的似然函数是多少呢？$$f(x_0 ,\\theta) = (1-\\theta)\\times\\theta\\times\\theta\\times\\theta\\times\\theta\\times(1-\\theta)\\times\\theta\\times\\theta\\times\\theta\\times(1-\\theta) = \\theta ^ 7(1 - \\theta)^3 = f(\\theta)$$ 最大化似然函数$$f(x_0 ,\\theta)$:我们对这个函数进行求导,我们可以得到当θ=0.7的时候,使得每条样本出现的概率最大 四.MAP与朴素贝叶斯Maximum a posteriori:最大后验概率 4.1 形式$\\large argmax P(\\theta|x_1,x_2,…x_n)=\\frac{P(x_1,..x_n|\\theta)P(\\theta)}{P(x_1,x_2…x_n)}$与MLE相比,最明显的就是: 添加了先验概率$P(θ)$; $P(x_1,..x_n|\\theta)$是似然函数,也就是最大似然估计; $P(x_1,x_2…x_n)$则是常数项,因为这个在题目中,只要根据频次一般都可以算出来,并且相对各个x来说,这个常数项的值都是一样的,因此,在进行各个x的概率比较时,我们可以省略分母项,因此最大化后验概率就是最大化$P(x_1,..x_n|\\theta)*P(\\theta)$. 4.2 朴素贝叶斯 朴素贝叶斯,最大的特点就是在MAP上,添加了特征之间($x_i$)相互独立的假设,因此朴素贝叶斯最后的形式为:$\\large argmax P(\\theta|x_1,x_2,…x_n)=\\frac{P(x_1,..x_n|\\theta)P(\\theta)}{P(x_1,x_2…x_n)}=\\frac{P(x_1|\\theta)P(x_2|\\theta)….P(x_n|\\theta)P(\\theta)}{P(x_1,x_2…x_n)}$ 但实际生活中这个是不存在的,因此当特征之间的关系独立性不强的时候,分类准确率不高.因此只适用于那些特征之间独立性较强的分类问题. 五.贝叶斯估计 贝叶斯估计则不是让使得最大后验概率最大来求模型参数(也就是不直接估计参数的值,而是求取模型参数的具体分布，之后再求得该模型参数对应分布的期望),而是使用全概率公式:也就是观察到样本X的概率:$$p(X) = \\int p(X | \\theta)p(\\theta)d\\theta$$ 假设模型参数或者先验分布服从β分布,通过求得模型参数的具体β分布,来得到该分布的期望($p(θ)$),而不是直接求模型参数的值$p(θ)$. 因此我们可以推导出,最后贝叶斯估计的模型参数值(期望)的准确率会提高,因为该值是基于整体来考虑的. 说明下: 一般来说P(θ)是先验分布,也就是一般的情况,因此一般的情况,只需要用平均值即可(也就是期望) 六.结论 随着数据的增多，三种方法估计结果相差不大: 少量观测数据下，最大似然结果不准确.具体见下图 说明:红色菱形为最大似然MLE的估计结果黑色雪花为最大后验概率Map的估计结果紫色的实线为贝叶斯估计结果，注意不是一个数，而是一个分布。注意:任何事件都是条件概率。任何事件的发生都会以其他事件的发生为基础。换句话说，条件概率就是在其他事件发生的基础上，某事件发生的概率。 七.N-Gram 上面三种概率求解方法,由于忽略了词序,对应概率精确度不高,以朴素贝叶斯为例:$$p(“在家日赚百万”|J)=p(“在”,”家”,”日”,”赚”,”百”,”万”|J)\\\\=p(“在”|J)p(“家”|J)p(“日”|J)p(“赚”|J)p(“百”|J)p(“万”|J)$$ 上面每一项条件概率都可以通过在训练数据的垃圾短信中统计每个字出现的次数得到，然而这里有一个问题，朴素贝叶斯将句子处理为一个词袋模型（Bag-of-Words, BoW），以至于不考虑每个单词的顺序。例如”我爱你”还是”你爱我”,这种问题.因此需要加入词序,具体方法就是N-Gram,所谓N-Gram就是语言模型,一个基于概率的判别模型，它的输入是一句话（单词的顺序序列），输出是这句话的概率.例如$$p(S)=p(w_1w_2\\cdots w_n)=p(w_1)p(w_2|w_1)\\cdots p(w_n|w_{n-1}\\cdots w_2w_1)$$虽然简单,但是会带来:两大问题 参数空间过大:参数空间过大,$p(w_n|w_{n-1}\\cdots w_2w_1)$就有O(n)个参数. 数据稀疏:因为如果文档很长,如何以tri-gram为组合,就会有很多组合,这些组合在词库当中找不到,因此很多地方概率会为0,导致数据稀疏.7.1 参数空间过大 为此我们引入马尔科夫假设,让一个词的出现仅与它之前的若干个词有关: 如果一个词的出现仅依赖于它前面出现的一个词，那么我们就称之为 Bi-gram：$$p(S)=p(w_1w_2\\cdots w_n)=p(w_1)p(w_2|w_1)\\cdots p(w_n|w_{n-1})$$ 如果一个词的出现仅依赖于它前面出现的两个词，那么我们就称之为 Tri-gram(效果最好)$$p(S)=p(w_1w_2\\cdots w_n)=p(w_1)p(w_2|w_1)\\cdots p(w_n|w_{n-1}w_{n-2})$$ N-gram的 N 可以取很高，然而现实中一般 bi-gram 和 tri-gram 就够用了。 那么问题来了,如何求取式子里面的概率,方法就是MLE,具体例子如下: 以Bi-gram为例，我们有这样一个由三句话组成的语料库： 容易统计，“I”出现了3次，“I am”出现了2次，因此能计算概率: $$p(am|I)=\\frac{2}{3}$$,以此类推 $$\\begin{matrix}p(I|)=0.67 &amp; p(Sam | am)=0.5 &amp; p(|Sam)=0.5 \\ p(do|I)=0.33 &amp; p(not|do)=1 &amp; p(like|not)=1\\end{matrix}$$ ，$p(I|)=0.25，p(|food)=0.68$，于是：$p(I want chinese food)\\\\=0.25\\times 0.33\\times 0.0065 \\times 0.52 \\times 0.68=1.896\\times 10^{-4}$ 我们算出了“I want chinese food”这句话的概率，但有时候这句话会很长，那么概率（都是小于1的常数）的相乘很可能造成数据下溢（downflow），即很多个小于1的常数相乘会约等于0，此时可以使用log概率解决。7.2 数据稀疏 使用拉普拉斯平滑,7.2.1 Add-one 拉普拉斯平滑，即强制让所有的n-gram至少出现一次，只需要在分子和分母上分别做加法即可。这个方法的弊端是，大部分n-gram都是没有出现过的，很容易为他们分配过多的概率空间。$$p(w_n|w_{n-1})=\\frac{C(w_{n-1}w_n)+1}{C(w_{n-1})+|V|}$$7.2.1 Add-K 在Add-one的基础上做了一点小改动，原本是加一，现在加上一个小于1的常数K。但是缺点是这个常数仍然需要人工确定，对于不同的语料库K可能不同。$$p(w_n|w_{n-1})=\\frac{C(w_{n-1}w_n)+k}{C(w_{n-1})+k|V|}$$7.3 用途7.3.1 词性标注 我们可以将词性标注看成一个多分类问题，按照Bi-gram计算每一个词性概率：$$p(词性_i|”龙龙”的词性, “爱”)=\\frac{前面是“名词”的“爱”作为词性_i 的出现次数}{前面是”名词”的”爱”的出现次数}$$ 选取概率更大的词性（比如动词）作为这句话中“爱”字的词性。7.3.2 分词器$X=”我爱深度学习”$三种分类方案:Y1,Y2,Y3$Y1={“我”,”爱深”,”度学习”}$$Y2={“我爱”,”深”,”度学”,”习”}$$Y3={“我”,”爱”,”深度学习”}$$p(Y1)=p(我)p(爱深|我)p(度学习|爱深)$$p(Y2)=p(我爱)p(深|我爱)p(度学|深)p(习|度学)$$p(Y3)=p(我)p(爱|我)p(深度学习|爱)$取概率最大的作为分类方案.八.参考文献MLE与MAP最大后验概率朴素贝叶斯N-Gram最大似然估计，最大后验估计，贝叶斯估计联系与区别MLP,MAP,贝叶斯估计在NLP中参数估计自然语言处理NLP中的N-gram模型","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[]},{"title":"PCA原理及其步骤","slug":"PCA原理及其步骤","date":"2019-02-25T03:42:35.000Z","updated":"2019-02-26T05:46:23.900Z","comments":true,"path":"2019/02/25/PCA原理及其步骤/","link":"","permalink":"http://yoursite.com/2019/02/25/PCA原理及其步骤/","excerpt":"本文主要是讲述PCA的原理及其计算步骤,主要是对之前的SIF算法理解的补充,PCA主要目的还是降维,重点把握协方差的构建及其优化.其亮点就是使用协方差矩阵进行说明.","text":"本文主要是讲述PCA的原理及其计算步骤,主要是对之前的SIF算法理解的补充,PCA主要目的还是降维,重点把握协方差的构建及其优化.其亮点就是使用协方差矩阵进行说明. 一.PCA原理 二.算法总结 三.参考文献 一.PCA原理 例子:当数据点在二维坐标中,呈现某种相关性的时候,此时某一个维度就是多余的,因此我们需要去掉一个维度,那么我们该如何去掉一个维度,标准就是根据数据点在某点的分布情况(数据点在某个方向上的投影),投影结果越分散,该维度/方向保留的可能性越大,主要实现手段就是构建协方差矩阵说明样本在不同维度的分布情况(对角线上的特征值)和各个维度之间的相关性. 通过计算协方差矩阵的特征值,来得到数据在某个方向的离散程度,对应特征向量则是代表该方向. 那么协方差矩阵该如何得到,主要是通过样本矩阵X得到,具体公式为:$C=\\frac{1}{n} XX^T$ 说明:协方差矩阵的对角线代表样本在各个维度上的离散程度;其余元素则是代表不同维度间的关系. 之后我们优化协方差矩阵,最好将其转换为对角矩阵,使得对角线上的值不为0,但是对角线两侧为0,也就是使得,维度间的相关性为0,使得数据在前几个特征值所对应的方向上的分布程度最广, 然后使用前几大特征值对应的特征向量构建投影矩阵(特征向量乘上自身的转置),作为新的维度空间,基底则是前几大特征值对应的特征向量,相互正交. 然后样本数据乘上投影矩阵从而实现降维. 二.算法总结设有m条n维数据，这里比较糊涂就是按行组织样本还是按列组织样本，下面是按行组织样本：1）将原始数据按行组成n行m列矩阵X，代表有n个数据，每个数据m个特征2）将X的每一列（代表一个属性字段）进行零均值化，即减去这一列的均值3）求出协方差矩阵$C=\\frac{1}{n} XX^T$4）求出协方差矩阵的特征值及对应的特征向量5）将特征向量按对应特征值大小从上到下按列排列成矩阵，取前k列组成矩阵P6）Y=XP即为降维到k维后的数据按列组织是这样的，理解一下：1）将原始数据按列组成n行m列矩阵X2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值3）求出协方差矩阵C=1/m*XXT4）求出协方差矩阵的特征值及对应的特征向量5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P6）Y=PX即为降维到k维后的数据 三.参考文献主成分分析（PCA）与Kernel PCA","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"PCA","slug":"PCA","permalink":"http://yoursite.com/tags/PCA/"},{"name":"协方差","slug":"协方差","permalink":"http://yoursite.com/tags/协方差/"},{"name":"降维","slug":"降维","permalink":"http://yoursite.com/tags/降维/"}]},{"title":"SIF算法解析","slug":"SIF算法解析","date":"2019-02-25T03:02:04.000Z","updated":"2019-02-26T05:58:42.720Z","comments":true,"path":"2019/02/25/SIF算法解析/","link":"","permalink":"http://yoursite.com/2019/02/25/SIF算法解析/","excerpt":"本文主要讲述的是SIF算法的主要思想,论文为:”A SIMPLE BUT TOUGH-TO-BEAT BASELINE FOR SEN- TENCE EMBEDDINGS”,该论文的主要特色之处为:使用α和词频的运算作为词向量的权重,和使用主成分分析,初步句向量-初步句向量的主成分,得到耦合度较低的词向量.","text":"本文主要讲述的是SIF算法的主要思想,论文为:”A SIMPLE BUT TOUGH-TO-BEAT BASELINE FOR SEN- TENCE EMBEDDINGS”,该论文的主要特色之处为:使用α和词频的运算作为词向量的权重,和使用主成分分析,初步句向量-初步句向量的主成分,得到耦合度较低的词向量. 一.主要思想 二.主要步骤 三.数学解释: 四.优缺点 五.SIF算法代码步骤 六.代码实现 七.参考文献 一.主要思想 根据加权平均的思想得到初步句子向量,之后减去所有句子向量的共性(表现为句子向量-句子向量在主成分上的投影),增强句子向量的独立性,增强鲁棒性. 或 模型的输入是一个已有的word embedding，基于该 word embedding 和 sentence s, 通过加权求平均的方法求得sentence s的embedding，然后使用主成分分析去掉一些special direction 二.主要步骤算法如图下: 通过使用对应词的词频$p(w)$和超参数$α$得到单词对应的权重(即$\\frac{a}{p(w)+a}$), 之后进行加权平均(即$v_s\\leftarrow \\frac{1}{|s|}\\sum\\limits_{w\\in{s}}\\frac{a}{p(w)+a}v_w$)得到初步的句子向量$v_s$,(频率越高,权重越低,抑制高频率词) 之后计算语料库所有初步句向量构成的矩阵的主成分,一般使用第一个主成分即可, 之后将初步向量$v_s$投影在主成分向量u中,得到单个句子向量的共性成分(相对于整个所有句子向量来说), 最后初步的句子向量减去对应句子向量的共性成分(起到平滑作用),得到最后的独有的句子向量(使得各个句子向量间的耦合度降低,增强句子的鲁棒性). 说明:一个向量$v$在另一个向量$u$上的投影定义如下$$\\text{Proj}_u v=\\frac{u u^Tv}{\\Vert u \\Vert^2}$$输入：预训练的词向量{$v_w:w∈V$}，例如word2vec、glove等待处理的句子集合 S参数a（论文中建议a的范围： [1e−4,1e−3])词频估计{$p(w):w∈V$}输出： 句子向量{$v_s:s∈S$} 三.数学解释: 主要是依据概率论来进行解释:一个单词w在句子s中出现的频率是由单词的词频$p(w)$及其上下文决定的.具体式子如下:$$Pr(\\text{w emitted in sentence s}| c_s)\\propto{\\alpha p(w) + (1-\\alpha)\\frac{\\exp(\\langle \\tilde{c}_s, v_w \\rangle)}{Z_{\\tilde{c}_s}}}$$有两项: 第一项是单词出现的频率项$p(w)$, 第二项是上下文项,根据随机游走模型得到的,其中c_0代表当前句子向量$c̃_s$的主成分.其中$c̃_s=βc_0+(1−β)c_s, c_0⊥c_s$(因为$c_0$⊥$c_s$,这样才能丰富当前句子向量c_s的含义), α和β都是超参数,是归一化常数. 因此一个与当前句子向量$c̃_s$没有关系的单词w, 也可以在句子中出现, 原因有: 来自$αp(w)$项的数值 与当前句子向量的主成分(当前主题)$c_0$相关 说明:随机游走模型:一个相邻单词出现的概率,可以由当前时刻中句子的主成分(也就是所谓的discourse vector)决定的.表现形式为:$Pr(\\text{w emitted at time t}| c_t)\\propto{\\exp(\\langle c_t,v_w \\rangle)}$,內积则表示单词和句子主成分的密切程度(相似程度).$c_t$表示:t时刻句子的主成分,主成分主要是通过PCA降维求得主成分的,一般取第一个主成分即可.$c_0$代表当前句子向量$c̃_s$的主成分,因为$c̃_s$是由$c_0$得到的,因此$c̃_s$包含一定的语义/主题, $c_t$:discourse vector，个人理解为表示当前时刻t的一个背景变量 $c_s$:句子s中对任意时刻t的$c_t$的近似，即所有时刻都一致—截止到当前句子的主旨 $v_w$:单词w的向量表示 四.优缺点 这种句子的建模方式非常高效且便捷。由于这是一种无监督学习，那么就可以对大规模的语料加以利用，这是该方法相比于一般有监督学习的一大优势。 通过对实验的复现，发现运行一次程序只需要十几分钟，并且主要的运行耗时都在将词向量模型载入内存这个过程中，这比动不动就需要训练几周的神经网络模型确实要好很多，并且在这个词相似性任务中，与神经网络旗鼓相当。 缺点就是没有考虑句子的语序,导致不能辨别(“我爱你”还是”你爱我”),优点就是速度快,无监督学习 五.SIF算法代码步骤句子转换成向量的基本步骤: 1.初步计算出句子的向量,存储在sentence_set当中; 2.使用PCA方法对初步的句子向量数组sentence_set进行PCA主成分分析–pca.fit(sentence_set) 3.取PCA当中最大特征值对应的特征向量U,与其本身的转置相乘,得到投影矩阵–uu^T 4.逐个词向量与投影矩阵进行相乘,得到该词向量的主成分–u u^T vs(因为cos0=1,cos90’=0) 5.最终的句子向量=句子向量-该词向量的主成分.减缓主成分所带来的明显波动倾向,从而增强向量的鲁棒性–vs = vs -u u^T * vs 六.代码实现12345678910111213141516171819202122232425262728293031323334353637# sentence_to_vec方法就是将句子转换成对应向量的核心方法def sentence_to_vec(sentence_list: List[Sentence], embedding_size: int, a: float=1e-3): sentence_set = [] for sentence in sentence_list: vs = np.zeros(embedding_size) # add all word2vec values into one vector for the sentence sentence_length = sentence.len() # 这个就是初步的句子向量的计算方法################################################# for word in sentence.word_list: a_value = a / (a + get_word_frequency(word.text)) # smooth inverse frequency, SIF vs = np.add(vs, np.multiply(a_value, word.vector)) # vs += sif * word_vector vs = np.divide(vs, sentence_length) # weighted average sentence_set.append(vs) # add to our existing re-calculated set of sentences################################################# # calculate PCA of this sentence set,计算主成分 pca = PCA() # 使用PCA方法进行训练 pca.fit(np.array(sentence_set)) # 返回具有最大方差的的成分的第一个,也就是最大主成分, # components_也就是特征个数/主成分个数,最大的一个特征值 u = pca.components_[0] # the PCA vector # 构建投射矩阵 u = np.multiply(u, np.transpose(u)) # u x uT # judge the vector need padding by wheather the number of sentences less than embeddings_size # 判断是否需要填充矩阵,按列填充 if len(u) &lt; embedding_size: for i in range(embedding_size - len(u)): # 列相加 u = np.append(u, 0) # add needed extension for multiplication below # resulting sentence vectors, vs = vs -u x uT x vs sentence_vecs = [] for vs in sentence_set: sub = np.multiply(u, vs) sentence_vecs.append(np.subtract(vs, sub)) return sentence_vecs 七.参考文献论文阅读 A SIMPLE BUT TOUGH-TO-BEAT BASELINE FOR SEN- TENCE EMBEDDINGS","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"句子向量","slug":"句子向量","permalink":"http://yoursite.com/tags/句子向量/"},{"name":"主成分","slug":"主成分","permalink":"http://yoursite.com/tags/主成分/"}]},{"title":"微分,导数,梯度的各种基本类型","slug":"微分,导数,梯度的各种基本类型","date":"2019-01-24T07:48:45.000Z","updated":"2019-01-24T08:00:27.450Z","comments":true,"path":"2019/01/24/微分,导数,梯度的各种基本类型/","link":"","permalink":"http://yoursite.com/2019/01/24/微分,导数,梯度的各种基本类型/","excerpt":"本文主要讲述了各类微分,导数,梯度,虽然在深度学习当中,这个知识很基础,甚至不会仔细涉及,但是这个是基础,有利于之后的模型优化,和之前的hessian矩阵和jacobian矩阵,这个是很重要的.","text":"本文主要讲述了各类微分,导数,梯度,虽然在深度学习当中,这个知识很基础,甚至不会仔细涉及,但是这个是基础,有利于之后的模型优化,和之前的hessian矩阵和jacobian矩阵,这个是很重要的. 一.增量 二.微分 三.导数 四.切线 五.偏导数 六.全微分 七.方向导数 八.梯度 8.1 梯度向量 8.2 梯度方向的确定 一.增量 一句话:增量(Δ)是有限小 增量$Δx$是有限小,只是增量的概念,不是无穷小,$dx$则是无穷小,因此只有当$Δx-&gt;0$时(相当于Δx趋近无穷小),$Δx=dx$; 二.微分 一句话:微分(d)是无限小的增量; 从上面我们可以看出,Δx趋近0/无穷小,说明Δx等于dx,正因为Δx趋近0/无穷小,因此对应的Δy也会趋近无穷小/0,等价于dy. d = differentiation,微分是微小的增量，即无穷小量; 三.导数 一句话:导数(f’)就是函数的变化率,无穷小增量间(微分之间)的比值;同时也是微分的系数(式子转换下) 四.切线 一句话:切线:因为上面说了导数是微分的系数,因此函数的微分dy就是切线(记住是函数的微分-dy).例子子请看下图: 从上图我们可以看出,函数的微分dy代表切线(好好理解),无穷小增量dx * 变化幅度因子f’(a)=无穷小增量dy,构成切线.因此微分: (1)在函数意义上是代表函数的变化幅度; (2)在几何意义上代表切线(一元函数为例),而导数则是切线的斜率. 五.偏导数 偏导数(∂):当函数有n个自变量(对应n个坐标轴),对某个自变量求导数(其余自变量视为常数),就是求函数在对应坐标轴上的变化率. 六.全微分 (1)函数意义:当函数有n个自变量,假如n个自变量同时变化,(各变量的变化程度根据各变量的偏导数),函数总的变化量(不是变化率),式子如下图:12f(x,y)=2x+4ydf=2dx+4dy 其中2和4分别是$x,y$的偏导数–代表各个自变量变化的幅度; $dx,dy​$则是–代表变化量(无穷小的增量–相当于一个单位的变化). $+$代表将各个自变量的单位内的变化幅度相加,组成函数单位内的变化幅度. (2)几何意义:对于某点P0=（X0,Y0）,z=f（X,Y）的切平面,全微分要求所有方向的切线均在一个平面内;(根据之前公式4:函数的微分是切线的原理推广而来的)七.方向导数1.函数意义:函数上某个点在不同方向上的导数.1方向导数=偏导数向量*方向单位向量 结合下面两张图来看:2.说明: (1)方向导数:$D_u f(x,y)$是曲面M点上在u方向的导数,也就是M点在u方向上切线的斜率(XOY平面上的P(x,y)点,投射在曲面上点M) (2)偏导数向量:$(∂f/∂x,∂f/∂x)$,函数各个自变量的导数(偏导数)构成的向量—没有方向的; (3)方向单位向量:$u$(若不是单位向量,需要先向量单位化),可以是XOY上P点出发的一个方向u形象解释:以上面几张图为例:1234函数在某个方向的变化率 点乘 u方向的单位向量 = 函数在u方向的变化率(u方向上的导数) 3.几何意义(实质):函数在某个方向的变化率在u方向(u单位向量)上的投影(注意向量顺序) 八.梯度8.1 梯度向量 那么如何找到什么方向使得方向导数最大？ 上文我们之前已经说了,方向导数是切线在方向线上的投影,因此要使得方向导数最大,需要使得u方向与切线的夹角θ为0,这样才会使得cosθ=1,因此也就是当u方向与函数切线方向一致的时候,方向导数最大,此时该偏导数向量也就称之为梯度向量简称梯度,该方向称之为梯度方向,通常梯度方向包含在梯度向量.因此1最大方向导数=|梯度的模| 梯度向量简称梯度,本质就是各个偏导数组成的向量是偏导数向量的一种; 在NN中,通过计算梯度向量就知道往哪个方向下降,因为只需要通过计算梯度的模就知道哪个方向的导数最大.因此我们在NN中,只需要计算梯度,就可以实现损失函数稳步下降.8.2 梯度方向的确定 在等值线上取点p,做等值线,然后作过点p的切向量,之后做一个垂直于切向量的梯度向量(包含梯度方向),我们可以看到如下效果:上图是二维空间的图示,不好理解,我们把它扩展到三维空间上. 紫色的A则是梯度向量(其实是在XOY面上的向量),绿色对应切平面(对应二维空间的切向量),红色代表立体凸函数的俯视图–对应二维空间的等值线,黑色虚线则是梯度向量映射在立体凸函数的方向(好好屡屡),θ则为其他方向的向量与梯度向量的夹角,其余部分为坐标轴.(好好理解,重点) 从三维图中我们可以看到梯度向量映射在立体凸函数的方向的坡度是最大的,因此我们认为梯度向量及其方向的选取是正确的. 从三维图中中我们可以看到梯度向量是垂直于切平面(全微分)","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"微分","slug":"微分","permalink":"http://yoursite.com/tags/微分/"},{"name":"导数","slug":"导数","permalink":"http://yoursite.com/tags/导数/"},{"name":"梯度","slug":"梯度","permalink":"http://yoursite.com/tags/梯度/"}]},{"title":"NN基础补全","slug":"NN基础补全","date":"2019-01-23T01:32:57.000Z","updated":"2019-01-23T01:49:21.240Z","comments":true,"path":"2019/01/23/NN基础补全/","link":"","permalink":"http://yoursite.com/2019/01/23/NN基础补全/","excerpt":"本文主要是对NN的基础知识做进一步的补全,讲述了,激活函数的类型及其作用,参数学习的过程:如何评价–Loss,如何学习–GD–参数作为经纬度,损失值作为高度;对前馈神经网络,可以理解为一个嵌套函数,因此在反向传播的时候需要链式求导和从右向左求导.最后简述了逐层预训练的基本思想,主要目的就是预训练模型的初始化参数,而不是随机初始化一组参数,通过后期的微调,加快学习,主要手段就是通过自编码器.隐藏层学习输入层的另一种表示,输出层则是经可能还原输入层.之后,逐层训练好了之后(分开训练),使用各层对应的参数,作为模型的初始化参数","text":"本文主要是对NN的基础知识做进一步的补全,讲述了,激活函数的类型及其作用,参数学习的过程:如何评价–Loss,如何学习–GD–参数作为经纬度,损失值作为高度;对前馈神经网络,可以理解为一个嵌套函数,因此在反向传播的时候需要链式求导和从右向左求导.最后简述了逐层预训练的基本思想,主要目的就是预训练模型的初始化参数,而不是随机初始化一组参数,通过后期的微调,加快学习,主要手段就是通过自编码器.隐藏层学习输入层的另一种表示,输出层则是经可能还原输入层.之后,逐层训练好了之后(分开训练),使用各层对应的参数,作为模型的初始化参数 一.激活函数 二.参数学习 2.1 评价模型参数 2.2 梯度下降法 三.多层前馈神经网络 四.反向传播 五.逐层预训练 六.参考文献 一.激活函数 激活函数负责将神经元的输入映射到输出端;其中激活函数有离散和连续的激活函数,使用时根据期望的输出来进行选择. 其中离散的激活函数,例如阶跃激活函数(激活值只有不连续的数—类似0,1,2..) 连续的激活函数(激活值在某一个区间,可以为(0,1)或者(-1,1)),有S形的激活函数如logistic函数或者非S形的激活函数–relu函数两者函数如下图:＞可导一定连续,但是连续不一定可导(例如尖的顶点),两边导数符号不一样，S形的激活函数是连续的,在图像上处处可导,这就便于之后使用梯度下降,进行梯度求导,然后实现参数更新,达到参数学习的效果. 二.参数学习2.1 评价模型参数 既然损失函数可以用来评价模型的好坏，那么让损失函数的值最小的那组参数就应该是最好的参数： 在数学上，这是一个无约束优化问题，即调整一组变量使得某个表达式最小（或最大），而对所调整的变量的取值没有限制 解决以上的无约束优化问题，就成了纯粹的应用数学问题。梯度下降法是解决这一类问题的基本方法，也被普遍用于神经网络参数的优化。 2.2 梯度下降法 梯度下降法来实现参数学习,梯度下降法是一种迭代的方法。首先任意选取一组参数，然后一次次地对这组参数进行微小的调整，不断使得新的参数的损失函数更小。会发现这个已经有理论缺陷的算法在实际使用中，有更多的问题限制了它的效果。 这个方法可以这样形象地理解，不妨设需要优化的模型参数只有两个，则参数空间是一个二维的平面。任意一组参数对应于一个损失函数值，这构成第三维。这个三维的空间形成一个曲面，如同高低不平的地形图，经纬度表示模型的参数，高度表示损失函数的值。那么优化问题就是找到高度最低的经纬度。梯度下降法的思路是，首先任意选择一个地点，然后在当前点找到坡度最陡的方向(找的是当前点的方向)（例如一个坡面南低北高，则南北方向坡度大，东西方向坡度小），沿着该方向向下坡方向迈出一小步，作为新的地点进行下次迭代。这样不断地进行迈步，就可以走到一个海拔较低的地方 梯度下降法是一个可以用来处理任何非约束优化问题的方法，但它却不能彻底解决该问题。它最大的不足是无法保证最终得到全局最优的结果，即最终的结果通常并不能保证使损失函数最小，而只能保证在最终结果附近，没有更好的结果。因为梯度下降算法在更新参数的过程中，只利用了参数附近的梯度，对于整个参数空间的趋势，它是没有考虑的。此外，如果真正尝试在计算机上实现梯度下降算法，会发现这个已经有理论缺陷的算法在实际使用中，有更多的问题限制了它的效果,步长需要自己人为设定。例如，步长η的确定，如果太小，算法需要很长时间收敛，如果太大，又无法稳定到参数空间中的某一点。不同的设计会影响算法速度以及最终结果的好坏。但是到目前为止并没有理论上的好办法解决，因此步长的确定就从一个科学问题变成了工程问题。一种方法是让步长随着时间t的推移而变小，即在初期大步走，到后期小步挪。 在以上的图中可以看到：（1）由于步长在开始时过小，图中开始时移动比较慢；（2）更新比较盲目，没有全局的信息，往往是曲线的(重点,好好理解)；（3）后期步长过大，移动过快，在局部最优点周围震荡；（4）最终不能收敛到全局最优点 三.多层前馈神经网络 上面的式子来看,多层前馈神经网络是一个嵌套函数,因此后面进行梯度计算的时候,需要进行链式求导,并且梯度的计算顺序是从里到内(从右向左) 本节以感知器神经网络为对象介绍了深度神经网络，此外基于受限玻尔兹曼机的深度网络也是主流方案.常见的层间连接方式除了全连接外还有卷积（Convolutional Neural Network，CNN）。而层的组织形式除了多层叠之外常见的还有递归形式（Recursive Neural Network，RNN）和循环形式（Recurrent Neural Network，RNN） 四.反向传播 $y=(2x+1)^2$ 其中$z=2x+1$,如果需要求得dy/dx,则需要先求出dz/dx.之后求出dy/dz,最后相乘才是dy/dx,从右向左计算–这就是后向传播算法名字的来历 dy/dx表示的是x变化一个微小量后，y的变化有多大。我们知道x会影响z，然后z再影响y。那么我们就可以先计算x的变化对z的影响有多大，然后计算z的变化对y的影响有多大，两者合起来（相乘）就可以得到x变化一个微小量后，y的变化有多大。 梯度–参数两个–代表经纬度–,损失值代表高度,因此对某个参数求导,就是求该点所在的经度方向或者维度方向,因为BP是批量更新,参数更新之后,相当于有进行了经纬度的更新,高度–损失值,也会重新求得—所以说梯度是有方向的 五.逐层预训练 逐层预训练中心思想就是通过自编码器,来进行逐层预训练,得到 一个比较好的初始化模型参数(权重+误差)–而不是随机初始化一组模型参数,之后在对新的训练样本中,进行参数的微调. 在介绍这种预训练之前，让我们先来了解一种特殊的神经网络——自动编码器（auto-encoder）（Vincent，et al.2008），它是一种只有一层隐层的神经网络，而且训练参数的目标很特殊——让输出尽量等于输入。但这并不是做了一件没有意义的事，因为当我们训练好整个网络的时候，我们在隐层得到了包含输入样本几乎全部无损信息的另一种表示形式，因为通过隐层到输出层的变换，我们能将其完全还原成输入。输入层到隐层的过程称为编码过程，隐层到输出层的过程称为解码过程。 可以想象，如果隐层神经元个数大于等于输入层，则只要简单地将输入层拷贝到隐层就可以达到这样的目的（因为不需要压缩了）。因此，通常隐层神经元个数小于输入层，或者在隐层加入额外的限制(需要压缩,提取)。 在介绍了自动编码器之后，就可以给出一种使用自动编码器逐层训练的深度神经网络预训练方法。 首先，构造一个自动编码器神经网络，输入为深度神经网络的输入，隐层神经元个数为深度神经网络第一个隐层神经元个数。如此训练自动编码器后，记录自动编码器输入层到隐层的连接权重，这些权重会作为深度神经网络输入层到第一个隐层权重的初始值。 这样深度神经网络输入层到隐层的权重被预训练。此时构造另一个自动编码器神经网络，输入层是刚才深度神经网络的第一个隐层（其每个输入样本根据输入样本和刚才预训练的权重计算得到），隐层是深度神经网络的第二个隐层。仍然按照刚才的方式训练自动编码器神经网络，让第二隐层的信号能够尽量还原成第一隐层的信号。如此训练自动编码器后，记录自动编码器输入层到隐层的权重，作为深度神经网络第一个隐层到第二个隐层的权重。 如此逐层重复上面的步骤，直到所有层之间的权重均被初始化。最后用这些初始化后的权重，按照前一节的方式训练整个深度神经网络，这最后一步称为微调（fine tuning）。 经过预训练后得到的神经网络，会明显优于随机给出初始权重训练得到的神经网络。这个现象在其他的研究中也能得到了印证。 如果读者熟悉隐马尔可夫模型，就知道，经典的隐马模型第三个问题就是模型参数的训练问题。使用EM迭代的方法，得到的也是一个局部最优解，而且结果严重依赖于初始值。有一篇论文（Goldberg，et al.2008）的题目是“EM Can Find Pretty Good HMM POS-Taggers（When Given a Good Start）”，就指出了这种模型训练初始值设定的重要性。 一个更为著名的模型是统计机器翻译中的IBM模型。要用随机的初始值直接训练一个效果好的复杂模型的参数几乎是不可能的。这个IBM模型是由一系列由简单到复杂的模型构成的，简单模型均是复杂模型的简化版本，参数也为复杂模型的一个子集。当要训练复杂模型时，先训练简单模型，并用简单模型得到的参数作为训练复杂模型时那部分参数的初始值。 六.参考文献梯度下降法","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"反向传播","slug":"反向传播","permalink":"http://yoursite.com/tags/反向传播/"},{"name":"逐层预训练","slug":"逐层预训练","permalink":"http://yoursite.com/tags/逐层预训练/"}]},{"title":"奇异与特征","slug":"奇异与特征","date":"2019-01-22T08:04:22.000Z","updated":"2019-03-08T02:52:46.900Z","comments":true,"path":"2019/01/22/奇异与特征/","link":"","permalink":"http://yoursite.com/2019/01/22/奇异与特征/","excerpt":"奇异值,奇异值分解,特征值分解,三者的差异需要理解清楚,因为奇异值分解可以针对任何矩阵,因此会对不同维度的矩阵进行维度变换,通常是降维,因此降维的线性变换是不可逆,并且奇异值分解除了投影,还有旋转和伸缩,这些变换,最后还说了奇异值分解的应用领域.","text":"奇异值,奇异值分解,特征值分解,三者的差异需要理解清楚,因为奇异值分解可以针对任何矩阵,因此会对不同维度的矩阵进行维度变换,通常是降维,因此降维的线性变换是不可逆,并且奇异值分解除了投影,还有旋转和伸缩,这些变换,最后还说了奇异值分解的应用领域. 一.概念 二.奇异值分解与特征值分解 2.1 公式 2.2 奇异值分解与降维 2.3 比较 2.3.1 相同点 2.3.2 不同点 三.计算步骤 四.优缺点 五.应用场景 一.概念奇异矩阵是属于方阵的范畴,判断步骤如下: 首先，看这个矩阵是不是方阵,如果不是,就不必判断了; 如果是,此矩阵的行列式|A|是否等于0，若等于0，称矩阵A为奇异矩阵,若不等于0，称矩阵A为非奇异矩阵 奇异矩阵的叫法来自英文singular matrix，我想主要就是因为在求逆的时候会产生奇性(singularity),产生矩阵不可求逆.如果A为奇异矩阵，则AX=0有无穷解(线性相关)，AX=b有无穷解或者无解； 如果A为非奇异矩阵，则AX=0有且只有唯一零解，AX=b有唯一解。 可逆矩阵就是非奇异矩阵，非奇异矩阵也是可逆矩阵,只有方阵才可能可逆，不是方阵的矩阵无从谈他的逆。 奇异值分解可以在任何矩阵上使用,但是奇异矩阵只能是方阵二.奇异值分解与特征值分解2.1 公式 对于任何的一个矩阵，我们要找到一组两两正交单位向量序列，使得矩阵作用在此向量序列上后得到新的向量序列保持两两正交。下面我们要说明的是，奇异值的几何含义为：这组变换后的新的向量序列的长度。 假设A是一个M N的矩阵，那么通过矩阵分解将会得到U，Σ，V’（V的转置）三个矩阵，其中U是一个M M的方阵，被称为左奇异向量，方阵里面的向量是正交的；Σ是一个M N的对角矩阵，除了对角线的元素其他都是0，对角线上的值称为奇异值；V’(V的转置)是一个N N的矩阵，被称为右奇异向量，方阵里面的向量也都是正交的。用图形展示如下图：2.2 奇异值分解与降维 从线性变换角度讲，逆矩阵可理解为原矩阵的反向变换，比如一个向量被顺时针旋转90度，逆矩阵可将其逆时针还原90度。对于没满秩的矩阵会导致线性变换是降维的，想象下3维空间被拍平成2维还能还原吗,既然要降维,肯定不单单是伸缩变换,还需要进行投影(例如3维降到二维),2.3 比较2.3.1 相同点 都是对线性变换(即矩阵)M进行分解–不一定降维，可以提取出主要的特征向量 奇异值分解适用范围更广，而特征值分解只适用于方阵—因为只有拉伸2.3.2 不同点 奇异值分解 特征值分解 适用范围 任何矩阵 方阵 方法 旋转,投影(没满秩),伸缩,旋转矩阵，拉伸矩阵。V*，U都是旋转矩阵，奇异值矩阵(Σ)就是拉伸（尺度）矩阵（在正交方向上的尺度变换 正交基上伸缩 形式 A=U∑V AX=b 是否可逆 不可逆的映射。因为奇异矩阵是不满秩,因此在使用奇异矩阵进行线性变换的时候,需要进行投影来实现降维. 可逆 是否降维 不一定降维 不降维 三.计算步骤该矩阵是如何分解的？奇异值和特征值是如何计算的？ （1）将矩阵A的转置 * A，将会得到一个方阵，将方阵进行特征值分解：其中得到的v，就是右奇异向量。 （2）通过方阵还可以求解σ和u： σ是上文提到的奇异值，u是上文提到的左奇异向量。其中奇异值σ跟特征值很类似，在矩阵Σ中也是从大到小排列，而且σ的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了。也就是说，我们也可以用前r大的奇异值来近似描述矩阵，其中r&lt;&lt;n，这里定义一下部分奇异值分解： （3）选择适当的r，其中r是一个远小于m、n的数，这样可将原矩阵分解为： 其中，右边的三个矩阵相乘的结果将会是一个接近于A的矩阵，而r越接近n，其相乘结果越接近A。根据储存原理，储存量与矩阵面积正相关，因此面积越小占用的储存空间越小。而三个矩阵的面积之和要远小于原矩阵。因此如果要储存A的信息，只需要储存U、Σ、V就可以，因此信息得到压缩。 四.优缺点 优点：可以简化数据，压缩维度，去除数据噪音，提升算法的结果，加快模型计算性能，可以针对任一普通矩阵进行分解（包括样本数小于特征数），不受限于方阵。 缺点：转换后的数据比较难理解，如何与具体业务知识对应起来是难点。 五.应用场景(1)隐性语义索引（Latent Semantic Indexing，LSI） 矩阵是有文档（M行）和词语（N列）组成，通过奇异值分解，可以分析出那些文档或词语属于同一主题或概念，可应用于更高效的文档检索(2)推荐系统 通过奇异值分解，可以计算项与人之间的相似度，而进行协同过滤，向用户推荐相关产品","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"奇异值分解","slug":"奇异值分解","permalink":"http://yoursite.com/tags/奇异值分解/"},{"name":"特征值分解","slug":"特征值分解","permalink":"http://yoursite.com/tags/特征值分解/"}]},{"title":"病态问题与正则化","slug":"范数与正则化","date":"2019-01-22T07:20:10.000Z","updated":"2019-01-22T08:19:31.000Z","comments":true,"path":"2019/01/22/范数与正则化/","link":"","permalink":"http://yoursite.com/2019/01/22/范数与正则化/","excerpt":"本文主要讲述了正则化的来源,形式,和从先验分布的角度看正则化,其中由于模型的病态问题,带来对系数矩阵求逆很困难,并且常常不容易求得解.因此使用损失函数的方法来避免对模型系数矩阵的求逆,结合使用正则化函数,加快梯度下降,最终实现求得近似解.","text":"本文主要讲述了正则化的来源,形式,和从先验分布的角度看正则化,其中由于模型的病态问题,带来对系数矩阵求逆很困难,并且常常不容易求得解.因此使用损失函数的方法来避免对模型系数矩阵的求逆,结合使用正则化函数,加快梯度下降,最终实现求得近似解. 一.模型的病态问题 1.1 评价模型 1.2 求解模型参数 二.各种正则化 三.先验角度解释正则化 四.参考文献 模型优化有两大难题，一是：局部最小值，二是：ill-condition病态问题。 一.模型的病态问题1.1 评价模型 病态问题是指输出结果(预测结果)对输入数据(样本输入)非常敏感的问题，即输入数据中微小的误差可能引起输出结果很大的变化，一般用条件数(Condition Number)来衡量模型的病态指标，条件数越大，模型病态程度越重。当模型表达成Ax=b时，该模型的条件数就是模型系数A的最大与最小特征值的比值，条件数大时称为ill-conditioned–远远大于1时，条件数小时称为well-conditioned–一般接近1时,说明模型越可信,稳定性好,鲁棒性强,敏感度低. 病态问题的本质是构成矩阵的行向量与所对应的超平面之间存在小角度，或者所交子集之间存在小角度，就会导致矩阵病态,也就是矩阵行向量不全部是正交基(也说明矩阵不满秩),导致有些行向量与正交基所形成的空间不平行,产生夹角.–其实就是矩阵不可逆,有多余的行向量.(方阵也有可能),所以会可能是一个病态问题,病态问题是问题本身的性质，只能避免解病态矩阵。模型系数矩阵A的条件数K具体式子如下: 该式子主要是用来判断模型的可信程度/病态程度–也就是结果变化的倍率.当上面式子时奇异的时候,条件数是无穷大的,因为模型的权重系数矩阵与对应的空间有夹角,导致 条件数的求解,其实就是需要对模型的权重系数矩阵A求逆,但是有很多时候,求逆不怎么好求(难度过大),并且不一定有逆(矩阵不一定满秩). 1.2 求解模型参数 传统的模型参数的求解方法:以模型AX=y来说明 通过求逆得到模型的参数解,但是这样做难度非常大,效率也低,并且当我们的样本X的数目比每个样本的维度还要小的时候，矩阵$X^TX$将会不是满秩的,那么矩阵$X^TX$不可逆,就会有无穷多个解.这样就会导致模型参数最优解W无解. 因此我们使用另外的方法来避免求得矩阵的逆,我们可以使用代价函数(损失函数求和+正则项)的形式,求接近最优解的解:(以L2正则化为例)只有正则项系数λ&gt;0的时候.才是强凸函数,其余则为普通的损失函数. 一般任务,我们使用的是L2正则化,因为L2正则项是强凸函数(类似抛物线),因此与代价函数结合,就会让代价函数更凸,加快梯度下降,具体含义如下图:也就是把$w$的解限制在黄色区域内，同时使得经验损失尽可能小。 直观来讲：用梯度下降的方法，当w小于1的时候，L2正则项的惩罚效果越来越小，L1正则项惩罚效果依然很大，L1可以惩罚到0，而L2很难（只能接近于0）,：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而如果所有特征中，大部分特征都能起作用，那么使用Ridge也许更合适。 二.各种正则化 L0正则化 L1正则化 L2正则化 意义 模型参数中非零参数的个数 模型参数中非零参数的个数 模型各个参数的平方的和的开方值(也就是模型矩阵的模)zuo 先验分布 拉普拉斯分布 高斯分布 数值计算 将模型参数的最优解与损失函数相交于坐标轴上,让模型矩阵的很多维度都为0,稀疏模型矩阵 将模型参数的最优解与损失函数相交于坐标轴附近,让模型矩阵的很多维度都接近0,L2:实现近似稀疏(最优解时各个权重都接近于0,) 影响 实际上不好操作 实际情况当中,不符合实际情况,且不好操作 ,不知道怎样设置哪些模型参数该为0,不该为0但是有利于特征的选取损失函数中惩罚效果明显,最极端的时候,可以使得模型中的很多权重参数为0 更接近实际情况;抑制权重过大;就是一种普通的规则化 梯度下降 在代价函数–中(损失函数(平方差形式)+正则项),绝对值函数能够一定程度上加快梯度下降 因为是强凸函数,因此能够加快梯度下降的步伐 回归类型 Lasso 回归 岭回归 相同点 缓解过拟合 缓解过拟合 缓解过拟合 形式 三.先验角度解释正则化 给损失函数加入正则项相当于加入了对参数的先验分布，因而能防止过拟合。 其中，L1正则等价于参数w的先验分布满足均值为0的拉普拉斯分布，均值为0的拉普拉斯在0附近突出(小部分地方有概率，之后是两侧概率极速下降)，周围稀疏(其他地方概率为0或者接近0)，对应容易产生稀疏解的模型; L2正则等价于参数w的先验分布满足均值为0的正态分布/高斯分布，均值为0的正态分布在0附近平滑(说明不为0的区域较大,并且由于是平滑,就会导致其他地方的概率不会一下子为0,会慢慢接近于0)，对应容易产平滑解的模型. 1.线性函数可以模拟/近似任何函数,根据泰勒公式原理,只要深度够深.2.范数当中的p,p越大,代表维度越大,正则项函数越接近正方形,p越小,代表正则项函数越接近于坐标轴: 四.参考文献L1正则先验分布是Laplace分布，L2正则先验分布是Gaussian分布范数科学计算-L0、L1与L2范数损失函数与正则项之间的关系和分析什么是 L1/L2 正则化 (Regularization)","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"模型病态","slug":"模型病态","permalink":"http://yoursite.com/tags/模型病态/"},{"name":"正则化","slug":"正则化","permalink":"http://yoursite.com/tags/正则化/"}]},{"title":"范数","slug":"范数","date":"2019-01-22T03:41:08.000Z","updated":"2019-01-22T05:03:39.560Z","comments":true,"path":"2019/01/22/范数/","link":"","permalink":"http://yoursite.com/2019/01/22/范数/","excerpt":"本文主要讲述的是范数的基础知识,涉及到对范数本身的理解,之后讲述了范数的形式和种类,这章的知识和归一化,正则化的关系比较密切,需要重点掌握.","text":"本文主要讲述的是范数的基础知识,涉及到对范数本身的理解,之后讲述了范数的形式和种类,这章的知识和归一化,正则化的关系比较密切,需要重点掌握. 一.范数与向量 二.范数的种类 2.1 P范数的推导: 2.1.1 范数的性质: 2.2 矩阵范数 2.3 关系 三.范数与标准化 一.范数与向量 范数分为向量的范数和矩阵的范数,向量范数代表一个事物在不同维度空间下的长度,矩阵代表一个向量在不同维度上的变化程度.矩阵范数则是代表向量在进行不同维度的空间映射,当中的变化幅度.总之就是向量范数计算长度,矩阵范数计算向量长度/大小的变化幅度. 向量的维数:就是在这个向量在不同维度空间下的表示; 向量的范数:其实是这个向量在不同维度空间下,对应长度的计算方式。 既然能求长度,就一定可以比较,从空间上,我们可以比较向量之间的大小–&gt;进而推出向量之间的距离–&gt;求相似性; 本节以向量范数为例,向量代表一个事物,向量的维度代表这个事物在不同维度空间的表示,那么如何度量向量的大小/长度,那么我们引出了范数,范数就是计算向量长度的规则/公式,在不同的维度空间中,计算向量长度的方法也是不一样的.下面我们就讨论不同种类范数在不同维度空间下的计算公式(范数) 映射是针对向量与向量之间,在高维空间中不方便描述的变化过程引出的一个概念,而这个映射关系则是用矩阵的形式进行描述,矩阵描述的是向量间的线性映射关系. 二.范数的种类 之前讲到,范数分为向量的范数和矩阵的范数不同种类范数,不同种类范数在不同维度下的具体范数公式是不一样的,但是遵循基本的范数公式–P范数.其中p≥1, 2.1 P范数的推导:以向量范数为例，首先看三维向量的2范数 相当于:三维向量在2维空间中进行计算,使用的是2维向量长度计算公式(2范数),也可以使三维向量在3维空间中进行计算,就相当于是使用了3范数,因此我们就可以得到,向量长度的基本计算公式:p代表不同维度空间.P≥1 (1)由于 p=0 ，在数学上一般定义任何实数的开零次方根得等于1，表示的是0维空间的向量长度计算，为了在该空间进行向量长度的计算,我们把0维空间的范数定义为向量中非0元素的个数,简称0-范数： (2)在一维空间中的向量长度计算方式为:向量中各个元素的绝对值之和.–所在空间是曼哈顿空间,计算出的长度是曼哈顿距离,可以用来度量两个向量间的差异，如绝对误差和. (3)在二维空间中的向量长度计算方式为:向量的模–各元素平方之和在开方.所在的空间的是欧几里得空间,计算出的长度是欧几里得距离–平方差函数是强凸函数(类似抛物线)具体化为: (4)在无穷维度空间上的,向量长度的计算需要分成两种情况:-计算的向量长度就是切比雪夫距离/棋盘距离. (a)∞/正无穷范数，所有向量元素中的最大值。 (b)−∞/负无穷范数，所有向量元素中的最小值。 说明:例如:“当今社会更需要通才还是专才”。通才是一范数二范数比较大，而专才就是无穷范数比较大。,范数是比较向量/矩阵是否“优秀”的一种标准而已, 2.1.1 范数的性质: 非负性–用来计算长度,没有负数. 三角不等性:两边之和大于第三边.–保证长度不会弯曲. 齐次性:常数项为0,次数相同,在特定的变换下保持某种不变性或者和某种规律,看下面的解释: 齐次性:所谓的「齐」，必然是有两个或者以上的对象，那么就以两个对象$x,y$为例齐次，是指所列的式子只和$x^n,y^n$相关，不存在$x^m,y^l(m\\ne n,l\\ne n)$的项，包括常数项也只有0,所以方程可以写成$ax^n+by^n=0$的形式，函数可以写成$f(x,y)=ax^n+by^n$的形式,当$x,y$同时变$N$倍时，即$x\\rightarrow Nx,y\\to Ny$((特定的变换))，方程仍然成立(保持了某种规律)，而函数$f(x,y)\\to N^nf(x,y)$，也就是在特定的变换下保持某种不变性或者规律性，物理上叫做对称性，拥有某种对称性将给我们的工作带来极大的方便 2.2 矩阵范数首先假设矩阵的大小为$m∗n$，即m行n列。 (0)矩阵0-范数是奇异值的和. (1)1-范数：又名列和范数。顾名思义，即矩阵列向量中绝对值之和的最大值–最大列和。$||A||_1=\\max_j \\sum_{i=1}^m|a_{ij}|$ (2)2-范数：又名谱范数，计算方法为$A^TA$矩阵的最大特征值的开平方–最大的奇异值。$||A||_2 = \\sqrt{\\lambda _1}$,其中$λ_1$为$A^TA$的最大特征值。 (3)$∞-​$范数：又名行和范数。顾名思义，即矩阵行向量中绝对值之和的最大值–最大的行和。$$||A||_{\\infty}=\\max_j \\sum_{i=1}^n|a_{ij}|​$$ (4)F-范数：Frobenius范数，计算方式为矩阵元素的绝对值的平方和再开方(联想到行列式)–奇异值平方和的开方。 2.3 关系向量范数是矩阵范数的特例.以向量2范数为例: 说明:矩阵2范数等于$\\sqrt{\\lambda_{\\max}(x^\\top x)}$，注意到$x^\\top x$是一个数，你也可以认为它是一个$1\\times 1$的矩阵，因此根据特征值得定义，它唯一的特征值就是它本身，因此x对应的矩阵2范数也等于$\\sqrt{x^\\top x} $ 矩阵的內积(转置与本身相乘)为向量中各个元素的平方之和再开方; 向量也是矩阵(例如:N * 1); 三.范数与标准化L2范数标准化上图所示,L2归一化过程:其实就是x本身/2范数","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"向量范数","slug":"向量范数","permalink":"http://yoursite.com/tags/向量范数/"},{"name":"矩阵范数","slug":"矩阵范数","permalink":"http://yoursite.com/tags/矩阵范数/"}]},{"title":"归一化与标准化","slug":"标准化与正则化","date":"2019-01-15T09:38:18.000Z","updated":"2019-01-22T01:19:03.060Z","comments":true,"path":"2019/01/15/标准化与正则化/","link":"","permalink":"http://yoursite.com/2019/01/15/标准化与正则化/","excerpt":"本文主要讲述的是标准化与归一化的区别,相同点和联系,重点讲述各自的使用场景,归一化主要是应用于没有距离计算的地方上,标准化则是使用在不关乎权重的地方上,因为各自丢失了距离信息和权重信息,最后还讲述了下归一化的使用场景,主要是针对数据分布差异比较大–标准化和奇异数据(单个数据对结果有影响的话)–归一化的情况下.使用","text":"本文主要讲述的是标准化与归一化的区别,相同点和联系,重点讲述各自的使用场景,归一化主要是应用于没有距离计算的地方上,标准化则是使用在不关乎权重的地方上,因为各自丢失了距离信息和权重信息,最后还讲述了下归一化的使用场景,主要是针对数据分布差异比较大–标准化和奇异数据(单个数据对结果有影响的话)–归一化的情况下.使用 一.不同点 二.相同点及其联系 三.归一化(广义)的场景 3.1 特征/数据需要归一化的场景 3.2 不需要归一化的场景 四.归一化(狭义)注意事项: 4.1 归一化的方法 4.1.1 小数定标标准化 4.1.2 softmax对数归一化 4.1.3 L2归一化: 4.2 指标衡量与权重保留 4.3 归一化的使用前提 五.标准化的过程 六.参考文献 一.不同点 标准化 规约化 概念 将数值规约到(0,1)或者是(-1,1)区间 将对应数据的分布规约在均值为0,标准差为1的分布上(近似高斯分布) 侧重点 数值的归一,丢失数据的分布信息,对数据之间的距离没有得到较好的保留,但保留了权值 数据分布的归一,较好地保留了数据之间的分布,但是数据对于的权重没有得到保留,因为规约化的分母是标准差,而标准差的大小也间接代表着权重的大小,(具体查看下面的说明.标准差与权重的关系),这就导致规约之后的分布,各个数据/样本的权重是平等的,但是保留样本之间的距离. 形式 :依赖所有的样本数据 :依赖所有的样本数据或者简单形式::是动态使用的,只依赖当前的数据x,x越大证明x∗ 越小，这样就可以把很大的数规范在[0-1]之间了。. 缺点 1.丢失样本间的距离信息2.鲁棒性较差:max为样本数据的最大值，min为样本数据的最小值,当有新的样本加入的时候,最大值与最小值非常容易受异常点影响,因此适合小/固定领域数据场景(例如:奇异样本数据) 1.丢失样本间的权重信息:分母是间接代表权重的标准差. 适合场景 1.小数据/固定数据的时候使用2.在不涉及距离度量、协方差计算、数据不符合正太分布的时候.3.进行多指标综合评价的时候 1.在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，标准化方法表现更好。–大样本(鲁棒性更好,)2.有较好的鲁棒性,有超出取值范围的离散数据 (也有可能是单个指标当中的数据) 或 对最大值或者最小值未知的情况—使用Z-Score的简单形式.–大数据 缩放方式 先使用最小值平移,后使用最值差缩放 先使用均值μ平移,之后用标准差σ进行缩放 目的 便于消除量纲,将各个指标的数据纳入到综合评价中 便于后续的梯度下降和激活函数对数据的处理.因为标准化后,数据以0为中心左右分布(不一定对称),而函数如Sigmoid、Tanh、Softmax等也都以0为中心左右分布（不一定对称） 标准差与权重:某个指标数据对应的数据集标准差过大,说明其不确定性增加,所提供的信息量也会增加,因此在进行综合指标评价的时候,权重也会对应的增大.—-类似熵权法 二.相同点及其联系1.联系:归一化广义上是包含标准化的,Z-Score方法也是归一化的方法之一,在这里主要是从狭义上,区分两者 2.本质上都是进行特征提取,方便最终的数据比较认识.都通过先平移(分子相减)后缩放(分母)进行进行提取; 3.都是为了缩小范围.便于后续的数据处理. 4.作用:(重点) 加快梯度下降,损失函数收敛;—速度上 提升模型精度–也就是分类准确率.(消除不同量纲,便于综合指标评价,提高分类准确率)—质量上 防止梯度爆炸(消除因为数据输入差距(1和2000)过大,而带来的输出差距过大(0.8,999),进而在 反向传播的过程当中,导致梯度过大(因为反向传播的过程当中进行梯度计算,会使用的之前对应层的输入x),从而形成梯度爆炸)—稳定性上 说明:特征缩放其实并不需要太精确，其目的只是为了让梯度下降能够运行得更快一点，让梯度下降收敛所需的循环次数更少一些而已 三.归一化(广义)的场景 除非本来各维数据的分布范围就比较接近，否则必须进行标准化，以免模型参数被分布范围较大或较小的数据支配 数据分布差异比较大–标准化和奇异数据(单个有影响的也要)–归一化 3.1 特征/数据需要归一化的场景 logistic regression模型:逻辑回归,虽然迭代若几次没有影响,但实际当中远不止若干次,这样就会导致逻辑回归模型的目标函数过于扁化,导致梯度很难下降,不容易得到较好的模型参数. SVM模型:因为涉及到向量/数据的距离(向量之间差异过大/过小,就会导致最佳分离超平面可能会由最大/远或者最小/近的几个向量支配,导致鲁棒性较差,因此需要进行标准化—可以保留向量间的模型) NeuralNetwork模型:初始输入值过大,反向传播时容易梯度爆炸(上面有解释) SGD:加快梯度下降. 3.2 不需要归一化的场景 0/1取值的特征通常不需要归一化，归一化会破坏它的稀疏性 决策树 基于平方损失的最小二乘法OLS不需要归一化(因为本质上是一个抛物线,强凸函数,下降速度快.) 四.归一化(狭义)注意事项:4.1 归一化的方法4.1.1 小数定标标准化​ 这种方法通过移动数据的小数点位置来进行标准化。小数点移动多少位取决于属性A的取值中的最大绝对值。将属性A的原始值x使用decimal scaling标准化到x’的计算方法是： ​ $x’=\\frac{x}{10^{j}}​$,其中，$j​$是满足条件的最小整数。 ​ ​ 例如 假定A的值由-986到917，A的最大绝对值为986，为使用小数定标标准化，我们用1000（即，j=3）除以每个值，这样，-986被规范化为-0.986。 ​ 注意，标准化会对原始数据做出改变，因此需要保存所使用的标准化方法的参数，以便对后续的数据进行统一的标准化。 4.1.2 softmax对数归一化$新数据’=\\frac{1}{1+e^{-旧数据}}​$ 4.1.3 L2归一化: 上图所示,L2归一化过程:其实就是x本身/2范数 4.2 指标衡量与权重保留 在归一化中,指标之间其实一般都存在单位的标准问题,例如:我们评价一个人的健康程度,有如下指标,假设一个人身高 180cm，体重 70kg，白细胞计数$7.50×10^{9}/L$,各个量纲都不一样,因此归一化就是消除各个量纲,然后将各个指标结合起来,共同参与到评价健康程度当中,这个就是归一化需要做的事情–消除量纲,便于数据(结合了各个指标的健康程度)/综合评价的比较.—–&gt;因此我们在进行归一化的时候,我们就需要对各个指标的权重进行保留,方便评价. 4.3 归一化的使用前提 在存在奇异样本数据的情况下，进行训练之前最好进行归一化，如果不存在奇异样本数据，则可以不用归一化. 五.标准化的过程即零-均值标准化 $$x_{new}=\\frac{x-\\mu }{\\sigma }​$$ 其中 $\\mu​$ 是样本数据的均值（mean）， $\\sigma​$ 是样本数据的标准差（std）。 上图则是一个散点序列的标准化过程：原图-&gt;减去均值(均值为0–&gt;数据以原点为中心)-&gt;除以标准差 对应到三维图像(以损失函数为例) 机器学习的目标无非就是不断优化损失函数，使其值最小。在上图中，$J(w,b)$就是我们要优化的目标函数,在上图中,我们可以看到,损失函数,未处理之前:梯度的方向就会偏离最小值的方向，走很多弯路,经过标准化处理之后,我们损失函数的曲线也变得比较圆,有利于加快梯度下降,加快找到最佳模型参数.具体如下图: 六.参考文献归一化与标准化-博客园归一化与标准化归一化、标准化区别的通俗说法","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"归一化","slug":"归一化","permalink":"http://yoursite.com/tags/归一化/"},{"name":"梯度下降","slug":"梯度下降","permalink":"http://yoursite.com/tags/梯度下降/"}]},{"title":"为hexo添加mathjax支持","slug":"为hexo添加mathjax支持","date":"2019-01-09T01:48:11.000Z","updated":"2019-01-09T02:58:07.390Z","comments":true,"path":"2019/01/09/为hexo添加mathjax支持/","link":"","permalink":"http://yoursite.com/2019/01/09/为hexo添加mathjax支持/","excerpt":"本次主要是对hexo的mathjax引擎配置进行说明,通过该配置实现对latex公式的支持.便于以后博客的书写.因为之后会涉及到很多算法的部分.需要用到latex.","text":"本次主要是对hexo的mathjax引擎配置进行说明,通过该配置实现对latex公式的支持.便于以后博客的书写.因为之后会涉及到很多算法的部分.需要用到latex.~ 一.添加mathjax 二.自动换行和自动编号 三.忽略换行符号 四.实现公式换行和编号 五.LATEX注意事项 六.参考文献 一.添加mathjax 记住是在博客目录下:npm install hexo-math -save 在博客根目录/themes/icarus/_config.yml下修改,在plugins:中设置mathjax: true 二.自动换行和自动编号123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;% if (theme.plugins.mathjax) &#123; %&gt; &lt;!-- Edit here --&gt; &lt;script type=\"text/x-mathjax-config\"&gt; MathJax.Hub.Config(&#123; &lt;!-- MathJax **处理 HTML/CSS 输出的配置**--&gt; \"HTML-CSS\": &#123; &lt;!--设置字体（preferredFont、availableFonts）--&gt; preferredFont: \"TeX\", availableFonts: [\"STIX\",\"TeX\"], &lt;!--换行（linebreaking）--&gt; linebreaks: &#123; automatic:true &#125;, &lt;!--渲染延迟（EqnChunk）--&gt; EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) &#125;, &lt;!--这部分是 tex2jax.js 预处理程序需要的配置--&gt; tex2jax: &#123; &lt;!--设置为可以使用“公式”或“\\\\( 公式 )\\\\”在行内内联公式--&gt; inlineMath: [ [\"$\", \"$\"], [\"\\\\(\",\"\\\\)\"] ], &lt;!--设置是否允许使用 \\$ 来 escape 一些信息。--&gt; processEscapes: true, &lt;!-- ignoreClass 用于设置具有哪些 css class 的标签不用 tex2jax 预处理。--&gt; ignoreClass: \"tex2jax_ignore|dno\", skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] &#125;, &lt;!--处理 TeX 及相关插件的输入的--&gt; TeX: &#123; &lt;!--加上公式编号和宏的功能（noUndefined）--&gt; equationNumbers: &#123; autoNumber: \"AMS\" &#125;, noUndefined: &#123; attributes: &#123; mathcolor: \"red\", mathbackground: \"#FFEEEE\", mathsize: \"90%\" &#125; &#125;, Macros: &#123; href: \"&#123;&#125;\" &#125; &#125;, &lt;!--属于通用配置，用于控制是否显示加载信息。--&gt; messageStyle: \"none\" &#125;); &lt;/script&gt;&lt;!-- 给MathJax元素添加has-jax class --&gt;&lt;script type=\"text/x-mathjax-config\"&gt; MathJax.Hub.Queue(function() &#123; var all = MathJax.Hub.getAllJax(), i; for(i=0; i &lt; all.length; i += 1) &#123; all[i].SourceElement().parentNode.className += ' has-jax'; &#125; &#125;);&lt;/script&gt;&lt;!-- 通过连接CDN加载最新的MathJax的js代码 --&gt; &lt;%- js('https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML') %&gt;&lt;% &#125; %&gt;&lt;% &#125; %&gt; 三.忽略换行符号 在博客根目录下的/node_modules/marked/lib/marked.js中修改:在第464行改为:1escape: /^\\\\([`*\\[\\]()# +\\-.!_&gt;])/, 即可.忽略换行符号. 四.实现公式换行和编号123456789101112131415161718#第一个公式:实现自定义公式编号:#\\tag&#123;1.2&#125;--常用于单个公式 或者常用于一个式子的计算#\\tag&#123;1.2&#125;只能用于行间,不能用于行内.$$t = r sin(a+b) = r sin(a) cos(b) - r cos(a) sin(b) \\tag&#123;1.2&#125;$$********************************************************************#第二个公式:常用于一个式子的计算.或者单个式子#\\begin&#123;equation&#125;+\\begin&#123;split&#125;与\\end&#123;split&#125;\\end&#123;equation&#125;最好固定的.$$\\begin&#123;equation&#125;\\begin&#123;split&#125;\\frac&#123;\\partial^2 f&#125;&#123;\\partial&#123;x^2&#125;&#125; &amp;= \\frac&#123;\\partial(\\Delta_x f(i,j))&#125;&#123;\\partial x&#125; = \\frac&#123;\\partial(f(i+1,j)-f(i,j))&#125;&#123;\\partial x&#125; \\\\&amp;= \\frac&#123;\\partial f(i+1,j)&#125;&#123;\\partial x&#125; - \\frac&#123;\\partial f(i,j)&#125;&#123;\\partial x&#125; \\\\&amp;= f(i+2,j) -2f(f+1,j) + f(i,j)\\end&#123;split&#125;\\end&#123;equation&#125;$$ 效果如下:第一个公式:$$t = r sin(a+b) = r sin(a) cos(b) - r cos(a) sin(b) \\tag{1.2}$$第二个公式:$$\\begin{equation}\\begin{split}\\frac{\\partial^2 f}{\\partial{x^2}} &amp;= \\frac{\\partial(\\Delta_x f(i,j))}{\\partial x} = \\frac{\\partial(f(i+1,j)-f(i,j))}{\\partial x} \\\\&amp;= \\frac{\\partial f(i+1,j)}{\\partial x} - \\frac{\\partial f(i,j)}{\\partial x} \\\\&amp;= f(i+2,j) -2f(f+1,j) + f(i,j)\\end{split}\\end{equation}$$ 五.LATEX注意事项latex在线地址1“$…$”和“$$…$$”分别代表公式的内嵌和居中展示两种表现形式 加颜色:${\\color{颜色名} {文本}}$ 六.参考文献 MathJax如何理解它的配置代码 在Hexo中渲染MathJax数学公式-码迷 在Hexo中渲染MathJax数学公式-博客园","categories":[{"name":"博客建站","slug":"博客建站","permalink":"http://yoursite.com/categories/博客建站/"}],"tags":[{"name":"mathjax","slug":"mathjax","permalink":"http://yoursite.com/tags/mathjax/"},{"name":"latex","slug":"latex","permalink":"http://yoursite.com/tags/latex/"}]},{"title":"wordembeding技术（二）","slug":"wordembeding技术（二）","date":"2019-01-08T02:18:18.000Z","updated":"2019-01-08T14:36:50.840Z","comments":true,"path":"2019/01/08/wordembeding技术（二）/","link":"","permalink":"http://yoursite.com/2019/01/08/wordembeding技术（二）/","excerpt":"glove模型是基于词频之后结合词向量,然后构建损失函数,求出误差,计算梯度,更新词向量,从而达到训练词向量的目的,速度比之前的SG和CBOW,更快一些.","text":"glove模型是基于词频之后结合词向量,然后构建损失函数,求出误差,计算梯度,更新词向量,从而达到训练词向量的目的,速度比之前的SG和CBOW,更快一些. 一.总结: 二.参考文献 一.总结: glove模型使用共现矩阵来实现全局信息的利用,同时根据从共现矩阵当中的规律,求出共现概率的具体公式,之后与初始化词向量进行损失函ｓｓ数的构建,从中求出误差,之后计算梯度,和更新词向量. 二.参考文献 理解GloVe模型（Global vectors for word representation）","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"glove","slug":"glove","permalink":"http://yoursite.com/tags/glove/"},{"name":"问题转换","slug":"问题转换","permalink":"http://yoursite.com/tags/问题转换/"}]},{"title":"wordmbeding技术（一）","slug":"wordembeing技术（一）","date":"2019-01-07T16:06:39.000Z","updated":"2019-01-08T02:33:43.530Z","comments":true,"path":"2019/01/08/wordembeing技术（一）/","link":"","permalink":"http://yoursite.com/2019/01/08/wordembeing技术（一）/","excerpt":"本文主要是wordmbeding技术（一）的第一篇,主要是讲了一下基于预测的技术类型,其中主要是以SG和CBOW这两种模型为主,其中SG模型又讲了标准的模型和负采样的模型,CBOW模型流程和SG模型一样,只不过将上下文词与中心词进行了对换,最后以表格的形式说明了SG模型与CBOW模型的区别.","text":"本文主要是wordmbeding技术（一）的第一篇,主要是讲了一下基于预测的技术类型,其中主要是以SG和CBOW这两种模型为主,其中SG模型又讲了标准的模型和负采样的模型,CBOW模型流程和SG模型一样,只不过将上下文词与中心词进行了对换,最后以表格的形式说明了SG模型与CBOW模型的区别. 一. 原因 二.wordEmbedding技术 2.1 基本类型 三.基于预测为基准 3.1 Skip-Gram 3.1.1 标准的SG 3.1.2 负采样的SG 3.2 CBOW 3.3 区别 一. 原因one-hot向量(独热向量)局限性: 不能进行词语间的联系性(互斥性与相似性)的计算;因为单词往往是联系的 维度会变得很高,带来很高的计算复杂度.(因为维度大小是词汇表的大小.); 会导致每个单词对应的词向量很稀疏;综上缺点,解决方法:降维,采用分布式向量,让单词的每个维度赋予不同的含义/特征. 二.wordEmbedding技术基本原理: 第1,2两张图上的神经网络模型各自是同一个神经网络模型(好好理解),只不过,便于描述,采用时间点的形式进行说明. 说明:RNN当中的神经网络模型是同一个,只是从不同的时间来看的,RNN用来存储语义序列,有记忆的功能,如下图: 2.1 基本类型 基于统计的(基于词频的):Glove,LSA,VSM(向量空间模型) 基于预测的:Skip-Gram,CBOW(连续词袋模型),使用交叉熵作为损失函数,好处之前讲过,参考”softmax与交叉熵”. 三.基于预测为基准总概括: 3.1 Skip-Gram3.1.1 标准的SG设问句:我现在在北京天安门看升旗仪式.设置窗口为3.上图流程如下: 首先将”北京”作为中心词,根据词表取得其对应的one-hot向量–V * 1, 之后根据与初始化的中心词词嵌入矩阵W–(d V),做点乘,取出北京的中心词词向量–(d 1) 说明:1.词嵌入矩阵W(d * V),d之前的是变量,但是由于经过大量的训练和验证,当我们一般设定d为300维度时候,这个训练效果比较好.2.初始化的方式也有很多种,因此不同方式的初始化的词嵌入矩阵(上下文词和中心词的都要),训练速度也是不一样的,这个要注意.3.一次窗口训练就相当于是训练一个小数据集(不是训练完全部才算一次),正向训练完之后,进行梯度计算和参数更新.4.在SG算法当中,词嵌入矩阵就相当于是神经网络当中的权重矩阵. 之后与初始化的上下文词的词嵌入矩阵W’(V * d)的转置矩阵 做点乘,这步就是用来计算该中心词与词表当中各个词的相似度得分–因为词表当中的所有词的上下文词向量都会保存该矩阵,便于计算,同时也便于后期反向传播更新梯度. 之后使用softmax函数,进行归一化,得到各词与中心词的相似度(用概率表示)–每个列向量的概率之和为1. 最后将预测值与正确值进行比对,我们可以发现,有两个错误的分类, 然后我们采用交叉熵损失函数进行计算(相减即可),得到误差,计算梯度,之后链式求导,更新权重(也就是两个词嵌入矩阵),之后可以人为设定损失函数的阀值,然后完成词嵌入矩阵的训练. 从图中我们可以看到,本次模型的窗口是3,由于篇幅有限,斯坦福只写了左边窗口的,总共有三个单词.3.1.2 负采样的SG 主要变化:损失函数/目标函数从之前的交叉熵函数(softmax-CE)变为最大估计函数(maximum likelihood estimation, MLE). 在标准的SG模型当中,由于使用的是softmax函数来进行概率(相似度)的计算,那我们先看softmax函数形式. 上图已经说明了softmax函数作为概率计算复杂度很高,加之softmax-CE损失函数(标准的SG的损失函数)计算复杂还是比较高(相对于sigmoid-neg-sample),因为在每次进行概率计算的时候,softmax始终拥有∑累加函数,sigmoid-neg-sample损失函数中的sigmoid函数没有累加,只是在最后的损失函数汇总的时候才使用了累加函数∑,(两者都可以实现向量化,只是复杂度). 上面引出了负采样的损失函数,那么我们先来了解:（１）负采样的思想: 在词表当中,随机选取若干个单词作为当前中心词的负例,用来教会模型识别该单词是否是窗口中的词(也就是正例),但是在抽取的时候可能会抽取到窗口中的词,因此针对不同情况,抽取负例的个数是不一样的,为了尽可能抽取到更多的负例,我们:（２）负采样个数的取值: １.在大数据集当中,抽取到窗口中的词的概率非常小,因此抽取次数一般为:5个即可; ２.在小数据集当中,抽取到窗口中的词的概率会比较大,我们抽取的次数就会多一些,一般为:15-20次左右. 抽取之后他的计算流程如下: 3.2 CBOW 连续词袋模型和Skip-Gram模型相反,多个上下文词来预测中心词,输入向量是上下文各个词的词向量(不是上下文词向量)相加做平均,因此就会抹平各个词语之间的差异.其流程图如下图: 输入X={x1k,x2k…..xck}:是one-hot向量,例子如下: 下图第一个权重矩阵W:各个词的词嵌入矩阵,这个是初始化. １.WX进行点乘得到对应词的词向量,也就是隐藏层的输入,之后将这些相同维度的词向量进行相加,取平均数,如下图: 2.W’则是上下文的词嵌入矩阵,为的就是求之前上下文词与目标词的相似度,如下图: 3.之后进行softmax归一化.如下图: 说明:标准softmax归一化,其实不包括最大值归一化,因此以后为了方便,就统一称:softmax最大归一化,这样就包含了. 3.3 区别 SG CBOW 对应类型 一对多 多对一 复杂度 高,因为要一个个单词进行训练,训练次数多 快 学习程度 因为窗口中的上下文词会进行反馈, 相当于多个老师教你知识,学到的东西多,也会比较深 一个目标词反馈给多个上下文词, 相当于一个老师教多个学生,学的不是很深 对生僻字 鲁棒性更好,因为训练速度慢,次数多,反馈多 不好,因为: 1.训练次数少;2.反馈较少;3.输入的时候,对输入向量做了平均,因此就会抹平词语之间语义的差异,没有考虑语序","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"Skip-Gram","slug":"Skip-Gram","permalink":"http://yoursite.com/tags/Skip-Gram/"},{"name":"CBOW","slug":"CBOW","permalink":"http://yoursite.com/tags/CBOW/"}]},{"title":"双层神经网络","slug":"双层神经网络","date":"2019-01-07T11:50:39.000Z","updated":"2019-01-07T11:52:00.000Z","comments":true,"path":"2019/01/07/双层神经网络/","link":"","permalink":"http://yoursite.com/2019/01/07/双层神经网络/","excerpt":"本文主要是对双层神经网络的基本运算流程进行说明,重点在于理解模型参数中的维度是多少,并且各个维度代表的含义,因此需要重点,熟悉把握.","text":"本文主要是对双层神经网络的基本运算流程进行说明,重点在于理解模型参数中的维度是多少,并且各个维度代表的含义,因此需要重点,熟悉把握. 一.双层神经网络运算过程 二.注意事项 一.双层神经网络运算过程 二.注意事项 了解各个模型参数的维度; 了解模型参数中各维度代表什么?总体遵循:行为特征,列为样本;后一层神经元个数(箭头)* 前一层层神经元个数(箭尾).","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"神经网络","slug":"神经网络","permalink":"http://yoursite.com/tags/神经网络/"},{"name":"参数维度","slug":"参数维度","permalink":"http://yoursite.com/tags/参数维度/"}]},{"title":"前后向传播的简单解释","slug":"前后向传播解释","date":"2019-01-07T04:03:35.000Z","updated":"2019-01-07T07:11:44.200Z","comments":true,"path":"2019/01/07/前后向传播解释/","link":"","permalink":"http://yoursite.com/2019/01/07/前后向传播解释/","excerpt":"本文主要是对前后向传播,做一个简单总结,主题思想就是链式求导和梯度平均,同时要注意各个所求参数的维度,方便在代码中实现.","text":"本文主要是对前后向传播,做一个简单总结,主题思想就是链式求导和梯度平均,同时要注意各个所求参数的维度,方便在代码中实现. 一.for循环的形式 二.向量化的形式 三.注意事项 一.for循环的形式 二.向量化的形式 三.注意事项 梯度参数要平均:因为每个样本都有自己的梯度下降方向,那么模型该如何选择一个合适的梯度下降方向呢?最简单的做法就是对样本的各个梯度方向做算术平均(偏差项b也要做平均,让模型选择一个合适的平移方向.); 求解模型梯度参数及其相关参数的维度要搞清:例如各个误差偏导项Z,模型参数W,b的维度要注意.但总体来说的遵循:(输出个数输入个数)或者是(当前层神经元个数 前一层神经元个数).","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"后向传播","slug":"后向传播","permalink":"http://yoursite.com/tags/后向传播/"},{"name":"梯度平均","slug":"梯度平均","permalink":"http://yoursite.com/tags/梯度平均/"}]},{"title":"softmax与交叉熵","slug":"softmax与交叉熵","date":"2019-01-07T02:22:16.000Z","updated":"2019-01-07T02:53:01.450Z","comments":true,"path":"2019/01/07/softmax与交叉熵/","link":"","permalink":"http://yoursite.com/2019/01/07/softmax与交叉熵/","excerpt":"交叉熵作为常用的损失函数,由于其计算的结果较为简单(本文基于softmax函数的交叉熵函数求导),加之其由于其参数的梯度不涉及激活函数的偏导项,能保持较为稳定的的学习速率,学习的速度也比较快,同时交叉熵函数形式有两种,分别用于多分类(softmax函数)和二分类(sigmoid).","text":"交叉熵作为常用的损失函数,由于其计算的结果较为简单(本文基于softmax函数的交叉熵函数求导),加之其由于其参数的梯度不涉及激活函数的偏导项,能保持较为稳定的的学习速率,学习的速度也比较快,同时交叉熵函数形式有两种,分别用于多分类(softmax函数)和二分类(sigmoid). 一.softmax与交叉熵的关系 二.交叉熵的作用 三.交叉熵的优点 四.交叉熵的导数 五.交叉熵损失函数的加快学习 5.1 平方差损失函数的弊端 5.2 交叉熵损失函数的优点 六.交叉熵的类型 一.softmax与交叉熵的关系 softmax函数作为模型最终的预测函数,交叉熵(Cross Entropy,简称CE)作为衡量softmax预测值与真实值的损失函数. 二.交叉熵的作用 交叉熵:衡量目标概率分布(真实分布)与预测分布之间的KL散度(差异) 三.交叉熵的优点 求导结果比较简单，易于计算:最终对最后层的Z(i)的求导值为:A(i)-Y.其中A(i)为最终预测值(也就是Z(i)的激活函数) 解决可以解决二次损失函数的学习缓慢: 因为在交叉熵损失函数中,关于对各个参数的梯度,只与激活函数值和期望输出(目标输出)有关,而与激活函数的偏导项Z(Z作为激活函数的输入/参数)无关,并且输出和期待的输出相差越大，梯度就越大，因此学习速率就会加快.具体参考”交叉熵损失函数的加快学习。”四.交叉熵的导数 从上面我们可以看到，以softmax作为最终分类器，以交叉熵作为损失函数，使得损失函数的求导结果较为简单（虽然推理过程比较复杂）， 有利于反向传播的计算：反向传播是链式求导，由于Z当中包含权重W和偏差b，因此求出关于Z的误差偏导就可以，可以顺藤摸瓜求出权重W和偏置项b的梯度，如果有多层也是这样链式求导。意思如下：２. 我们最终的目的就是求模型的参数（W，b），其余Z这些参数都是为模型的参数（W，b）服务的。３. 反向传播的过程：计算误差，计算相关参数梯度，计算各层模型参数梯度，更新权重。，如果直接把权重的梯度算好了，再上一层的权重就没法更新了（重点理解），因为没有误差作为纽带。五.交叉熵损失函数的加快学习 一句话总结：交叉熵损失函数只与激活函数值sigmoid（z）、和期待输出，两者相差越大，就会使得对于梯度越大，学习速率也就越快。 说明:1.代价函数=目标函数=损失函数(通常是一样的),但是我们把损失函数之和叫作代价函数/目标函数,目标函数有时候也有可能是最大似然函数,(因此不总是最小化–如负采样方式的skip-gram模型的目标函数就是最大化最大似然估计.)2.神经网络学习速度由损失函数分别对权重W和偏置项b的偏导数/梯度决定的,如果学习速度比较慢是因为这两个值比较小 5.1 平方差损失函数的弊端 平方差损失函数:y为期待输出,a为实际输出，x是输入.其对各个模型参数的梯度/偏导为: 从上面我们可以看出,各个模型参数的梯度不仅和:期待输出y、实际输出a有关，同时和激活函数的偏导项有关，那么我们先看sigmoid激活函数的曲线： 从上图我们看出：当输入Z非常大或者非常小的时候，曲线对应的导数就比较平滑，因此对Z的偏导数就会比较小，也会导致偏置项b也比较小（也与激活函数的偏导项有关），因此最后学习速率就会变得很小很小。5.2 交叉熵损失函数的优点 说明：一个函数可以作为一个损失函数，需要具备两大特征：（1）输出非负数：交叉熵：负负为正，是非负的。（2）当输入和期待值非常接近时，函数值接近0。交叉熵：实际输出a和期待输出y非常相近的时候误差函数的结果也接近0 交叉熵函数形式：（二分类类型的，后面会讲） 其中，n是训练的样本数量，y是期待的输出，a是实际输出。导数形式为： 从上面两张图，我们可以看到：CE损失函数，与激活函数值sigmoid（z）、和期待输出，两者相差越大，就会使得对于梯度越大，学习速率也就越快。 六.交叉熵的类型 这两个交叉熵损失函数对应不同的最后一层的输出。第一个对应的最后一层是 softmax，常用于多分类，第二个对应的最后一层是 sigmoid，常用于二分类。","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"交叉熵","slug":"交叉熵","permalink":"http://yoursite.com/tags/交叉熵/"},{"name":"反向传播","slug":"反向传播","permalink":"http://yoursite.com/tags/反向传播/"}]},{"title":"softmax函数的深入理解","slug":"softmax函数理解","date":"2019-01-06T15:28:18.000Z","updated":"2019-01-07T01:04:40.490Z","comments":true,"path":"2019/01/06/softmax函数理解/","link":"","permalink":"http://yoursite.com/2019/01/06/softmax函数理解/","excerpt":"本文是softmax函数的深入理解,主要是对其输入的数据格式和softmax的来源及其进化/改进,以”减去最大值+指数化+归一化”.实现了以较小损失实现了分类,最后说明了softmax的实现,特别要注意axis=0和1的区别,不同数据格式的处理,注意shape的代表的含义.","text":"本文是softmax函数的深入理解,主要是对其输入的数据格式和softmax的来源及其进化/改进,以”减去最大值+指数化+归一化”.实现了以较小损失实现了分类,最后说明了softmax的实现,特别要注意axis=0和1的区别,不同数据格式的处理,注意shape的代表的含义. 一.softmax的输入 二.softmax函数理解 三.softmax常数不变性 四.softmax的作用(重点) 五.axis与shape 六.softmax 的实现过程 一.softmax的输入特征: 1. 输入可以是矩阵或者是向量 2. 都呈现: 样本 * 特征数,行为样本,列为特证数. 二.softmax函数理解 softmax函数常用于最后一层的多分类,其中输入是行向量(在代码上常用行向量)或者是矩阵,但是一般呈现:(样本 * 特征),注意softmax的值是向量,因此常运用于多分类,sigmoid函数是标量(常用于二分类). 上图我们可以看到,这张图含有三个类别倾向,经过一番计算之后,我们计算出了三个特征/类别的得分:cat:3.2 ,car:5.1, frog=-1.7,之后我们进行指数运算,我们可以发现两个作用:负数得分全部变为正数;概率大的类别得分变得更明显了,最后经过归一化,我们可以看到,三个类别的概率,我们观察到有如下特征:三者概率加起来为1,得分最小的frog类别对应的概率竟然为0,这就说明有softmax在进行运算的过程当中,信息损失太大,原因就是:某个类别的指数化后,变化太大,并且极有可能溢出,出现所谓的NAN情况(Not a Number:表示未定义或不可表示的值),同时也会导致最大值与最小值(指数化后)相差太大,加之概率之和是1,因此值小的占的比例非常小,接近于0,使得最终概率接近于0,解决方法就是:借助softmax函数的常数不变性的性质,各个类别和最大值相减,之后在做指数化运算,减少过大的差异,减少损失,防止溢出. 三.softmax常数不变性作用:常数不不变性为之后的最大值归一化提供了数学基础,一定程度上防止了溢出. 四.softmax的作用(重点) 将负数得分变为正数:由于 指数函数的存在; 归一化:将得分/数值转换为概率,且规约到(0,1)区间; 防止溢出:由于softmax的常数不变性,减去最大值,防止指数函数溢出; 突出特征:由于指数函数的存在,导致概率大所对应的类别,在指数作用的加成下会更加明显; 五.axis与shapeaxis:代表横竖方向,shape代表数据格式的尺寸. axis=1代表横轴,横向选择,但不一定是行,这个样搞清楚.;—把1看成大写”一” x.shape[1]:代表横向/向右开始数数的维度. axis=0代表的纵轴取值,竖向选择,不一定是列;把0看成瘦瘦的”0” x.shape[0]:代表竖向/向下开始数数的维度. 六.softmax 的实现过程因为softmax 拥有两种类型的输入:矩阵和向量,输出结果还是向量.(多分类)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576#! python3# -*- coding: utf-8 -*-\"\"\"notice: softmax has two advantage: 1.expand the distance of categories 2.normalization\"\"\"import numpy as npdef softmax(x): # restore the input's information about shape to compare the result's shape orgin_shape=x.shape # shape=1==&gt;it means like the form of (2,), # equally ,it is List,1-dimension Array if len(x.shape)&gt;1: # Firstly,find max per row in matrix to avoid the overflow of number # because the exponential of element in matrix may be very large # so we need maximum normalization, row_max=np.max(x,axis=1).reshape(x.shape[0],1,keepdim=true) # excute maximum normalization x-=row_max # transfer the form of exponential exp_x=np.exp(x) # add eachline element and store new array. row_exp_num=np.sum(exp_x,axis=1).reshape(exp_x.shape[0],1) # caculate the result # notice eachline represents each class x=exp_x/row_exp_num else: # when the shape of array is 1 # it means we needn't consider the axis # choose cloum axis x_vector=np.max(x,axis=0) # maximum normalization x-=x_vector # Convert into the form of exponential,why do this? # to make the distance of class obviously,according to the exponential exp_x=np.exp(x) # the expression make the origin numerical value convert into (0,1). x=exp_x/np.sum(exp_x) # it use to verify the dimension bettween the result and the input.(important) # the input isn't origin word vector,just is Unnormalization classfied result assert x.shape==orgin_shape return xdef test_softmax(): \"\"\" vector test \"\"\" # we can easily know test is 1-dimension array, # according to the amount of `[` test1=softmax(np.array([1,2])) answer_1 = np.array([0.26894142, 0.73105858]) # np.allclose is used to verify test1 and answer_1 # whether it's equal to the tolorance(alias deviation) assert np.allclose(test1,answer_1,rtol=1e-05,atol=1e-06) print(test1) print(answer_1) \"\"\" matrix test 2*2 \"\"\" test2=softmax(np.array([[1001,1002],[3,4]])) answer_2=np.array([ [0.26894142, 0.73105858], [0.26894142, 0.73105858]]) assert np.allclose(test2,answer_2,rtol=1e-05, atol=1e-06) print(test2) print(answer_2)if __name__ == \"__main__\": test_softmax() print(\"for the test the shape:\\n\") # notice the shape's meaning--need understand deeply print(np.shape([ [[1,3]], [[2,3]] ])) 打印输出为:","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"softmax","slug":"softmax","permalink":"http://yoursite.com/tags/softmax/"},{"name":"矩阵","slug":"矩阵","permalink":"http://yoursite.com/tags/矩阵/"}]},{"title":"反向传播与项目经验（五）","slug":"反向传播与项目经验(五)","date":"2019-01-03T13:54:09.000Z","updated":"2019-01-04T12:38:49.450Z","comments":true,"path":"2019/01/03/反向传播与项目经验(五)/","link":"","permalink":"http://yoursite.com/2019/01/03/反向传播与项目经验(五)/","excerpt":"本文主要是讲述反向传播的基本原理,因为这个是非常重要的概念,因此这个要重点掌握,之前虽然有涉及,但是没有对其进行深究,这个是往往不够的,因为这个涉及到以后的模型的优化.只有这样你才知道,以后需要优化什么地方.","text":"本文主要是讲述反向传播的基本原理,因为这个是非常重要的概念,因此这个要重点掌握,之前虽然有涉及,但是没有对其进行深究,这个是往往不够的,因为这个涉及到以后的模型的优化.只有这样你才知道,以后需要优化什么地方. 零.总结 一.反向传播 1.1 前向传播与反向传播的关系 1.2 神经网络注意事项 1.3 任意层的通用公式 1.4 偏差项的梯度 二.反向传播形象解释: 2.1 电路图解释 2.2 流程图解释 三.课程项目建议 本文参考：CS224n笔记5 反向传播与项目指导 零.总结 反向传播的核心特性就是重复使用,一次传播,批量更新参数.其实与正向传播刚好相反,可以参考流程图解释,但是都是链式,只不过一个是链式求导,一个是链式求结果.其中反向传播主要是反向传递误差,从而最终求出梯度,主要的方法就是通过链式来进行传递,加上上层的误差信信号往往是可以进行复用,例如δ^(i),并且不同层的误差信号有专门的推导公式,详见请看反向传播的公式推导,其中的误差信号分成从上层传过来的全局误差信号,和本地的局部误差信号,因此需要求出此处的全局误差信号:将两个误差信号进行点乘即可. 记住,反向传播的最终目的,就是将误差传递到最底层的输入变量–X,各层的权重和偏差.因此反向传播的中间项都是为这些参数服务,只是暂时的值,不是固定的,这点要记住. 一.反向传播1.1 前向传播与反向传播的关系 前向传播用来计算概率，得分之类的情况，主要是用计算结果。 反向传播主要是用来计算梯度； 前向传播为反向传播的计算提供了许多便利,正如上方图中,在The max-margin loss function当中,计算关于权重矩阵U的梯度,a在进行前向传播的过程当中就进行了计算,因此还可以简化计算. 1.2 神经网络注意事项说明: 在上图当中,z是线性函数值,a是激活函数值,上标代表激活函数的层数,下标,代表所在层的第几个数. 不同层数之间对应的权重矩阵的维度/内容可以不等,但是本质上都要为线性函数(W^T * X+b) 不同层之间的激活函数可以是不一样的,一般来说,一个模型当中,使用两个同的激活函数较好. 偏差项b是:便于分类函数的左右调整;在维度空间中,分类函数的方向(分割平面或者分割线)是通过权值W来实现的,因此模型的参数常常会这样(W,b).,从而实现更好,更快的分类.—因为可以既可以通过W来改变分类函数的形状,并且还可以通过偏差项b来实现分类函数的移动/平移. 偏差项的维数也就是下一层神经元的个数,但是每层都有偏差项.如下图:具体参考:神经网络中w,b参数的作用神经网络输入中偏置量bias的作用 b参数的作用，是决定竖直平面沿着垂直于直线方向移动的距离，当b&gt;0的时候，直线往左边移动，当b&lt;0的时候，直线往右边移动W参数的作用:在二个输入中，可以得到w=[w1,w2],令方程w1x1+w2x2+b=0，那么该直线的斜率就是-w1/w2。随着w1,w2的变动，直线的方向也在改变，那么分割平面的方向也在改变,偏差项与x没有直接关系,是模型自带的,方便后期调整.也可以把b放在权重矩阵中,但是为了后期对偏差项梯度的计算,所以单独拆分. s=U^T * α^(T),没有激活函数,但有线性函数(类似WX),主要目的使求得最终分数的步骤简单些.內积. 分类不一定要为概率,也可以为得分.便于识别即可, 使用softmax来进行分类,之后使用交叉熵函数作为代价/目标函数,作为优化的目标. 1.3 任意层的通用公式 上图是将单层的梯度计算推广到多层,变化主要是两个,δ变为上层的误差信号,x变为第二层的输入–激活函数值α^(2),形式上还是:上层误差信号 * 该层的输入. 其中δ的计算公式如下: 推导如下:从上图我们可以看到: 当前层的δ(误差信号)的公式为:上一层传来的W的全局梯度 与 当前层的激活函数的导数 的点乘 就得到当前层的误差信息δ. 当前层的W^(l)参数的梯度为: 上层的误差信号δ(l+1) 与 本层的输入α^(l). 因此最终任意层的通用公式(含正则项)为: 对于顶层来讲，误差就是根据某个损失函数得到的误差。–也就是图中的a^(3)-y,对于底层来讲，激活函数不存在，或相当于f(x)=x。(好好理解,就是那么回事.因为第一层是原始输出)说明: 其中f是激活函数： z是线性函数： 1.4 偏差项的梯度 正则化的损失函数关于第l层偏差项的梯度：其中，a是激活值： 这里偏置单元与普通神经元在数学上并无不同，只不过由于激活值a(l)=1，所以可以把激活值省略掉。 二.反向传播形象解释:2.1 电路图解释 比如函数f(x,y,z)=(x+y)z可视作如下加法器和乘法器电路： 我们可以从输出到输入反向计算，先得到输出关于输出自己的导数：然后得到f关于z的导数：另一条路，f关于q的导数：f关于y的导数：f关于x的导数：这种反向回溯的过程放到神经元中就是反向传播了：反向传播时每通过一级，就用链式法则乘以这一级的导数。另一个稍微复杂一点的例子：其中，sigmoid相关的元件可以合并为一个sigmoid gate：—这个是重点,有时候需要合并某些步骤-节省时间. 2.2 流程图解释 将上述电路视作有向无环流程图去理解链式法则，比如一条路径：2条路径：推广到多条路径：推广到更复杂的流程图：只要找到z的所有父节点应用链式法则并求和即可。神经网络可以视作流程图的一个实例：任意流程图都可以执行反向传播： 三.课程项目建议 把需要完成的任务,用一句话总结–力求简洁; 选择合适的数据集;–可以自己手工弄也可以选择不同任务的专用数据集(建议);否则会很花时间;现成的学术数据集会节省你很多时间,并且已经将对应的 数据集分割好了(比例合适,各个部分内容也是很合理) 更根据数据选择合适的评价指标,而不单单只有准确率、召回率等等，在学术数据集当中都有一些评价指标,可以优先选用这种数据集,节省入门时间.例如:数据集比较杂:F1指标 如果你一开始实验做的很不错:说明任务比较简单,第二个说明,数据集分割并不正确,因此最好的方法就是使用官方数据集,这样可以专注于算法的优化,而不是弄那些所谓的数据集. 建立一个基准模型,首先最好建立一个比较简单的模型,甚至逻辑回归都可以,最重要的是建立一套评判标准.–用代码实现采用哪些评价指标. 简单起步（总结）: 首先设定任务–&gt;熟悉数据–&gt;熟悉评价指标(根据数据来进行评价指标的设定)–&gt;设定一个简单的模型–&gt;代码实现基准–&gt;之后模型根据基准方法,得出来的不足进行调整,改进–&gt;最后用一个已有比较前沿的模型来完成任务.–&gt;思考该模型的不足之处–&gt;之后使用自己的方法进行改进–&gt;没准就能创造一个新模型","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"反向传播","slug":"反向传播","permalink":"http://yoursite.com/tags/反向传播/"},{"name":"误差δ","slug":"误差δ","permalink":"http://yoursite.com/tags/误差δ/"}]},{"title":"词窗分类与神经网络（四）","slug":"窗口分类与神经网络（四）","date":"2019-01-02T13:04:59.000Z","updated":"2019-01-08T02:44:22.790Z","comments":true,"path":"2019/01/02/窗口分类与神经网络（四）/","link":"","permalink":"http://yoursite.com/2019/01/02/窗口分类与神经网络（四）/","excerpt":"传统的机器学习方法本质上来讲还是线性分类,由于现实世界的非线性(复杂性)和局部性,因此在进行分类的时候,需要增加分类函数的非线性,和利用被分类事物的上下文(窗口).因此就引出了上下文的词向量与神经网络的结合实现分类.","text":"传统的机器学习方法本质上来讲还是线性分类,由于现实世界的非线性(复杂性)和局部性,因此在进行分类的时候,需要增加分类函数的非线性,和利用被分类事物的上下文(窗口).因此就引出了上下文的词向量与神经网络的结合实现分类. 一. 总结 二.大纲 三.分类问题 3.1 softmax详细 3.2 交叉熵误差 3.3 softmax与交叉熵 3.4 优化 3.5 re-training词向量失去泛化效果 四.Window classification 4.1 最简单的分类器：softmax 4.2 softmax（等价于逻辑斯谛回归）效果有限 五.使用神经网络 5.1 从逻辑斯谛回归到神经网络 5.2 前向传播网络 5.3 间隔最大化目标函数 六.反向传播训练 6.1 对U求导:–&gt;2 x 3 6.2 对W求导:–&gt;2 x 3 6.3 对偏差b求导: 6.4 对输入X求導: 本人参考:CS224n笔记4 Word Window分类与神经网络 一. 总结本文主要介绍了: 各个模型参数的维度: 输出维度*输入维度; softmax分类器与交叉熵误差的关系:由于交叉熵的目标概率分布是理想分布,one-hot向量存储,这就导致交叉熵等于log形式的softmax分类器; softmax分类器的缺点:只对权重矩阵进行训练,导致模型参数比较少,不能适应大型数据集,同时,输入向量X是固定的,这就导致词向量不能进行实时调整,导致泛化性不强;–函数间隔–几何间隔 词窗分类:将输入向量改为窗口向量,其组成是窗口中各个单词进行拼接起来的向量,也就是列向量(直接增加维度),常常适用于实体识别,分类. 神经网络的优点:①模型参数多,不仅有权重参数,并且还可以调整向量,超参数,只不过,这些都需要通过反向传播进行梯度计算; ②层度深,激活函数赋予非线性特征; 最大间距损失函数:主要是将类别之间的间隔调整到一个合适的范围,便于增强模型的泛化性.实验证明:一般将间隔设置为1就可以,效果比较好.其中正负样的选择,可以根据自己的实际情况来进行选择.本文中选择单位主要是以词窗为基本单位.正确词窗,错误词窗,判别这两个词窗的主要方法就是看中心词是不是地点. 反向传播:最大特点就是重复使用某几项,一般用δ来标记,减少算式复杂度. 优化最大间距损失函数通过反向传播求出了各个模型参数的梯度/导数,因此为了优化the max margin(最大间距损失函数),需要结合各个参数的梯度计算方法,从而调整各个参数,实现损失函数的最优化,最好趋近于0. 二.大纲本节主要是从以下方面来进行叙述: 分类问题背景 在分类任务中融入词向量 窗口分类和交叉熵误差推导技巧 一个单层的神经网络 最大间隔损失和反向传播 三.分类问题基本任务:窗口分类给定训练集,输入X,输出Y其中 x(i)是一个 d-维向量，y(i)是一个 C-维one-hot向量，N是总数。在传统的机器学习方法中，不管在训练还是测试还是验证,输入是固定的,往往通过诸如逻辑斯谛回归和SVM找到分类决策边界：说明: 一般也是线性决策边界. 其实X不单单只是一个单词的词向量,也可以是文档的词向量或者是窗口的词向量. 通常在机器学习当中,多分类的常用方式是使用softmax函数. 3.1 softmax详细说明: c表示的是类别的个数.因此softmax的权重矩阵的为R^(c*d),其中d为词向量的维数.说明: softmax的分类方法中只有x是固定的,因此我们只需要训练权值softmax分类函数：计算方法分为两个步骤：取权值矩阵的某一行乘上输入向量，归一化得到概率。 归一化:在softmax分类函数中,由于权值矩阵的某一行乘上输入向量得到的是一个实际的值,作为指数函数的输入,然后将权值矩阵与所有的输入向量,一一对应相乘之后相加,这样使得各个分类的类别始终处于(0,1)之间.因此,softmax:有两个好处: 归一化到(0,1)区间; 让类别的概率差异更明显(因为指数函数的存在); 3.2 交叉熵误差 交叉熵:就是理想概率分布与实际预测分布的误差之和.所谓理想概率分布就是:预测正确类别的概率是1,也就是最大概率,错误的类别的概率是0,因此他的y的向量是one-hot向量,但是实际情况当中是不存在的,存在误差,因此我们就把其叫做交叉熵误差.其实就是理想分布与预测分布的差距(KL散度) 3.3 softmax与交叉熵 训练的时候，可以直接最小化正确类别的概率的负对数,因为我们习惯最小化的方法,并且softmax的对数形式与原形式的效果是一样的：其实这个log形式的softamx等效于交叉熵：说明: y是目标分布概率(固定好的),^y是预测分布概率 因为类别y是one-hot向量,只有正确的类别的位置是1,其余位置都为0,因此式子只有y(i)不等于0的时候才有用,否则都为o.之后当y(i)=1的时候,相当于前面的y(i)消去了,因此交叉熵的最后形式是`-log(^y),正好是softmax的log形式,其中^y=softmax的原形式. 因此我们最小化这个损失函数也就是最小化理想分布与预测分布的KL散度.对N个数据点来讲有：加上正则化项有：说明: 正则化主要是为了防止某些模型的参数Θ(在这里就指的是权值W,因为)过大,因为某些权重过大就会导致模型对某个参数的依赖性加大,随机性也就减少了,从而导致模型的泛化程度有所下降.在图形上就是显示为函数有时候会很突兀,曲线不那么光滑,导致分类效果不是很好.正则化的思想可以这样理解:权值过大,那么我就使得误差也变大,之后根据误差的大小,来反向调节权重.红线是test error，蓝线是training error，横轴是模型复杂度或迭代次数。直线是方差偏差均衡点。 3.4 优化 一般的ML问题中，参数由权值矩阵的列组成维度不会太大。假设单词维度是100维,类别个数是3,因此构成的权重矩阵就是100*3=300个参数,这300个参数需要学习.(好好理解,权重矩阵W:3 x 100,3个单词的维度X:100 x 3,之后做內积(W*X),得到的类别的概率矩阵为:3 x 3.)而在词向量或其他深度学习中，需要同时学习权值矩阵和词向量。参数一多，就容易过拟合： 上图说的是对代价函数(各个局部损失函数之和)的参数梯度求导. 3.5 re-training词向量失去泛化效果 比如有一个给单词做情感分析的小任务，在预训练的词向量中，这三个表示电视的单词都是在一起的： 但由于情感分析语料中，训练集只含有TV和telly，导致re-training之后两者跑到别处去了： 于是在测试集上导致television被误分类。 这个例子说明，如果任务的语料非常小，则不必在任务语料上,把已经训练好的预词向量重新训练，否则会导致词向量过拟合–因为模型复杂度高,但是数据集的复杂度没有跟上去,因此就会导致模型对当前数据过拟合。顺便介绍一下词向量相关的术语： 词向量矩阵L常被称作lookup table： Word vectors = word embeddings = word representations (mostly) 可能有些人认为“词向量”是个很俗的翻译，但根据这门课的课件，其实与“词嵌入”是等价的。而且Rocher也几乎是一直在用word vector，而不是其他术语。 四.Window classification原因: 很少单独对单词做分类,因为单词有很多意思/特性,具体是哪类特性是要结合语境的,因此根据上下文对单词进行分类也是合情合理. 那么该如何对单词分类呢,我们可以使用窗口的方法来对单词分类:设定一个窗口,例如窗口是2,那么就将某个词的左右两边的各两个词作为上下文,将第三个词作为中心词,因此我们就可以根据上下文来预测中心词的词类. 那么上下文的词向量该怎么得到,我们可以使用拼接窗口中的所有向量得到.形成一个列向量(实际代码中是行向量—便于计算,但是列向量便于理解) 既然可以根据窗口来进行分类,因此在也可以识别该单词是不是实体和消歧.这是一个的列向量。(上面已经说过) 4.1 最简单的分类器：softmax 上图第一个公式是对单个类别的概率的计算,第二个公式则是对总体类别预测的计算.J对x求导，注意这里的x指的是窗口所有单词的词向量拼接向量。说明: 上图其实就是:实际预测分布中每个类别的概率向量-理想分布中每个类别的概率向量 上图就是对这个理想分布与实际分布的偏差进行向量化的一个步骤,δ就是一个标记,方便后面重复使用,使得公式不那么复杂,目的就是为了便于矩阵运算,重复使用(之后的梯度计算)于是就可以更新词向量了： 另一方面，对W求偏导数，将所有参数的偏导数写到一起有： 最后两步是将其转换为矩阵的內积形式,因为矩阵运算的效率更高.而不是这种∑的形式.虽然在更新过程中计算代价最高的有矩阵运算f=Wx和指数函数，但矩阵运算比循环要快多了（在CPU上快一个数量级）。 4.2 softmax（等价于逻辑斯谛回归）效果有限 softmax只是训练权重W，加上输入X是固定的，也就是说输入的词向量是一层不变的，不能学习调整，因此参数较少，模型的复杂度较低，因此只能适用于较小的数据集,能够提供一个勉强的线性分类决策边界。 因为softmax分类即使叠加本质上还是一个线性分类器。 五.使用神经网络 神经网络不仅学习权重矩阵，而且还学习词向量的表示，因此参数的数量就会大大增多，同时借助激活函数增加非线性，通过添加层数增加参数数量从而增加模型复杂度，因此也能更好地拟合数据。 神经网络可以提供非线性的决策边界： 5.1 从逻辑斯谛回归到神经网络老生常谈了，一些术语：每个神经元是一个二分类逻辑斯谛回归单元：神经网络同时运行多个逻辑斯谛回归，但不需要提前指定它们具体预测什么：我们把预测结果喂给下一级逻辑斯谛回归单元，由损失函数自动决定它们预测什么：于是就得到了一个多层网络： 5.2 前向传播网络一个简单的网络：这种红点图经常在论文里看到，大致代表单元数；中间的空格分隔开一组神经元，比如隐藏层单元数为2×4。U是隐藏层到class/类别的权值矩阵：其中a是激活函数： 5.3 间隔最大化目标函数例如: 正确样本:窗口的中心词为地点的X(window)为正确样本. 错误样本:窗口的中心词不为地点的X(window)为正确样本. 怎么设计目标函数呢，记sc代表误分类样本的得分，s表示正确分类样本的得分。则朴素的思路是最大化(s−sc)或最小化 (sc−s)。但有种方法只计算sc&gt;s⇒(sc−s)&gt;0时的错误，也就是说我们只要求正确分类的得分高于错误分类的得分即可，并不要求错误分类的得分多么多么小。这得到间隔最大化目标函数： 但上述目标函数要求太低，风险太大了，(因为你在测试的时候可能有些样本落在分类曲线的另一侧,因此就会预测错误,需要空出一定间隔的缓冲区.)没有留出足够的“缓冲区域”。可以指定该间隔的宽度(s−sc&lt;Δ) ，得到：经过大量实验证明,Δ=1的时候使得该间隔为1,分类效果比较明显： 这实际上是将函数间隔转换为几何间隔，参考SVM：http://www.hankcs.com/ml/support-vector-machine.html#h3-3 在这个分类问题中，这两个得分的计算方式为：通常通过负采样算法得到负例。 另外，这个目标函数的好处是，随着训练的进行，可以忽略越来越多的实例，而只专注于那些难分类的实例。说明: 一般情况下:最大间距损失函数值是大于0的,因此在训练的时候,如果出现等于0,则说明分类效果变好, 六.反向传播训练 s是最后结果的分类函数. 反向传播最好从上向下看,在本章的神经网络当中,我们有四种需要训练的参数:X={x1,x2…},W,b,U,其中W和U都是权重矩阵,因此都需要进行梯度求导.结果如下: 6.1 对U求导:–&gt;2 x 3 6.2 对W求导:–&gt;2 x 3我们先考虑单个权重W(ij) 从上图我们可以看到δ(i)是一个偏差信号,并且是一个常数,因为U(i)和f’(z)在前向传播的过程当中已经算出来了,X(j)则是一个输入信号.说明:其中δ(i)是第i个的误差, 最后我们把对W的求导转换成向量形式,在之后的运算当中我们会发现,这个偏差项δ可以复用,这也就是反向传播的特点之一.,并且几乎都要将最后的结果转换成向量化的形式,便于计算,从而实现批量化更新.权重矩阵的导数:δ*X^T,当中,维度的注意事项:我们可以得到δ为2 x 1 ,W是2 x 3 ,输入向量x则是3 x 1—&gt; 6.3 对偏差b求导:合起来就是δ,不是α,上图我们可以发现,偏差的偏導为最一个常数. 6.4 对输入X求導: 我们首先对单个x(i)进行求导,由于连接时每个输入单元连到了多个隐藏单元，所以对某个输入单元的偏导数是求和的形式（残差来自相连的隐藏单元）：红色部分是误差，相乘后得到一个标量，代表词向量第j维的导数。之后转换成向量化的形式:我们可以发现偏差项δ还是实现了复用,最后我们总结下各个参数的求导: 那么如果需要优化窗口分类的损失—因此前面各个类型的参数的求解,只是为窗口分类服务.那么就需要通过反向传播,求出各个参数的梯度,例如,计算U的梯度.","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"softmax","slug":"softmax","permalink":"http://yoursite.com/tags/softmax/"},{"name":"损失函数","slug":"损失函数","permalink":"http://yoursite.com/tags/损失函数/"}]},{"title":"python基础知识（一）","slug":"python基础知识（一）","date":"2018-12-11T11:35:11.000Z","updated":"2018-12-11T11:47:02.000Z","comments":true,"path":"2018/12/11/python基础知识（一）/","link":"","permalink":"http://yoursite.com/2018/12/11/python基础知识（一）/","excerpt":"本文主要是对python当中的基础知识进行查漏补缺,涉及到self含义,lazy的阈值,or ,and表达式,Numpy的axis等等,其中,Numpy的axis为重点,理解较为不易,因此进行了重点讲解.","text":"本文主要是对python当中的基础知识进行查漏补缺,涉及到self含义,lazy的阈值,or ,and表达式,Numpy的axis等等,其中,Numpy的axis为重点,理解较为不易,因此进行了重点讲解. 一.python中的self 二.lazy=false与lazy=true 三.python当中的or ,and 四.python数据结构 五.NumPy的axis 六.zip()与zip(*) 七.List中的for循环 一.python中的self 引用Python中self的含义 def()之后的箭头:提示该函数 输入参数 和 返回值 的数据类型123async def fetch(self, url: str, keys: object, repeat: int) -&gt; (int, object): dosomething() return None 箭头后面只有一个的话就是返回值 二.lazy=false与lazy=true 引用:lazy=false和lazy=true的区别 lazy代表延时加载，lazy=false，代表不延时，如果对象A中还有对象B的引用，会在A的xml映射文件中配置b的对象引用，多对一或一对多，不延时代表查询出对象A的时候，会把B对象也查询出来放到A对象的引用中，A对象中的B对象是有值的。如果lazy=true，代表延时，查询A对象时，不会把B对象也查询出来，只会在用到A对象中B对象时才会去查询，默认好像是false，你可以看看后台的sql语句的变化就明白了，一般需要优化效率的时候会用到. 三.python当中的or ,and注意事项:1231.在没有其他限定时，and的优先级大于or,除非有括号之类的;2.and从右往左3.or从左往 3 and 4等价于:1234if(4) return 4 else return 3 3 or 4等价于:1234if(3) return 3 else return 4 所以下面式子的结果为: 四.python数据结构1234List--[]tuple--()dict--&#123;key:value&#125;set--&#123;&#125; 参考:Python中的数据结构 五.NumPy的axisNumPy（axis=0/1/2…） 六.zip()与zip(*) zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表简称元组列表。如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，利用 * 号操作符，可以将元组解压为列表具体代码如下1234567891011121314151617s=[2,1]&gt;&gt;&gt; type(sorted(s))&lt;type 'list'&gt;&gt;&gt;&gt; type(zip(s))&lt;type 'list'&gt;&gt;&gt;&gt; z1=[1,2,3]&gt;&gt;&gt; z2=[4,5,6]&gt;&gt;&gt; result=zip(z1,z2)&gt;&gt;&gt; result[(1, 4), (2, 5), (3, 6)]&gt;&gt;&gt; z3=[4,5,6,7]&gt;&gt;&gt; result=zip(z1,z3)&gt;&gt;&gt; result[(1, 4), (2, 5), (3, 6)]zip()配合*号操作符,可以将已经zip过的列表对象解压&gt;&gt;&gt; zip(*result)[(1, 2, 3), (4, 5, 6)] 只有一个list的情况：123x = [1, 2, 3]x = zip(x)print (x) 运行的结果是：1[(1,), (2,), (3,)] 总结: zip()对列表z1=[1,2,3],z2=[4,5,6]进行打包,得到元组列表如:[(1, 4), (2, 5), (3, 6)],而zip(*)则对元组列表进行解压得到解压过后的元组列表,本质上只不过将原先的List格式数据z1=[1,2,3],z2=[4,5,6]转换成Tuple元组格式数据,取出来之后得到(1, 2, 3),(4, 5, 6) 七.List中的for循环1234L1 = ['x','y','z']L2 = [1,2,3]L3 = [ (a,b) for a in L1 for b in L2 ]print L3 得到结果1[('x', 1), ('x', 2), ('x', 3), ('y', 1), ('y', 2), ('y', 3), ('z', 1), ('z', 2), ('z', 3)] 根据上式结果我们可以得知:1L3 = [ (a,b) for a in L1 for b in L2 if a!='w'] (a,b):循环对象变量构成输出的基本单位元素–如(&#39;x&#39;, 1)for a in L1和for b in L2则是循环条件if a!=&#39;w&#39;则是判断条件[]则是最终的输出格式–列表格式 因此也就可以对zip(*(pair.split(&quot;###&quot;) for pair in pairs))进行判断split()方法返回的是列表格式[],但是(pair.split(&quot;###&quot;) for pair in pairs)有一对(),因此输出的是Tuple基本格式,加上zip(*)返回的是[],因此上述式子最终返回的是[([]),([])…..]","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"},{"name":"基础知识","slug":"python/基础知识","permalink":"http://yoursite.com/categories/python/基础知识/"}],"tags":[{"name":"Numpy","slug":"Numpy","permalink":"http://yoursite.com/tags/Numpy/"},{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"AllenNLP安装","slug":"ALLENNLP安装","date":"2018-12-11T11:32:17.000Z","updated":"2018-12-11T11:49:36.000Z","comments":true,"path":"2018/12/11/ALLENNLP安装/","link":"","permalink":"http://yoursite.com/2018/12/11/ALLENNLP安装/","excerpt":"本文主要讲述的是AllenNLP的安装,安装过程比较繁琐,特此记录下,便于回忆","text":"本文主要讲述的是AllenNLP的安装,安装过程比较繁琐,特此记录下,便于回忆 1234pip3 install http://download.pytorch.org/whl/cpu/torch-0.4.1-cp36-cp36m-win_amd64.whlpip3 install pillowpip3 install torchvisionpip install allennlp 如果出现如下异常123error: Microsoft Visual C++ 14.0 is required. Get it with \"Microsoft Visual C++ Build Tools\":http://landinghub.visualstudio.com/visual-cpp-build-tools 说明需要安装Microsoft Visual C++ Build Tools,参考安装Microsoft Visual C++ Build Tools 安装之后重启,再次使用pip install allennlp即可安装,安装成功如下图:","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"环境搭建","slug":"NLP/环境搭建","permalink":"http://yoursite.com/categories/NLP/环境搭建/"}],"tags":[{"name":"AllenNLP","slug":"AllenNLP","permalink":"http://yoursite.com/tags/AllenNLP/"}]},{"title":"NLP基础理论系列之词嵌入技术（一）","slug":"NLP基础理论系列之词嵌入技术（一至三）","date":"2018-12-11T11:18:24.000Z","updated":"2019-01-03T14:44:43.930Z","comments":true,"path":"2018/12/11/NLP基础理论系列之词嵌入技术（一至三）/","link":"","permalink":"http://yoursite.com/2018/12/11/NLP基础理论系列之词嵌入技术（一至三）/","excerpt":"本文是自然语言处理基础系列的第一篇，主要讲述的是计算机语言的基本处理单位－－词向量，涉及到词嵌入技术，随着时间的发展，词嵌入技术从原来的one-hot表示–&gt;word2vec中的模型表示–&gt;glove模型表示–&gt;elmo模型–&gt;bert模型,分别是对词向量进行由浅到深的表示:相似度–&gt;上下文–&gt;预训练.","text":"本文是自然语言处理基础系列的第一篇，主要讲述的是计算机语言的基本处理单位－－词向量，涉及到词嵌入技术，随着时间的发展，词嵌入技术从原来的one-hot表示–&gt;word2vec中的模型表示–&gt;glove模型表示–&gt;elmo模型–&gt;bert模型,分别是对词向量进行由浅到深的表示:相似度–&gt;上下文–&gt;预训练. 一.自然语言处理涉及到的层级 二.深度学习与机器学习，表示学习 三.NLP与DL 四.单词与向量 五.词向量的表示 5.1 one-hot表示向量 5.2 分布式表示向量 六.单词的两种表示方法 七.实现工具word2vec 7.1 SG模型 7.1.1 过程 7.1.2 场景 7.2 CBOW模型 7.2.1 过程 7.2.3 优缺点 7.4 负采样 7.5 softmax函数 八.模型比较 九.Glove模型 十.ELMO模型 10.1 作用 10.2 缺点 十一.BERT模型 11.1 思想 11.2 过程 11.3 Masked LM 11.4 Transformer 11.4.1 核心思想 11.4.2 评价 十二.词嵌入矩阵 十三.损失函数J中θ的理解 十四.交叉熵损失函数 十五.参考文献 一.自然语言处理涉及到的层级 由于词具有语音特征、句法特征和语义特征，形态学处于音位学、句法学和语义学的结合部位，形态就是最直观的层面，不需要分析的。 二.深度学习与机器学习，表示学习 深度学习，这是机器学习的一个子集，机器学习需要手工设置特征，然后把特征交给某个机器学习算法，比如线性分类器。机器为这些特征调整找到合适的权值，将误差优化到最小。 深度学习是表示学习的一部分，用来学习原始输入的多层特征表示.提供了一种通用的学习框架，可用来表示世界、视觉和语言学信息–表示学习（表征学习）,易扩展,时机成熟(数据多,算力大) 三.NLP与DL 人类语言的解读依赖于现实世界、常识以及上下文，由于说话速度书写速度阅读速度的限制，人类语言非常简练，省略了大量背景知识。 将自然语言处理的思想与表示学习结合起来，用深度学习的手法解决NLP目标。这提高了许多方面的效果： （１）层次：语音、词汇、语法、语义 （２）工具：词性标注、命名实体识别、句法\\语义分析 （３）应用：机器翻译、情感分析、客服系统、问答系统 四.单词与向量 （１）单词是人类语言学的符号,表示对事物的看法和指示; （２）向量则是计算机语言学的符号.其中因此为了实现计算机与现实世界的相连,需要构建单词与向量的联系,也就是词嵌入技术. 五.词向量的表示5.1 one-hot表示向量 表示简单,但是仅仅是对词语的表示,只是针对单词及其位置构建向量表示.对于词语之间的相似度不能计算,因为不同的one-hot表示向量是正交的;例如Dell notebook battery size和Dell laptop battery capacity相似,但是在one-hot向量中不能体现. 5.2 分布式表示向量 （１）one-hot表示向量只是代表词语的某个维度的含义,而分布式表示向量则是对某个词语的多维度进行表示,这样就可以进行词语之间的相似度的计算;如下图: （２）分布式表示可以解决One hot表示的问题,其实就是将词汇空间的词向量降维成特征/维度空间.因为一个词汇表其实含有多个维度,因此单词也有相应的维度值. 六.单词的两种表示方法 上下文向量,单词本身的表示方法 七.实现工具word2vec 因为普通的将词语转换成词向量的方式比较费时(使用DNN),因为如果词汇表一般在百万级别以上，这意味着我们DNN的输出层需要进行softmax计算各个词的输出概率的的计算量很大。因此Google与2013年开发了一个word2vec,它是将单词转换成词向量的工具.主要有两种神经网络语言模型(SG模型,CBOW模型)训练得到词向量的嵌入矩阵,其中Negative Sampling(负采样的方法),使得模型更加快速地训练. 其实模型训练的最终目的就是词嵌入矩阵(每个单词的每个维度/特征的情况–每个单词的词向量) 7.1 SG模型7.1.1 过程 SG模型则是根据一个中心词的输入,之后乘以各个词的嵌入矩阵得到该中心词的词向量表示,之后与各个词的词向量进行相似度比较,在数学上也就是以乘积的形式进行说明,算出词汇表中各个词的相似度,之后使用softmax激活函数,将数据归一化到(0,1),得到各个词的为下文的概率,(相似度越高,概率越大)最后取得到softmax值前2m个最大值,其中m是窗口,其中softmax函数的分母(就是各个指数之和)为的是让数据分化更明显,小的更小,大的更大;也就是即输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。具体如下图: 7.1.2 场景 因为是单个输入,因此学习程度较细,适合数据较少的情况;但是这也会导致学习速度较慢;直接训练Skip-Gram类型的算法，很容易使得高曝光词汇得到过多的权重. 7.2 CBOW模型7.2.1 过程 CBOW模型则是将输入改成是上下文的输入(写在同一个one-hot向量中),输出的是中心词.过程也是一样,将上下文的词转换成词向量之后与此表中的每个单词对应的词向量(他们存储在词嵌入矩阵中)进行相似度比较,并且计算各个单词之间的相似度,经过softmax激活函数,最后取概率最大对应的单词,作为中心词.具体如下图(注意每列的维度): 7.2.3 优缺点 因为是上下文的单词输入,因此适合数据较多的情况下 7.4 负采样 想一想，给你10000张写有数字的卡片，让你找出其中的较大值，是不是特别费力？但是如果把里面的较大值事先抽出来，跟五张随机抽取的卡片混到一起，让你选出其中的较大值，是不是就容易多啦？ 负采样就是这个思想，即不直接让模型从整个词表找最可能的词了，而是直接给定这个词（即正例）和几个随机采样的噪声词（即采样出来的负例），只要模型能从这里面找出正确的词就认为完成目标啦。学习给定的词中最有可能的词 其实就是让模型学习辨别正负例的功能,间接让模型找出最有可能的单词,这样就没有必要直接让模型从整个词表找最可能的词了,只要学会这种技能,就可以间接找到最有可能的词. 7.5 softmax函数 在数学，尤其是概率论和相关领域中，Softmax函数，或称归一化指数函数，是逻辑函数的一种推广。它能将一个含任意实数的K维的向量z的“压缩”到另一个K维实向量σ(z) 中，使得每一个元素的范围都在(0,1)之间，并且所有元素的和为1 例(来自wiki)：输入向量 [1,2,3,4,1,2,3] 对应的Softmax函数的值为 [0.024,0.064,0.175,0.475,0.024,0.064,0.175]。输出向量中拥有最大权重的项对应着输入向量中的最大值“4”。这也显示了这个函数通常的意义：对向量进行归一化，凸显其中最大的值并抑制远低于最大值的其他分量。示例说明： 向量第一个元素带入公式的值为:e^1/(e^1+e^2+e^3+e^4+e^1+e^2+e^3) =0.024 八.模型比较预测型模型(CBOW,RNN,SG等): 使用窗口方法,没有成功利用全局信息,只使用了局部信息; 这也导致训练速度较慢; 但不仅仅用于相似度计算.统计型模型(LSA,PCA,共现矩阵): 使用全局信息; 因此也就会导致训练速度较快; 但是用途仅仅受限于相似度九.Glove模型 Global Vector融合了预测型模型与统计型模型优势。实现在利用全局的先验统计信息下,加快模型的训练速度，又可以控制词的相对权重–&gt;f()。 我的理解是skip-gram、CBOW每次都是用一个窗口中的信息更新出词向量，但是Glove则是用了全局的信息（共线矩阵），也就是多个窗口进行更新十.ELMO模型10.1 作用 提供预训练的生成含有上下文词向量的语言模型;–源头上生成. 至此为止，词向量都是上下文无关的。也就是说，同一个词在不同的语境中总是相同的词向量，很明显这就导致词向量模型缺乏词义消歧（WSD）的能力。于是，人们为了让词向量变得上下文相关，开始在具体的下游任务中基于词向量sequence来做encoding–LSTM上下文。这就会导致模型的移植性非常差,会导致模型越来越复杂,既然发现在各个NLP任务中基本都有encoding的需要，那么为啥不在最开始就让词向量拥有上下文相关的能力呢？ 因此ELMO模型则是使用Bi-LSTM模型,实现含有上下文向量的生成,ELMo在模型层上就是一个stacked bi-lstm（严格来说是训练了两个单向的stacked lstm），所以当然有不错的encoding能力。 不过话说回来，ELMo的目标也仅仅是学习到上下文相关的、更强大的词向量，其目的依然是为下游任务提供一个扎实的根基，而是其通过实验间接说明了在多层的RNN中，不同层学到的特征其实是有差异的，因此ELMo提出在预训练完成并迁移到下游NLP任务中时，要为原始词向量层和每一层RNN的隐层都设置一个可训练参数，这些参数通过softmax层归一化后乘到其相应的层上并求和便起到了weighting的作用，然后对“加权和”得到的词向量再通过一个参数来进行词向量整体的scaling以更好的适应下游任务。 ps:其实最后这个参数还是非常重要的，比如word2vec中，一般来说cbow和sg学出来的词向量方差差异比较大，这时那个方差跟适合下游任务后续层方差匹配的词向量就收敛更快，更容易有更好的表现 通过这样的迁移策略，那些对词义消歧有需求的任务就更容易通过训练给第二隐层一个很大的权重，而对词性、句法有明显需求的任务则可能对第一隐层的参数学习到比较大的值（实验结论）。总之，这样便得到了一份”可以被下游任务定制“的特征更为丰富的词向量，效果比word2vec好得多也就不足为奇了。10.2 缺点 虽然ELMo有用双向RNN来做encoding，但是这两个方向的RNN其实是分开训练的，只是在最后在loss层做了个简单相加。这样就导致对于每个方向上的单词来说，在被encoding的时候始终是看不到它另一侧的单词的。而显然句子中有的单词的语义会同时依赖于它左右两侧的某些词，仅仅从单方向做encoding是不能描述清楚的.原因一想就很清楚了，毕竟传统的语言模型是以预测下一个词为训练目标的，然而如果做了双向encoding的话，那不就表示要预测的词已经看到了嘛，这样的预测当然没有意义了。十一.BERT模型11.1 思想 BERT则是根据ELMO模型的启发,得到预训练对迁移学习也是很有帮助,因此BERT模型重在训练一个龙骨级的模型(不单单是语言模型),包含对单词,句子,句子对之间(也就包括段落了)的表示及其关系的判断,ELMO则仅仅是对词语的表示.表示部分则是使用Transformer这个框架来实现.11.2 过程 它会先从数据集抽取两个句子，其中第二句是第一句的下一句的概率是 50%，这样就能学习句子之间的关系。其次随机去除两个句子中的一些词，并要求模型预测这些词是什么，这样就能学习句子内部的关系。最后再将经过处理的句子传入大型 Transformer 模型，并通过两个损失函数同时学习上面两个目标就能完成训练。11.3 Masked LM 模型会从数据集抽取两句话，其中 B 句有 50% 的概率是 A 句的下一句，然后将这两句话转化前面所示的输入表征。现在我们随机遮掩（Mask 掉）输入序列中 15% 的词，并要求 Transformer 预测这些被遮掩的词，以及 B 句是 A 句下一句的概率这两个任务 注意最后 10% 保留原句是为了将表征偏向真实观察值，而另外 10% 用其它词替代原词并不会影响模型对语言的理解能力，因为它只占所有词的 1.5%（0.1 × 0.15）。此外，作者在论文中还表示因为每次只能预测 15% 的词，因此模型收敛比较慢11.4 Transformer11.4.1 核心思想 Transformer所使用的注意力机制的核心思想是去计算一句话中的每个词对于这句话中所有词的相互关系，然后认为这些词与词之间的相互关系在一定程度上反应了这句话中不同词之间的关联性以及重要程度。因此再利用这些相互关系来调整每个词的重要性（权重）就可以获得每个词新的表达。这个新的表征不但蕴含了该词本身，还蕴含了其他词与这个词的关系，因此和单纯的词向量相比是一个更加全局的表达。 Transformer无视距离长短建模两个词之间的关系。因此为了令 Transformer 感知词与词之间的位置关系，我们需要使用位置编码给每个词加上位置信息(位置编码)。11.4.2 评价 完全基于注意力机制:Transformer舍弃了RNN的循环式网络结构，(用位置编码标记单词位置)完全基于注意力机制来对一段文本进行建模,不需要循环，而是并行处理序列中的所有单词或符号，同时利用自注意力机制将上下文与较远的单词结合起来; 训练速度快;ransformer 的训练速度比 RNN 快很多，而且其翻译结果也比 RNN 好得多,可以并行处理。 更适合在大数据中;在更小、更加结构化的语言理解任务或简单的算法任务中（如拷贝一个字符串（如将输入「abc」转换为「abcabc」）），Transformer 则表现欠佳。相比之下，在这些任务中表现良好的模型（如神经 GPU 和神经图灵机）在大型语言理解任务（如翻译）中表现不好.十二.词嵌入矩阵 (1)多个词的词向量组成的矩阵就是词嵌入矩阵 (2)词嵌入为单词的每个维度/特征赋予值.one-hot向量就是单词的位置,词嵌入矩阵就是每个单词的维度特征的汇总,词向量则是根据这个one-hot向量,也就是单词的位置,在词嵌入矩阵中提取出对应单词的每个维度/特征的情况,也就是词向量.十三.损失函数J中θ的理解 为了简化计算量,在word2vec中,我们只使用θ作为模型的参数 因为θ是词嵌入矩阵,θ是词嵌入矩阵,也就是各个单词的组成词向量,但是单词的词向量有两种形式:一种是本身向量形式,一种是单词的上下文形式的词向量.因此我们使用语言模型训练的单词词向量有两个词嵌入矩阵,也就分别是图中的W,W’(本身词向量矩阵,还有一个是单词的上下文矩阵),因此求微分的时候需要对中心词的词向量矩阵V,上下文的词向量矩阵U这两个矩阵求取.见SG模型图即可. 第一红框是对模型参数–中心词的词向量求梯度,第二红框中的U0则是上下文的词向量,第二红框中的后面的一项(即期望),当(U0-期望)的值接近0向量,表明中心词的词向量与上下文的词向量越接近,然后梯度随之会变得越小,其中w则是1-V的随机数–&gt;就代表随机性十四.交叉熵损失函数 交叉熵使用在二分类的情况比较多，而使用Softmax进行多分类十五.参考文献 ❋交叉熵损失函数的直观理解 ❋BERT模型详解 ❋NLP的游戏规则从此改写？从word2vec, ELMo到BERT ❋softmax函数 ❋词表征、Word2Vec、ELMo和BERT模型学习总结 ❋NLP专题","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"训练营","slug":"NLP/训练营","permalink":"http://yoursite.com/categories/NLP/训练营/"}],"tags":[{"name":"word2vec","slug":"word2vec","permalink":"http://yoursite.com/tags/word2vec/"},{"name":"词向量","slug":"词向量","permalink":"http://yoursite.com/tags/词向量/"}]},{"title":"使用colab平台进行训练","slug":"使用colab平台进行训练","date":"2018-12-01T05:54:06.000Z","updated":"2018-12-01T07:25:42.000Z","comments":true,"path":"2018/12/01/使用colab平台进行训练/","link":"","permalink":"http://yoursite.com/2018/12/01/使用colab平台进行训练/","excerpt":"本文主要是讲述模型在借组谷歌硬盘使用colab实现后台运行,该平台运行速度能让人接受,并且操作也较为便捷.","text":"本文主要是讲述模型在借组谷歌硬盘使用colab实现后台运行,该平台运行速度能让人接受,并且操作也较为便捷. 一.简介 二.准备工作: 三.安装必要的包和软件 四.挂载Drive 五.更改工作目录 六.运行 七.总结 八.参考文献 一.简介 Google Colab 提供免费的 Jupyter 笔记本环境，不需要进行任何设置就可以使用，并且完全在云端运行，其默认的后台深度学习框架是TensorFlow, 除此之外，你也可以在上面安装并使用Keras、PyTorch、OpenCV等等流行的深度学习库来． 可以把Colab看成是一台带有GPU的Ubuntu虚拟机，只不过我们只能用命令行的方式操作它。你可以选择执行系统命令，亦或是直接编写运行python代码。（速度也是能让人接受,不会太慢,比本身用笔记本慢些,关键离线） Colab最多连续使用12小时，超过时间系统会强制掐断正在运行的程序并收回占用的虚拟机。（好像再次连接到虚拟机后，虚拟机是被清空的状态，需要重新配置和安装库等等），下面这几步在每次启动的时候都要进行(右上角显示连接的时候),比较麻烦,免费的才是最贵的.,如图: 二.准备工作:123451.谷歌账号;2.在谷歌硬盘当中兴建文件夹,本次命名为BERT;3.之后上传代码文件(预训练模型文件看情况是否上传,清空output文件夹--模型文件太大);4.修改--&gt;笔记本设置:硬件加速器改为GPU5.点击右上部分的连接按钮,链接即可 三.安装必要的包和软件123456789101112!apt-get install -y -qq software-properties-common python-software-properties module-init-tools!add-apt-repository -y ppa:alessandro-strada/ppa 2&gt;&amp;1 &gt; /dev/null!apt-get update -qq 2&gt;&amp;1 &gt; /dev/null!apt-get -y install -qq google-drive-ocamlfuse fusefrom google.colab import authauth.authenticate_user()from oauth2client.client import GoogleCredentialscreds = GoogleCredentials.get_application_default()import getpass!google-drive-ocamlfuse -headless -id=&#123;creds.client_id&#125; -secret=&#123;creds.client_secret&#125; &lt; /dev/null 2&gt;&amp;1 | grep URLvcode = getpass.getpass()!echo &#123;vcode&#125; | google-drive-ocamlfuse -headless -id=&#123;creds.client_id&#125; -secret=&#123;creds.client_secret&#125; 如果出现:E: Package &#39;python-software-properties&#39; has no installation candidate直接按回车即可(多次),如果运行中出现网址会提示输入验证码，点击程序给出的网址进行验证得到验证码,复制粘贴即可。 四.挂载Drive 其实完成前面的操作我们就可以在Colab中敲写代码或者输入一些系统命令了（linux下的一些基本命令），但是我们现在连接的虚拟机是和Google Drive脱离的，也就是说我们跑的程序无法使用谷歌云盘里的文件，这就非常受限制了。所以我们一般需要将谷歌云盘看作是虚拟机中的一个硬盘挂载，这样我们就可以使用虚拟机轻松访问谷歌云盘。(1)清空上次缓存:12!mkdir -p drive!google-drive-ocamlfuse drive (2)或者保留上次缓存12!mkdir -p drive!google-drive-ocamlfuse drive -o nonempty 挂载完Google Drive，会在虚拟机里生成一个drive文件夹，直接将Google Drive当成是一块硬盘即可。访问drive文件夹里的文件，就是在访问你的Google Drive里的文件。 五.更改工作目录 Colab中cd命令是无效的，切换工作目录使用chdir函数。12import osos.chdir('drive/BERT') 执行以上代码，当前工作目录会进入到drive文件夹下的BERT文件夹下。我们再使用!ls命令会发现系统输出的是drive文件夹下的目录。 回到上级目录：os.chdir(‘../‘) 六.运行12345678910111213!python run_classifier.py \\ --task_name=vega \\ --do_train=true \\ --do_eval=true \\ --data_dir=data \\ --vocab_file=gs://cloud-tpu-checkpoints/bert/uncased_L-24_H-1024_A-16/vocab.txt \\ --bert_config_file=gs://cloud-tpu-checkpoints/bert/uncased_L-24_H-1024_A-16/bert_config.json \\ --init_checkpoint=gs://cloud-tpu-checkpoints/bert/uncased_L-24_H-1024_A-16/bert_model.ckpt \\ --max_seq_length=16 \\ --train_batch_size=32 \\ --learning_rate=2e-5 \\ --num_train_epochs=8.0 \\ --output_dir=output \\ 结果为: 七.总结 （１）最好在本地编辑好,在本地能够运行在提交到云平台当中,要不然修改麻烦.并且效率低； （２）云平台上热启动有时候可能会报错，这个要注意； （３）如果出现不能保存模型结果,或者其他异常问题，最安全的方法就是删除对应colab对应的文件(以.ipynb为结尾的)之后重新安装,最后实在不行的话,全部代码和.ipynb全部删除,再次建立。因为一个colab对应的.pynib就相当于是个linux的虚拟机环境 八.参考文献 ❋Google免费GPU使用教程 ❋Google Colab——用谷歌免费GPU跑你的深度学习代码 ❋使用Google免费GPU进行BERT模型fine-tuning ❋colab中文版","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"优化","slug":"深度学习/优化","permalink":"http://yoursite.com/categories/深度学习/优化/"}],"tags":[{"name":"Colab","slug":"Colab","permalink":"http://yoursite.com/tags/Colab/"}]},{"title":"Bert模型的微调","slug":"Bert模型的微调","date":"2018-12-01T05:27:05.000Z","updated":"2018-12-01T07:10:54.000Z","comments":true,"path":"2018/12/01/Bert模型的微调/","link":"","permalink":"http://yoursite.com/2018/12/01/Bert模型的微调/","excerpt":"本文主要大概讲述了BERT模型的基本原理,BERT是超多参数和超大数据训练得到的预训练模型,因此模型复杂度比较高,并且可移植性比较好,因此就有了微调的环节,本文重点讲述微调的具体步骤,便于日后进行场景的转换.","text":"本文主要大概讲述了BERT模型的基本原理,BERT是超多参数和超大数据训练得到的预训练模型,因此模型复杂度比较高,并且可移植性比较好,因此就有了微调的环节,本文重点讲述微调的具体步骤,便于日后进行场景的转换. 一.概述 二.代码解析 2.1 主函数部分： 2.2 任务名及其方法的映射 2.3 根据任务名进行方法获取 2.4 数据集的准备 2.5 任务方法的建立 三.运行参数的设置 3.1 命令行运行 3.2 本地运行 四.运行结果 4.1 验证结果 4.2 测试结果 五.参数选择 六.参考文献: 一.概述 今年是迁移学习进步比较大的一年，典型的代表就是BERT模型,BERT模型就是Bidirectional Encoder Representation from Transformers，即Transformer的双向编码,通过超大数据、巨大模型、和极大的计算开销训练而成.就是一个预训练,后期通过微调(Fine-Tuning)来适应不同的NLP任务,其主要思想就是通过双向编码来理解文本的语义(词与词之间的关系)–输入主要有三个部分:词语本身的表示,词的位置信息,还有一个词在句子中的表示,三个相加构成BERT模型的输入.之后通过双向编码(也就是同时利用当前位置前面的词和后面的词两部分信息),同时借助随机遮掩的方法,增加不确定,提高模型的复杂度,加上借助超大数据集,平衡数据和模型的关系,最终得到一个预训练的模型.其实就是得到了一个提取文本特征的编码器,这个其实就是迁移学习的典型体现:实现了较为复杂的特征提取,之后根据自己的需求,编写对应的具体的NLP任务(一般设置好数据格式和数据的读写即可),之后进行调试超参数,得到一个较高准确率的解码器,这个就是BERT模型微调的过程 因为BERT模型已经有训练好的中文模型,因此我们就没有必要再次进行预训练,因为耗时非常大,因此我们就可以根据这个预训练的模型进行微调,实现模型应用场景的迁移(也就是迁移学习的思想) 本文以分类任务的迁移进行说明.因此主要修改地方为:run_classifier.py 二.代码解析2.1 主函数部分：123456789if __name__ == \"__main__\": # 模型的输入文件地址/目录,之后有专门的方法读取文件也就是Processor做的事情 flags.mark_flag_as_required(\"input_file\") # 模型的配置文件的路径 flags.mark_flag_as_required(\"bert_config_file\") # 模型的输出文件的路径 flags.mark_flag_as_required(\"output_dir\") # 模型运行 tf.app.run() 2.2 任务名及其方法的映射 以字典的形式完成映射123456789# 任务的名称与任务方法名 processors = &#123; \"cola\": ColaProcessor, \"mnli\": MnliProcessor, \"mrpc\": MrpcProcessor, \"xnli\": XnliProcessor, ＃自定义的方法 \"vega\": MoveProcessor &#125; 不同的方法对应处理不同的数据集,这个要搞清楚.如MrpcProcessor这个方法是对MRPC训练集进行模型训练,验证,测试. 2.3 根据任务名进行方法获取1234567# task_name is used to select processor,And the name is lowercase task_name = FLAGS.task_name.lower() # if task_name as a key don't match with Processor which is type of dict ,os will print error if task_name not in processors: raise ValueError(\"Task not found: %s\" % (task_name)) # if exist,we will get the processor which aims to deal with data processor = processors[task_name]() 2.4 数据集的准备 主要是有三个数据集：训练集，验证集，测试集，自定义数据集的格式需要和原有的数据集格式最好一直，这样能减轻开发的成本．便于照猫画虎．类别与句子用\\t即tab键,隔开,具体如下：注意: 数据集的格式最好是utf-8(无BOM)格式存储,否则读取容易出问题. 各个阶段的数据量大小最好呈现为：７：２：１，逐步缩小（我的就不太标准）（１）训练集train.tsv:1234567891011120 company1获得了什么奖1 prize是哪个公司获得的3 prize的证书上编号是多少3 我想查看prize的证书编号2 company1与company2的关系是怎样的0 company1有哪些奖3 我想知道prize的证书编号2 company1与company2的联系1 prize是授予给哪个公司的1 prize是表彰给哪个公司的2 company1与company2是什么关系3 prize的证书编号是多少 （２）验证集dev.tsv:123456782 company1与company2之间的关系3 prize的编号是多少2 company1与company2有怎样的联系1 prize属于哪个公司1 prize是颁发给哪个公司的1 哪个公司获取的奖项是prize0 company1荣获哪些奖3 我想知道prize上的证书编号 （３）测试集test.tsv123451 prize的奖项由哪个公司得到0 有哪些奖是company1获得的2 company1与company2是怎样的关系0 company1获取了哪些奖3 我想查看prize上的编号 2.5 任务方法的建立 方法的建立是根据数据集的情况来建立的,我的情况是:数据集的标签为４个；数据格式为：标签＋’\\t’+句子；数据集有三个．自定义方法如下：12345678910111213141516171819202122232425262728293031323334353637#这个就是自定义的方法,针对自己的数据集设定的.class MoveProcessor(DataProcessor): \"\"\"Processor for the move data set .\"\"\" def get_train_examples(self, data_dir): \"\"\"See base class.\"\"\" return self._create_examples( self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\") def get_dev_examples(self, data_dir): \"\"\"See base class.\"\"\" return self._create_examples( self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\") def get_test_examples(self, data_dir): \"\"\"See base class.\"\"\" return self._create_examples( self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\") def get_labels(self): \"\"\"See base class.\"\"\" return [\"0\", \"1\", \"2\", \"3\"] #设置统一的读取转换格式,便于不同训练集的调用 def _create_examples(self, lines, set_type): \"\"\"Creates examples for the training and dev sets.\"\"\" examples = [] for (i, line) in enumerate(lines): guid = \"%s-%s\" % (set_type, i) if set_type == \"test\": text_a = tokenization.convert_to_unicode(line[0]) label = \"0\" else: text_a = tokenization.convert_to_unicode(line[1]) label = tokenization.convert_to_unicode(line[0]) examples.append( InputExample(guid=guid, text_a=text_a, text_b=None, label=label)) return examples 从上面的程序也可以看出processor的方法主要是进行数据的读取,基本方法就是使用_create_examples()奠定读取的方法. 三.运行参数的设置 由于本地上以命令行运行有些问题，需要在Google的colab中才能正常运行，本文同时讲解命令行的形式和本地的形式 3.1 命令行运行123456789101112131415161718192021222324252627#!是代表在linux下运行!python run_classifier.py \\ #任务的名称 --task_name=vega \\ #是否需要进行训练 --do_train=true \\ #是否需要进行验证 --do_eval=true \\ #是否需要进行测试,false时不会进行测试,true时则会优先使用已经存在的模型, #没模型的话,则会从头训练,直至模型保存. --do_predict=false #数据文件的目录,训练之前需要将训练集,验证集,测试集的文件准备好 --data_dir=data \\ #下面三个colab会调用谷歌自家保存的预训练模型,不需要自己上传,也可以自己上传(比较耗时) --vocab_file=gs://cloud-tpu-checkpoints/bert/uncased_L-24_H-1024_A-16/vocab.txt \\ --bert_config_file=gs://cloud-tpu-checkpoints/bert/uncased_L-24_H-1024_A-16/bert_config.json \\ --init_checkpoint=gs://cloud-tpu-checkpoints/bert/uncased_L-24_H-1024_A-16/bert_model.ckpt \\ #分词时一个词语的最大长度 --max_seq_length=16 \\ #批量训练的的大小每32个为一批 --train_batch_size=32 \\ #学习率(一般不动) --learning_rate=2e-5 \\ #轮询的次数:8次 --num_train_epochs=8.0 \\ #模型及其各个阶段的结果的保存目录 --output_dir=output \\ 上面代码是在colab中的运行的方法,根据自己的需要进行修改,之后直接粘贴复制即可． 3.2 本地运行 本地运行主要是在run_classifier.py中,修改的情况为:1234567891011121314151617181920# 设置常量.便于后期修改DATA_DIR=\"E:/Project/bert/Before/data/\"MODEL_DIR=\"E:/Project/bert/Before/PreTrainingModel/\"OUTPUT_DIR=\"E:/Project/bert/Before/output/\"#参数修改(其实还有很多,找几个关键的参数修改即可)\"data_dir\", DATA_DIR,\"bert_config_file\",MODEL_DIR+\"bert_config.json\",\"task_name\", \"vega\",\"vocab_file\", MODEL_DIR+\"vocab.txt\",\"init_checkpoint\", MODEL_DIR+\"bert_model.ckpt\",\"max_seq_length\", 16,\"do_train\",True,\"do_eval\", True,\"do_predict\", True,\"train_batch_size\",32,\"eval_batch_size\",6, \"predict_batch_size\", 2, \"learning_rate\", 5e-5,\"num_train_epochs\", 8.0,\"warmup_proportion\", 0.1, 四.运行结果4.1 验证结果屏幕上也会输出验证结果情况: 验证结果保存在output目录的eval_results.txt,如下图: 4.2 测试结果 测试结果保存在output目录的test_results.tsv: 上表中:每行代表每类的概率,如第一行则代表第二类的概率最大,为0.30405113,所以其对应的标签为2.以此类推 五.参数选择以下是本次调整的参数.123456789\"max_seq_length\", 16,\"train_batch_size\",32,\"eval_batch_size\",6,\"predict_batch_size\", 2,\"learning_rate\", 5e-5,\"num_train_epochs\", 8.0,\"warmup_proportion\", 0.1,\"save_checkpoints_steps\", 1000,\"iterations_per_loop\", 1000, 六.参考文献: ❋谷歌最强NLP模型BERT解读 ❋BERT的理解 ❋BERT模型fine-tuning代码解析（一）","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"优化","slug":"深度学习/优化","permalink":"http://yoursite.com/categories/深度学习/优化/"}],"tags":[{"name":"BERT","slug":"BERT","permalink":"http://yoursite.com/tags/BERT/"},{"name":"微调","slug":"微调","permalink":"http://yoursite.com/tags/微调/"}]},{"title":"Pytorch基本使用(三)","slug":"Pytorch基本使用(三)","date":"2018-11-28T16:20:37.000Z","updated":"2018-11-28T16:20:40.000Z","comments":true,"path":"2018/11/29/Pytorch基本使用(三)/","link":"","permalink":"http://yoursite.com/2018/11/29/Pytorch基本使用(三)/","excerpt":"本文主要是Pytorch使用系列的第三篇，该篇主要讲述的是模型构建,主要是使用Pytorch进行模型构建,流程包括:数据加载,超参数设置,模型构建,训练流程等等,一开始上手流程较为复杂,多写几次就能孰能生巧.","text":"本文主要是Pytorch使用系列的第三篇，该篇主要讲述的是模型构建,主要是使用Pytorch进行模型构建,流程包括:数据加载,超参数设置,模型构建,训练流程等等,一开始上手流程较为复杂,多写几次就能孰能生巧. 一.模型构建 1.1 传统方法 1.2 快速方法 二.模型保存及其恢复 2.1 全部保存及其恢复 2.2 参数保存及其恢复 三.超参数设置 四.优化器的选择 五.模型构建运行完整步骤: 一.模型构建1.1 传统方法 一般用于比较复杂的模型,主要有两个方法:初始化方法与关于数据处理流程的方法,具体代码如下:1234567891011class Net(torch.nn.Module): # 5.1 initial def __init__(self,inputdata,hidden_num,outputdata): super(Net, self).__init__() self.hidden = torch.nn.Linear(inputdata, hidden_num) # hidden layer self.predict = torch.nn.Linear(hidden_num, outputdata) # output layer # 5.2 setting the processing of data def forward(self, x): x = F.relu(self.hidden(x)) # activation function for hidden layer x = self.predict(x) # linear output return x 1.2 快速方法 一般用于较为简单的模型构建:123456#Sequential可以引申为有顺序的意思net4 = torch.nn.Sequential( torch.nn.Linear(2, 10), torch.nn.ReLU(), torch.nn.Linear(10, 2)) 二.模型保存及其恢复 主要有两种模型保存方式,全保存和参数保存. 2.1 全部保存及其恢复 全保存直接直接加载保存的模型名就可以12345678# The instance of Net stores in net1net1 = Net(n_feature=2, n_hidden=10, n_output=2) # define the network# 1st:save model completely,including the parameter,model Framework,the computing graph and so ontorch.save(net1, \"net.pkl\")# 1st:restore the complete modelnet2 = torch.load(\"net.pkl\")print(\"\\nnet1:\", net1)print(\"\\nnet2:\", net2) 2.2 参数保存及其恢复 参数保存的模型,则需要先构建一模一样的模型才能进行模型的恢复.12345678910111213141516# build new NN quicklynet3 = torch.nn.Sequential( torch.nn.Linear(2, 10), torch.nn.ReLU(), torch.nn.Linear(10, 2))print(\"\\nnet3:\", net3)# 2rd:Only save the parameter of model in order to load quickertorch.save(net3.state_dict(), \"net_parameter.pkl\")net4 = torch.nn.Sequential( torch.nn.Linear(2, 10), torch.nn.ReLU(), torch.nn.Linear(10, 2))net4.load_state_dict(torch.load(\"net_parameter.pkl\"))print(\"\\nnet4:\", net4) 两种模型的恢复结果如下图: 三.超参数设置 一般在开头设置全局常量,便于修改1234# 1st:hyper parameterLR = 0.01BATCH_SIZE = 32EPOCH = 12 四.优化器的选择 一般Adam优化速度较快,能使梯度线快速躺平,从而使得误差达到局部最小 五.模型构建运行完整步骤:主要步骤:123456789101112131415161718(1)hyper parameter--超参数设置;(2)fake dataset--测试数据(接近实际的数据) showing the generate dataset(显示数据的生成--numpy的基本类型格式)(3)put dateset into torch dataset--将数据转换成Tensor形式(4)Config load about dataset--设置模型加载(批处理大小,是否随机打乱)(5)Creat NN--构建模型 initial--模型初始化 setting the processing of data--数据处理流程(6)initial the model--实例化模型(7)Generate the Optimizer,loss_func--选择优化器和损失函数(8)training--开始训练 predict y--预测 compute the loss--根据预测值进行误差计算 Gradient to Zero--梯度归零,因为梯度是批量累加的,因此每训练一批要进行归零,否则就会梯度计算错误 backpropagation, compute gradients--反向传播计算梯度 apply gradients--将计算出来的梯度矫正参数W record loss--每次训练的损失(9)showing the speed and effect of optimizer 完整代码如下:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import torchimport torch.utils.data as Dataimport torch.nn.functional as Fimport matplotlib.pyplot as plttorch.manual_seed(1) # reproducible# 1st:hyper parameterLR = 0.01BATCH_SIZE = 32EPOCH = 12# 2rd:fake datasetx = torch.unsqueeze(torch.linspace(-1, 1, 1000), dim=1)y = x.pow(2) + 0.1*torch.normal(torch.zeros(*x.size()))# 2.1:showing the generate datasetplt.scatter(x.numpy(), y.numpy())plt.show()# 3th:put dateset into torch datasettorch_dataset = Data.TensorDataset(x, y)# 4th: Config load about datasetloader = Data.DataLoader( dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)# 5th:default networkclass Net(torch.nn.Module): # 5.1 initial def __init__(self,inputdata,hidden_num,outputdata): super(Net, self).__init__() self.hidden = torch.nn.Linear(inputdata, hidden_num) # hidden layer self.predict = torch.nn.Linear(hidden_num, outputdata) # output layer # 5.2 setting the processing of data def forward(self, x): x = F.relu(self.hidden(x)) # activation function for hidden layer x = self.predict(x) # linear output return xif __name__ == '__main__': # 6th initial the model net_SGD = Net(1,20,1) # 7th:Generate the Optimizer,loss_func opt_SGD= torch.optim.SGD(net_SGD.parameters(), lr=LR) optimizer = [opt_SGD] loss_func = torch.nn.MSELoss() losses_his = [[], [], [], []] # record loss # 8th:training for epoch in range(EPOCH): print('Epoch: ', epoch) for step, (b_x, b_y) in enumerate(loader): for opt,l_his in zip(optimizer,losses_his):# for each training step # 8.1:predict y output = net_SGD(b_x) # get output for every net # 8.2:compute the loss loss = loss_func(output, b_y) # compute loss for every net # 8.3:Gradient to Zero # Beacause the grad is cumulative between each batch opt.zero_grad() # clear gradients for next train # 8.4 backpropagation, compute gradients loss.backward() # # 8.5 apply gradients opt.step() # 8.6: record loss l_his.append(loss.data.numpy()) # # 9th:showing the speed and effect of optimizer plt.plot(l_his, label=\"SGD\") plt.legend(loc='best') plt.xlabel('Steps') plt.ylabel('Loss') plt.ylim((0, 0.2)) plt.show() plt.close()","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"基础","slug":"深度学习/基础","permalink":"http://yoursite.com/categories/深度学习/基础/"}],"tags":[{"name":"模型构建","slug":"模型构建","permalink":"http://yoursite.com/tags/模型构建/"},{"name":"超参数","slug":"超参数","permalink":"http://yoursite.com/tags/超参数/"}]},{"title":"Pytorch基本使用(二)","slug":"Pytorch基本使用(二)","date":"2018-11-28T16:08:50.000Z","updated":"2018-11-28T16:28:58.000Z","comments":true,"path":"2018/11/29/Pytorch基本使用(二)/","link":"","permalink":"http://yoursite.com/2018/11/29/Pytorch基本使用(二)/","excerpt":"本文是Pytorch使用系列第二篇,主要讲述的是max()、view()、squeeze()与unsqueeze()、gather()、optimizer.zero_grad()函数的使用及其注意事项.本文易混淆,因此要多加留心.","text":"本文是Pytorch使用系列第二篇,主要讲述的是max()、view()、squeeze()与unsqueeze()、gather()、optimizer.zero_grad()函数的使用及其注意事项.本文易混淆,因此要多加留心. 一.torch.max() 二.view() 三.squeeze()与unsqueeze() 3.1 c.unsqueeze(0) 3.2 c.unsqueeze(1) 四.torch.gather() 五.optimizer.zero_grad 一.torch.max() （１）作用:按行/列取出对应张量的最大值(第一个即[0])及其索引(第二个即[1]). （２）完整:torch.max(input, dim, keepdim=False, out=None) -&gt; (Tensor, LongTensor)返回最大数值(按条件查找有多个,用tensor承载)及其索引.参数说明如下:12345input (Tensor) – 输入 Tensork (int) – 第 k 个最小值dim (int, optional) – 沿着此维进行排序keepdim (bool) – 输出张量是否保持与输入tensor维度相同out (tuple, optional) – 输出元组 (max, max_indices) 例子如下:123456789101112&gt;&gt;&gt;d=torch.Tensor([[1,3],[2,4]])&gt;&gt;&gt;d 1 3 2 4[torch.FloatTensor of size 2x2] #显示最大值及其索引&gt;&gt;&gt;torch.max(d,0)( 2 4 [torch.FloatTensor of size 1x2], 1 1 [torch.LongTensor of size 1x2]) max(d,0/1):d代表张量数据,参数0/1分别代表列和行,结果为最大值对应行和列. 二.view() 相当于numpy当中的reshape view()函数作用是将一个多行的Tensor,拼接成一行。 a.view(i,j)表示将原矩阵转化为i行j列的形式 i为-1表示不限制行数，输出1列 三.squeeze()与unsqueeze() （１）torch.squeeze():降维, （２）torch.unsqueeze():增加维度根据参数决定是按行/列升降维度.squeeze、unsqueeze操作不改变原矩阵1234567&gt;&gt;&gt;c=torch.Tensor(3)&gt;&gt;&gt;c 7.5589e+28 5.2839e-11 1.8888e+31 #注意是size 3[torch.FloatTensor of size 3] 3.1 c.unsqueeze(0)1234&gt;&gt;&gt;c.unsqueeze(0) 7.5589e+28 5.2839e-11 1.8888e+31 #注意是size 1x3[torch.FloatTensor of size 1x3] 3.2 c.unsqueeze(1)123456&gt;&gt;&gt;c.unsqueeze(1) 7.5589e+28 5.2839e-11 1.8888e+31 #注意是size 3x1[torch.FloatTensor of size 3x1] 四.torch.gather() （１）作用:沿给定维度dim,通过索引张量index实现指定位置的值的聚集,也就是聚集操作,说白了就是对若干个指定位置的值进行统一取出,存储在张量中. （２）完整:torch.gather(input, dim, index, out=None) → Tensor，说明如下:1234input (Tensor) – 源tensor dim (int) – 指定的轴数（维数） index (LongTensor) – 需要聚集起来的数据的索引 out (Tensor, optional) – 目标tensor 例子:1234567891011121314151617&gt;&gt;&gt;b = torch.Tensor([[1,2,3],[4,5,6]])&gt;&gt;&gt;print b1 2 34 5 6[torch.FloatTensor of size 2x3]&gt;&gt;&gt;index_1 = torch.LongTensor([[0,1],[2,0]])&gt;&gt;&gt;print torch.gather(b, dim=1, index=index_1)1 26 4[torch.FloatTensor of size 2x2]&gt;&gt;&gt;index_2 = torch.LongTensor([[0,1,1],[0,0,0]])&gt;&gt;&gt;print torch.gather(b, dim=0, index=index_2)1 5 61 2 3[torch.FloatTensor of size 2x3] 其中b = torch.Tensor([[1,2,3],[4,5,6]])是二维输入张量(因为有[[),torch.LongTensor([[0,1],[2,0]])与torch.LongTensor([[0,1,1],[0,0,0]])则是二维索引张量,因此要求两者(输入张量与索引张量)的维度需要相同(也就是[[的连续数量要相等,但是[1,0]之类数量可以不用),因为这样才能索引, (1)其中dim=1表示的是按行索引,其中索引张量的[1,0]则是按行查找(因为dim=1)输入张量:121 2 34 5 6 的第一行的第一个(下标为0)与第一行的第2个(下标/序号为1)也就是分别是1和2,之后第二个[2,0]则是第二行的第3个(下标/序号为1)和第二行的第一个(下标为0),也就是6和4,因此最后输出张量为:1231 26 4[torch.FloatTensor of size 2x2] 从上面我们可以得知,索引张量的大小及其维度就是输出张量的大小及其维度(很好理解) (2)dim=0则是按列查找输入张量:121 2 34 5 6 ,其中[0,1,1]则是求出每列的索引对应的值,也就是第一列的1,第二列的5,第三列的6,[0,0,0]则是每列的第一个元素:1,2,3.因此torch.gather(b, dim=0, index=index_2)的结果为:1231 5 61 2 3[torch.FloatTensor of size 2x3] 五.optimizer.zero_grad Pytorch为什么每一轮batch需要设置optimizer.zero_grad? 根据pytorch中的backward()函数的计算，当网络参量进行反馈时，梯度是被积累的而不是被替换掉；但是在每一个batch时毫无疑问并不需要将两个batch的梯度混合起来累积，因此这里就需要每个batch设置一遍zero_grad","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"基础","slug":"深度学习/基础","permalink":"http://yoursite.com/categories/深度学习/基础/"}],"tags":[{"name":"torch函数","slug":"torch函数","permalink":"http://yoursite.com/tags/torch函数/"}]},{"title":"Pytorch基本使用(一)","slug":"Pytorch基本使用(一)","date":"2018-11-28T16:05:09.000Z","updated":"2018-11-28T16:21:56.000Z","comments":true,"path":"2018/11/29/Pytorch基本使用(一)/","link":"","permalink":"http://yoursite.com/2018/11/29/Pytorch基本使用(一)/","excerpt":"本文主要是Pytorch使用系列的第一篇,主要讲述了Pytorch的包结构和Tensor这数据处理格式,Tensor与Storage、Scalar、Variable之间的关系，及其新版中Tensor的变化。","text":"本文主要是Pytorch使用系列的第一篇,主要讲述了Pytorch的包结构和Tensor这数据处理格式,Tensor与Storage、Scalar、Variable之间的关系，及其新版中Tensor的变化。 一.Pytorch包结构 二.torchvision包结构 三.Tensor与Storage 四.Tesor、numpy、List之间的转换 五.Variable与Tensor合并 5.1 Variable 5.2 Tensor 5.3 查看Tensor的类型 5.4 requires_grad 5.5 .data与.detach() 六.其他 一.Pytorch包结构1234567891011121314torch:Torch的基本运算torch.Tensor:tensor数据类型(逻辑类型)的基本处理torch.Storage:Storage数据类型(物理存储类型)的基本处理torch.nn:模型层及其内部函数,模型训练时的损失函数torch.nn.functional:非线性激活函数,池化函数,卷积函数,标准化函数,DropOut函数,线性函数,损失函数,距离函数(张量之间的距离)torch.nn.init:模型初始化相关方法torch.optim:实现了各种优化算法的库torch.autograd:自动求梯度函数torch.multiprocessing:用于在相同数据的不同进程中共享视图torch.legacy:方便Lua Torch使用者将模型torch.cuda:对CUDA张量类型的支持，实现了与CPU张量相同的功能，但使用GPU进行计算torch.utils.ffi:创建并配置一个cffi.FFI对象,用于PyTorch的扩展,主要适用于当已有的 op 无法满足我们的要求的时候，那就需要自己动手来扩展。torch.utils.data:数据集的相关操作(加载),表示Dataset的抽象类torch.utils.model_zoo:使用这个工具, 加载大家共享出来的模型 二.torchvision包结构12345torchvision:包含了目前流行的数据集，模型结构和常用的图片转换工具torchvision.datasets:七种数据集torchvision.models:包含已经训练好的模型,或者也可以自定义训练:AlexNet,VGG,ResNet,SqueezeNet,DenseNet torchvision.transforms:pytorch中的图像预处理包torchvision.utils:常用的图片转换工具 三.Tensor与Storagetorch.Tensor是一种包含单一数据类型元素的多维矩阵torch.Tensor是默认的tensor类型（torch.FlaotTensor）的简称。一个torch.Storage是一个单一数据类型的连续一维数组。每个torch.Tensor都有一个对应的、相同数据类型的存储。例如:torch.FloatTensor对应torch.FloatStorageTorch定义了七种CPU tensor类型和八种GPU tensor类型：| Data tyoe | CPU tensor | GPU tensor || ———————— | ——————– | ————————- || 32-bit floating point | torch.FloatTensor | torch.cuda.FloatTensor || 64-bit floating point | torch.DoubleTensor | torch.cuda.DoubleTensor || 16-bit floating point | N/A | torch.cuda.HalfTensor || 8-bit integer (unsigned) | torch.ByteTensor | torch.cuda.ByteTensor || 8-bit integer (signed) | torch.CharTensor | torch.cuda.CharTensor || 16-bit integer (signed) | torch.ShortTensor | torch.cuda.ShortTensor || 32-bit integer (signed) | torch.IntTensor | torch.cuda.IntTensor || 64-bit integer (signed) | torch.LongTensor | torch.cuda.LongTensor | 四.Tesor、numpy、List之间的转换1234567 #arange代表分配6个元素np_data = np.arange(6).reshape((2, 3))torch_data = torch.from_numpy(np_data)tensor2array = torch_data.numpy() #data是List类型data = [-1, -2, 1, 2]tensor = torch.FloatTensor(data) 五.Variable与Tensor合并5.1 Variable.data属性来访问变量中的原始张量,关于这个变量的梯度被计算放入.grad属性中Variable是autograd包的核心类.它包装了张量(Tensor),支持几乎所有的张量上的操作.一旦你完成你的前向计算,可以通过.backward()方法来自动计算所有的梯度,执行反向传播,out.backward()相当于执行out.backward(torch.Tensor([1.0])) 5.2 Tensor Tensor现在默认requires_grad=False的Variable了. torch.Tensor和torch.autograd.Variable现在其实是同一个类!没有本质的区别! 5.3 查看Tensor的类型使用.isinstance()或是x.type(), 用type()不能看tensor的具体类型.1234567&gt;&gt;&gt; x = torch.DoubleTensor([1, 1, 1])&gt;&gt;&gt; print(type(x)) # was torch.DoubleTensor\"&lt;class 'torch.Tensor'&gt;\"&gt;&gt;&gt; print(x.type()) # OK: 'torch.DoubleTensor''torch.DoubleTensor'&gt;&gt;&gt; print(isinstance(x, torch.DoubleTensor)) # OK: TrueTrue 5.4 requires_gradrequires_grad 已经是Tensor的一个属性了12345678910111213&gt;&gt;&gt; x = torch.ones(1)&gt;&gt;&gt; x.requires_grad #默认是FalseFalse&gt;&gt;&gt; x.backward()RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn&gt;&gt;&gt; # 可以将`requires_grad`作为一个参数, 构造tensor&gt;&gt;&gt; w = torch.ones(1, requires_grad=True)&gt;&gt;&gt; w.requires_gradTrue&gt;&gt;&gt; #不自动求导的x+(设置为自动求导的w)=可以自动求导的total&gt;&gt;&gt; total = w + x&gt;&gt;&gt; total.requires_gradTrue 5.5 .data与.detach() 以前.data是为了拿到Variable中的Tensor,但是后来, 两个都合并了. 所以 .data返回一个新的requires_grad=False的Tensor! 然而新的这个Tensor与以前那个Tensor是共享内存的. 所以不安全.12345y = x.data # # y和x是共享内存的,但是这里y已经默认为不支持autograd,。#但是x需要进行autograd, 这样就会不安全x.detach()#这个仍旧是共享内存的, 但是各自的求导功能分离了，所以也变得安全。 ###5.6 scalar与Tensor通过引入scalar, 可以将返回值的类型进行统一. 重点: 取得一个tensor的值(返回number), 用.item() 创建scalar的话,需要用torch.tensor(number) torch.tensor(list)也可以进行创建tensor123456&gt;&gt;&gt; torch.tensor(3.1416) # 用torch.tensor来创建scalar(标量)tensor(3.1416) # 注意 scalar是打印出来是没有[]的&gt;&gt;&gt; torch.tensor(3.1416).size() # size是0torch.Size([]) #说明是0维向量,也就是标量,没有数字被包围.&gt;&gt;&gt; torch.tensor([3]).size() # compare to a vector of size 1torch.Size([1]) # 如果是tensor, 打印出来会用`[]`包上说明是1维向量 六.其他 torch.nn 只支持小批量输入,整个torch.nn包都只支持小批量样本,而不支持单个样本. 如果想在GPU上训练,则要添加:模型.cuda,输入值.cuda 与目标值.cuda autograd包为张量上的所有操作提供了自动求导.它是一个运行时定义的框架,这意味着反向传播是根据你的代码如何运行来定义,并且每次迭代可以不同 nn.Parameter-一种变量,当把它赋值给一个Module时,被自动的注册为一个参数. torch.Tensor-一个多维数组/多维矩阵(好好屡屡) 信息增益就是不确定减少的程度","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"基础","slug":"深度学习/基础","permalink":"http://yoursite.com/categories/深度学习/基础/"}],"tags":[{"name":"Pytorch","slug":"Pytorch","permalink":"http://yoursite.com/tags/Pytorch/"},{"name":"Tensor","slug":"Tensor","permalink":"http://yoursite.com/tags/Tensor/"}]},{"title":"Numpy与Torch的区别","slug":"Numpy与Torch","date":"2018-11-28T11:13:27.000Z","updated":"2018-11-28T11:20:54.000Z","comments":true,"path":"2018/11/28/Numpy与Torch/","link":"","permalink":"http://yoursite.com/2018/11/28/Numpy与Torch/","excerpt":"本文主要讲述的是Numpy与Torch之间的区别,Numpy主要使用数组作为数据的处理类型；Torch则是使用Tensor作为数据的处理类型,其中Storage则是用来存储Tensor类型变量的值.","text":"本文主要讲述的是Numpy与Torch之间的区别,Numpy主要使用数组作为数据的处理类型；Torch则是使用Tensor作为数据的处理类型,其中Storage则是用来存储Tensor类型变量的值. 一.python的List,数组矩阵 二.numpy数组,列表与Torch 三.Variable与Torch合并 一.python的List,数组矩阵 python中列表的嵌套可形成多维数组,因为python中列表是基本数据结构，数组不是。但是numpy对数组进行优化,使得数组的存储、输入与输出的效率比python直接操作嵌套列表（多维数组）要快，数组越大，numpy优势越明显。其中numpy中的矩阵是数组的一种，矩阵是二维数组，矩阵的优点是矩阵乘积较为方便，并且二维数组用的比较多。因此numpy的基本处理数据格式是数组Array,Torch处理的基本数据格式是Tensor. List可以存放任意类型的数据，但是List保存的是数据的地址，list1=[1,2,3,&#39;a&#39;],虽然list1保存数据对应的4个指针,如果完全存储则还需要保存四个数据，增加了存储和消耗cpu,Array则是要求存放的数据类型相同,并且方便处理,同时数组名可以作为数组变量的地址,因此相同条件下,只需要存储数据和一个地址指针(数组名)。 二.numpy数组,列表与Torch12345678910111213import torchimport numpy as npnp_data = np.arange(6).reshape((2, 3))torch_data = torch.from_numpy(np_data)tensor2array = torch_data.numpy()data = [-1, -2, 1, 2]tensor = torch.FloatTensor(data) # 转换成32位浮点 tensorprint( '\\nnumpy array:', np_data, # [[0 1 2], [3 4 5]] '\\ntorch tensor:', torch_data, # 0 1 2 \\n 3 4 5 [torch.LongTensor of size 2x3] '\\ntensor to array:', tensor2array, # [[0 1 2], [3 4 5]]) 三.Variable与Torch合并 Variable:就是数值可以变动的变量,更多运用在模型的参数及其计算图/步骤产生的变量中;Tensor则是更多运用在数据的存储中.现在已经合并.说明Tensor既可以作为随时变化的变量,也可以作为数据的存储变量.1234 # requires_grad是决定张量w参不参与误差反向传播, 要不要计算梯度&gt;&gt;&gt; w = torch.ones(1, requires_grad=True)&gt;&gt;&gt; w.requires_gradTrue 获取tensor:Tensor.detach()和.data123456789&gt;&gt;&gt; a = torch.tensor([1,2,3.], requires_grad = True)&gt;&gt;&gt; out = a.sigmoid()&gt;&gt;&gt; c = out.detach()&gt;&gt;&gt; c.zero_()tensor([ 0., 0., 0.])&gt;&gt;&gt; out # modified by c.zero_() !!tensor([ 0., 0., 0.])&gt;&gt;&gt; out.sum().backward() # Requires the original value of out, but that was overwritten by c.zero_()RuntimeError: one of the variables needed for gradient computation has been modified by an 上述错误的原因:因为c = out.detach()中张量c获取张量out的时候,虽然是共享tensor里面的 数据和requires_grad=False,即在计算梯度的时候默认是不参与误差反向传播, 不要计算梯度,因此也不改变张量c的值.","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"基础","slug":"深度学习/基础","permalink":"http://yoursite.com/categories/深度学习/基础/"}],"tags":[{"name":"Numpy","slug":"Numpy","permalink":"http://yoursite.com/tags/Numpy/"},{"name":"Torch","slug":"Torch","permalink":"http://yoursite.com/tags/Torch/"}]},{"title":"Pytorch神经网络基础","slug":"PyTorch 神经网络基础","date":"2018-11-28T10:38:47.000Z","updated":"2018-11-28T11:38:56.000Z","comments":true,"path":"2018/11/28/PyTorch 神经网络基础/","link":"","permalink":"http://yoursite.com/2018/11/28/PyTorch 神经网络基础/","excerpt":"本文主要讲述的是深度学习的一些基本概念和一些常用的模型,包括激活函数,损失函数,常见模型CNN,RNN,LSTM,自编码器,常见的数据处理方法有:针对过拟合的DropOut和BN(批量标准化).","text":"本文主要讲述的是深度学习的一些基本概念和一些常用的模型,包括激活函数,损失函数,常见模型CNN,RNN,LSTM,自编码器,常见的数据处理方法有:针对过拟合的DropOut和BN(批量标准化). 一.激励函数 1.1 作用 1.2 类型 1.2.1 sigmoid与softmax 1.2.2 tanh 1.2.3 Relu 二.损失函数 2.1 MSELoss 2.2 Crossentropyloss 三.CNN 3.1 通俗解释 3.2 各层的含义: 四.RNN与LSTM 4.1 目的: 4.2 NN工作原理 4.3 RNN的问题 4.4 LSTM 五.自编码器 5.1 原因 5.2 过程 六.过拟合 6.1 原因 6.2 方法 6.2.1 正则化方法 6.2.2 DropOut 七.批标准化 7.1 原因 7.2 效果 八.Torch的动态体现 一.激励函数 1.1 作用 激励函数/激活函数(AF–Active Function):主要目的增加特征的非线性,更好地拟合实际情况(逼近拟合实际的任意函数,因为参数多).来丰富特征之间的联系(因为可以学习到若干特征之间的不同关系),而不是单单只有线性关系,最后也利于增加模型的复杂度,即增加模型对非线性情况的拟合程度. 因为生活当中,线性情况(成正比,成反比)在实际中比较少,更多的是不确定的事情比较多,所以使用激励函数使原本的预测结果与输入值的线性关系得以改变,变成非线性关系(就好比中间一根直线,代表线性关系的直线,突然由于线上的某个点在激励函数的作用下,改变了值,导致线性关系被破环.),同时有时可以缓解梯度消失.(因为sigmoid函数反向传播时，很容易就会出现梯度消失的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0,导致反向传播时,梯度线接近躺平,出现梯度消失，这种情况会造成信息丢失），从而无法完成深层网络的训练). 引入非线性函数作为激励函数，这样深层神经网络就有意义了（不再是输入的线性组合，可以逼近任意函数）。激活函数的输出有界，很容易充当下一层输入,激活函数对神经网络的深层抽象功能有着极其重要的意义,主要是对非线性特征的抽象. 1.2 类型1.2.1 sigmoid与softmax收敛速度可以理解为梯度线的移动速度 sigmoid:有效区间较短,但曲线平缓,该函数收敛速度较慢. 说明:虽然在一定程度会增加非线性,但是容易过饱和(例如:输入值只要大于-5或者小于,梯度线就会躺平),也就是我们所说的是有效区间较短,边缘区间(梯度线接近躺平)较长,会导致很多数据落在边缘区间,梯度线躺平.导致激活值相差不大,数据的非线性关系覆盖率减少,导致梯度消失.(非线性,软饱和),观察sigmoid函数,可以得知其曲线较为平缓,因此该函数收敛速度慢,可以用tanh来进行缓解. sigmoid将一个real value映射到（0,1）的区间（当然也可以是（-1,1）），这样可以用来做二分类。 (根据激活值的正负来进行二分类) softmax把一个k维的real value向量（a1,a2,a3,a4….）带入到softmax函数当中,之后映射成一个类似（b1,b2,b3,b4….）的向量(就是概率),其中bi是一个0-1的常数(对应类别的概率)，然后可以根据bi的大小,取最大bi值,之后根据该值所对应的类别(也就是序号)进行分类.(根据概率值进行分类) 虽然多个sigmoid函数通过叠加也同样可以实现多分类的效果，但是 softmax函数进行的多分类，类与类之间是互斥的，即一个输入只能被归为一类；多个sigmoid函数进行多分类，输出的类别并不是互斥的，即”苹果”这个词语既属于”水果”类也属于”3C”类别。 1.2.2 tanh tanh:有效区间较sigmoid窄,但是收敛速度更快(),但是有效区间较短,边缘区间(梯度线接近躺平)较长的现状尚未改变.(非线性,软饱和) 1.2.3 Relu (1)有效区间较大:占一半,即大于0的部分,有效缓解了梯度消失的问题; (2)收敛速度快:函数简单,且右侧为线性函数,求导运算较为简单,因为是线性,收敛速度更快,所以梯度下降的时候,只需沿着那条线周边的点走即可. (3)稀疏激活性:Relu函数值会使一部分神经元的输出为0(左侧为0)，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生.如果学习率太快,虽然Relu函数可以更快收敛,到达局部最优,即梯度线躺平,许多知识会忽略,即容易简化模型过度,导致很多神经元死亡,因此要控制学习率注意:Relu只能用于隐藏层 其中tanh与sigmoid是增加非线性,丰富特征,缓解欠拟合.Relu/Softplus则是缓解过拟合,其会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生123Rectified Linear Unit(ReLU) - 用于隐层神经元输出:用于缓解过拟合Sigmoid/tanh - 用于隐层神经元输出:用于缓解欠拟合,丰富特征程度Softmax - 用于多分类神经网络输出 二.损失函数2.1 MSELoss (实际值-预测值)的平方 2.2 Crossentropyloss 交叉熵主要是比较真实标签分布p与预测标签分布q的相似性,相似度越高,就说明q值越大,最后交叉熵越小,说明损失也越小 三.CNN3.1 通俗解释 之前是以图片的像素点作为基本的处理单位,这样使得学习到的特征之间没有关联,导致泛化性差,现在的CNN则是以图片的一小块作为处理单元,之后用卷积核(特征提取器)来提取每块的特征,之后设置移动的步伐,一步步提取,这样就能加强图片间信息的连续性和联系性,从而能够提取出特征及其之间的关系,最终便于机器加深对图片的理解,因为让机器看到的是一个图形而不是一个个像素点.(图片–&gt;边缘信息–&gt;脸和鼻子之类的特征信息–&gt;特征组合–&gt;区别不同的类.) 3.2 各层的含义: （１）卷积层:丰富信息阶段,提取特征.多个卷积核(参数不同),则是为了尽量地保留更多信息,实现提取多个不同类/情况的特征,加深机器对输入图片更深的理解,不压缩长宽,增加高度(卷积核的个数) （２）池化层:用来压缩信息,压缩高度,筛选具有代表性的特征(最大池化(取最大值),平均池化) （３）全连接层:实现特征间的不同组合,配合不同的权重参数,得到众多不同的类别.配合softmax函数,实现多分类. 四.RNN与LSTM4.1 目的: 实现数据之间的关联, 4.2 NN工作原理 以语音转文本产生为例,就是将之前输入数据X(t)的状态S(t),这个状态则是由第一个NN(一层或者几层神经元)产生,根据这个状态进行处理判断,得到结果Y(t),即第一个短语,之后将这个状态传递给下一个状态S(t+1),第二个状态S(t+1)则是X(t+1)通过NN产生的,之后将第一个状态S(t)与第二个状态S(t+1)进行结合,得到当前的状态,根据当前的状态进行判断处理,得到第二个短语Y(t+1),以此类推,最后得到一个完整的句子.其中X(t)与Y(t)的对应关系根据不同的情况进行设置.(例如根据一段文字,判断句子的正负面,那就有输入X(t),X(t+1)….X(n),只有一个结果Y(n).) 4.3 RNN的问题 例子:输入值/长序列X是以下一句话: ‘我今天要做红烧排骨, 首先要准备排骨, 然后…., 最后美味的一道菜就出锅了,用RNN进行判断,如果判断错误,判断为辣子鸡,那么就需要学习红烧排骨与长序列X的关系来修正错误,因此需要借助误差进行反向传播,不断调整参数,学习两者的关系,但是由于红烧排骨离句尾的距离很远,如果W只要大于1,那么之后的误差就会越来越大,形成梯度爆炸,调整的参数更加错误了,也就是梯度爆炸,如果W小于1,那么经过长途跋涉,最后的误差只会越来越小,到句首的时候,误差接近为0,参数相当于没调整,学习不到正确的参数.也就是梯度消失的现象. 4.4 LSTM 只不过是在序列状态当中增加了三个门,遗忘门,输入门和输出门.我们将序列状态改叫为细胞状态,相当于一条总线,贯穿RNN始终,当前输入的数据和上一个状态(之前的数据)构成当前的输入状态. （１）遗忘门:主要根据当前的输入决定细胞状态中哪些状态需要遗忘;(舍弃部分状态)–用sigmoid函数 （２）输入门:主要根据当前的输入确定细胞状态还需要增加哪些状态,之后用加法实现细胞状态的更新;(增加部分状态,实现细胞状态的更新) （３）输出门:根据最新细胞状态和当前输入状态,确定最终输出的是什么 sigmoid(当前输入状态)*tanh(最新细胞状态):在基于当前输入和最新细胞状态的条件下,确定输出的是什么. 五.自编码器5.1 原因 高清图片作为输入,就会学习上千万个像素点,非常耗时耗力,卷积效率也不高,因此提取高清图片的关键性/代表性信息(特征)作为输入,是一种好的方法.这就是压缩部分,也就是自编码器的编码部分,(提取关键信息) 5.2 过程 自编码器由编码器(提取特征,压缩)和解码器(根据特征还原,解压). 以白X作为输入,对白X进行压缩,提取关键信息,之后进行解压得到预测值–黑X,将黑X与白X进行比对,求出误差进行反向传递,然后逐步调整编码器部分和解码器部分的神经网络的参数,减少误差,提升自编码器的准确度. 其中,中间的部分则是编码器的输出结果,也是作为解码器的输入部分. 六.过拟合6.1 原因 过于追求误差小,导致模型复杂度超过数据复杂度,导致训练时误差小,测试时误差大. 6.2 方法 (1)增加数据量抵消模型复杂度 (2)简化模型 6.2.1 正则化方法 因为在过拟合的时候,参数W的变化值往往变化比较大,这是由于之前过于追求误差小的结果,因此在训练的时候,如果W值变大,我们就需要使得误差也会变大来进行调整,反之则相反,这就是对W参数惩罚,因此也就有了L1,L2正则化,分别是为误差加上参数W的绝对值和参数W的的平方,这种方法就是正则化的思想.这种思想适用于机器学习与深度学习 6.2.2 DropOut 每次训练随机忽略各层的若干个神经单元,有利于简化模型,缓解过拟合;同时不会过度依赖某些神经元,从而导致某些神经元权重过大,增加模型的稳定性.但是在测试的时候则需要关闭此功能. 七.批标准化 Batch Normalization:对每层进行标准化–批标准化 7.1 原因 (1)数据的分布情况对于机器理解学习的数据程度有至关重要的作用.(针对数据的分布)假如有些数据值很大,有些很小,并且很分散,这样就很不利于机器的学习.因此需要对这些数据进行统一处理,将其规约在某个区间上,便于机器学习到数据之中的规律. (2)不进行标准化,会让神经网络对特征值大的数据不敏感(针对某些数据),例如输入两个数值:1和20,权重系数为0.1,之后经过tanh激活函数,但是结果分别是0.1和0.96,0.96所在的梯度线差不多快躺平了,之后进行反向传播,进行参数调整,就会让該点误差变得非常小,导致参数调整缓慢,不利于机器对某些特征的学习. 7.2 效果 由于进行了BN,导致每层的输入数据大部分分布在激活函数的有效区间(梯度线较为倾斜的区间).好处有两个: (1)不仅方便机器在训练,进行反向传播的时候,根据误差求得梯度,进而调整参数,增加准确率; (2)而且还有利于实现机器学习不同特征之间的关系. 八.Torch的动态体现动态RNN:12 # 随机 time step 长度dynamic_steps = np.random.randint(1, 4) 动态设置步数","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"概念","slug":"深度学习/概念","permalink":"http://yoursite.com/categories/深度学习/概念/"}],"tags":[{"name":"神经网络","slug":"神经网络","permalink":"http://yoursite.com/tags/神经网络/"},{"name":"激励函数","slug":"激励函数","permalink":"http://yoursite.com/tags/激励函数/"}]},{"title":"深度学习的优化问题","slug":"深度学习优化问题","date":"2018-11-28T10:23:09.000Z","updated":"2018-11-28T10:29:20.000Z","comments":true,"path":"2018/11/28/深度学习优化问题/","link":"","permalink":"http://yoursite.com/2018/11/28/深度学习优化问题/","excerpt":"本文主要讲述的是优化问题,优化问题主要是加快损失函数的最小化(局部/全局),本质还是要找调节权重参数,因此学习率和梯度成为优化问题的关键,目前主要是有四种优化器/算法,主要是从两个方面加快梯度的下降:调节学习率和调节学习率的更新幅度.","text":"本文主要讲述的是优化问题,优化问题主要是加快损失函数的最小化(局部/全局),本质还是要找调节权重参数,因此学习率和梯度成为优化问题的关键,目前主要是有四种优化器/算法,主要是从两个方面加快梯度的下降:调节学习率和调节学习率的更新幅度. 一.训练步骤 二.优化问题 三.误差 3.1 二维平面的误差 3.2 三维及其以上的误差 四.全局最优与局部最优 五.学习率 5.1 学习率衰减 六.优化器 6.1 种类 6.1.1 SGD 6.1.2 Momentum: 6.1.3 RMSprop 6.1.4 Adam 6.2 算法代码 一.训练步骤（１）单个样本（２）根据前向算法,得到误差（２）反向传播得到梯度（３）根据之前设定的学习率,结合公式:new_weight = existing_weight — learning_rate * gradient,调整权重参数（４）循环往复,训练完全部样本（５）得到损失函数(损失函数L = 1/N ∑ Li （每个样本损失函数的叠加求均值）。这个损失函数L变量就是θ，其中L中的参数是整个训练集,损失函数是通过整个训练集的训练叠加得到的) 二.优化问题 主要是通过优化减少误差,优化的方法主要有三个系列:牛顿法 (Newton’s method), 最小二乘法(Least Squares method), 梯度下降法 (Gradient Descent) ,其中梯度下降法 (Gradient Descent)在神经网络当中较为常用.主要方法有(Stochastic gradient descent)随机梯度下降,MSProp,Adam等方法.其中梯度就是某点的导数 优化问题就是减少误差问题，减少误差问题在NN中主要是使用梯度下降法，因此误差问题本质上就是找出使得梯度为0的权重W解.该解若为局部最优解也可以出色地完成任务. 三.误差3.1 二维平面的误差 误差就是实际值与预测值的差距,其中x是输入数据,ωx则是预测值,y是实际值。因为只需要求出cost最小，加上x，y都是固定的，所以为了简化，大胆假设把x，y去掉，留下ω，之后进行求导，图中可以看出，最低点时，也就是误差最小的时候，也就是导数、梯度为0。因此求cost最小就演变成梯度等于0即找到梯度线躺平的问题。其中梯度线则是沿着梯度下降的方向移动的，直至梯度线躺平。 3.2 三维及其以上的误差 三维及其以上的梯度线则是不那么好找，因为W的参数个数也会增加，导致对应的误差曲线也会变复杂，会不止有一个小山沟，这就容易会导致有多个躺平的梯度线，同时 不同的 W 初始化的位置, 将会带来不同的下降区域. 不同的下降区域, 又会带来不同的 W 解，得到误差曲线值的局部最小. 从而有多个局部最优解（简化为关于权重W的）和一个全局最优解（简化为关于权重W的）。于是就有了全局最优解与局部最优解的问题。 四.全局最优与局部最优 鉴于时间与成本的压力，神经网络的局部最优解一般也能出色的完成任务。 五.学习率 主要是用在参数更新的速度上,结合公式:new_weight = existing_weight — learning_rate * gradient 5.1 学习率衰减 训练的epoch越多,学习率就会越小,否则很容易越过某个权重W使得损失函数错失最小值. 六.优化器 主要是优化学习率的增减幅度和更新速度.从而更快逼近全局最优解:W(min),使得损失函数最小 6.1 种类6.1.1 SGD Momentum=0的特殊情况. 6.1.2 Momentum: 全称：Gradient Descent with Momentum:动量梯度下降算法 作用：加快对权重参数调整的学习率.对前几次的梯度进行加权平均,减少震荡,同时使用动能这个参数,如果没有阻力,增大学习率,反之减少学习率; 参数:有动能η 6.1.3 RMSprop 作用：实现为不同权重参数W设置不同的学习率.主要是哪个方向振荡幅度大,就减少该方向的更新速度,反之则相反,使得梯度下降平缓. 参数:有beta/β,epsilon/ε参数,防止分母为0; 6.1.4 Adam 全称：Adaptive Moment Estimation:自适应优化算法 Adam算法则是结合Momentum与RMSprop算法的优点.允许你使用Momentum算法使用β调节学习率(其实就是Momentum算法中的Momentum参数)，从而加快你的算法学习速度。同时RMSprop算法为不同的参数计算不同的自适应学习率. 参数:β1通常设置为0.9,β2通常设置为0.999，ε一般为10^(−8),V是学习率,S是震荡幅度. 6.2 算法代码1234567opt_SGD = torch.optim.SGD(net_SGD.parameters(), lr=LR) #momentum:学习率的增减幅度参数opt_Momentum = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8) #alpha:学习率更新速度的影响参数opt_RMSprop = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9) #betas=(0.9, 0.99):学习率的增减幅度,学习率更新速度的影响参数opt_Adam = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"优化","slug":"深度学习/优化","permalink":"http://yoursite.com/categories/深度学习/优化/"}],"tags":[{"name":"SGD","slug":"SGD","permalink":"http://yoursite.com/tags/SGD/"},{"name":"学习率","slug":"学习率","permalink":"http://yoursite.com/tags/学习率/"}]},{"title":"语义相似度（一）","slug":"语义相似度","date":"2018-11-19T01:18:06.000Z","updated":"2018-11-19T05:07:36.000Z","comments":true,"path":"2018/11/19/语义相似度/","link":"","permalink":"http://yoursite.com/2018/11/19/语义相似度/","excerpt":"本文是语义相似度系列的第一篇,主要讲诉的是语义相似度的基本概念及其基本情况,为后面使用工具包进行语义相似度的计算打下坚实的基础.相似度计算是作为语义解析的核心,相似度计算的复杂度是根据语义的复杂度来计确定的，将语义复杂度从低到高，我们可以使用关键词匹配，浅层语义匹配，和深层语义匹配三个思路来进行解决，其中思路的具体方法会在后面进行说明．","text":"本文是语义相似度系列的第一篇,主要讲诉的是语义相似度的基本概念及其基本情况,为后面使用工具包进行语义相似度的计算打下坚实的基础.相似度计算是作为语义解析的核心,相似度计算的复杂度是根据语义的复杂度来计确定的，将语义复杂度从低到高，我们可以使用关键词匹配，浅层语义匹配，和深层语义匹配三个思路来进行解决，其中思路的具体方法会在后面进行说明． 一.场景 二.概念辨析 三.主要方式: 3.1 基于词语方法: 3.2 基于句法特征的方法 3.3 基于深度学习的方法 3.4 结论 四.相似度具体计算方法 五.相似度种类及其方法 5.1 词语相似度计算 5.2 短语相似度计算 5.3 句子相似度计算 5.4 文本相似度计算 5.5 近似词 六.向量空间/词向量模型与余弦相似性通俗解释 七.关键词提取 一.场景 在问句解析的时候中，我们经常要计算两个 query 的相似度，例如「推荐好看的电影」与「2016年好看的电影」。通过词向量按位累加的方式，得到这两个 query 的向量表示之后，可以利用 Cosine Similarity 来计算两者的相似度。对于较难的短文本-短文本语义匹配任务，则可以考虑引入有监督信号的训练数据并利用 Deep Structured Semantic Model (DSSM) 或 Convolutional Latent Semantic Model (CLSM) 这些更复杂的神经网络模型进行语义相关性的计算. 二.概念辨析三.主要方式:3.1 基于词语方法: （１）特点：只考虑词语（或词语之间的关系）,不考虑句法 （２）类型: (1)问句间的词语或关键词公共个数; (2)词语语义矩阵(词语之间的相似度); (3)向量空间模型(词向量模型)+余弦距离; (4)主题模型(LDA):在短文本(问句)上的效果不太理想，在短文本-短文本匹配任务中词向量的应用比主题模型更为普遍。LDA模型就是符合该主题的词有很多,通过计算文档中的这些词,然后求出该文档更倾向于哪(几)个主题/类型. (5)word2vec–浅层的神经网络模型:适用于简单的语义关系–浅层语义,用这种模型训练出来的词向量进行向量之间的余弦值.说明：以向量空间模型为代表／例子： 向量空间模型(词向量模型)+余弦距离=余弦相似性:将文本转换成向量,之后向量间求余弦值. 涉及到的技术:wordEbeding技术,word2Vec模型构建,词典构建,特征词汇表的构建; 工具包:HANLP,FNLP,StardNLP. 3.2 基于句法特征的方法 问句的意思不仅仅有其词语组成，还与词语间的结构有关。基于句法的方法在词语语义方法的基础上，引入了句法特征，从句法特征角度考虑对应句法成分间的词语相似度，进而衡量句子间的相似度。 技术：句法分析，规则建立（Drool规则推理引擎）； 工具包：HANLP,FNLP,StardNLP之类的的句法分析器； 3.3 基于深度学习的方法 技术：深层语义匹配模型（DSSM、CLSM、DeepMatch、MatchingFeatures、ARC-II、DeepMind，具体依次参考下面的Reference） 工具包：matchzoo（ ①DSSM；②CDSSM；③ARC-I；④ARC-II；⑤MV-LSTM；⑥DRMM；⑦K-NRM；⑧CONV-KNRM；⑨DRMM-TKS；⑩BiMPM），allenNLP（ESIM），SPM_toolkit（DecAtt、ESIM、PWIM、SSE） 具体参考：自然语言推断(NLI)、文本相似度相关开源项目推荐(Pytorch 实现) 3.4 结论 只有融合了深度学习，语言逻辑，向量建模，规则，问答才能真正强大起来。而不是单单只靠某一某型就可以完全搞定。在query的过程中不断施加约束，或者说是action会有多个，我们可以从中选择出与quesion相似度得分最高的那个action作为最优的action,如此不断地施加约束，缩小范围，得到最终的结果。 四.相似度具体计算方法 关键词匹配（TF-IDF、BM25） 浅层语义匹配（WordEmbed隐语义模型，用word2vec或glove词向量直接累加构造的句向量） 深层语义匹配模型（DSSM、CLSM、DeepMatch、MatchingFeatures、ARC-II、DeepMind） 五.相似度种类及其方法5.1 词语相似度计算 词林编码法相似度 汉语语义法相似度 知网词语相似度 字面编辑距离法5.2 短语相似度计算 简单短语相似度5.3 句子相似度计算 词性和词序结合法 编辑距离算法 Gregor编辑距离法 优化编辑距离法5.4 文本相似度计算 余弦相似度 编辑距离算法 欧几里得距离 Jaccard相似性系数 Jaro距离 Jaro–Winkler距离 曼哈顿距离 SimHash + 汉明距离 Sørensen–Dice系数5.5 近似词 word2vec工具包(神经网络模型)六.向量空间/词向量模型与余弦相似性通俗解释以问句相似性为例:12句子A：我喜欢看电视，不喜欢看电影。句子B：我不喜欢看电视，也不喜欢看电影。 第一步，分词。12句子A：我/喜欢/看/电视，不/喜欢/看/电影。句子B：我/不/喜欢/看/电视，也/不/喜欢/看/电影。 第二步，列出所有的词。1我，喜欢，看，电视，电影，不，也。 第三步，计算词频。12句子A：我 1，喜欢 2，看 2，电视 1，电影 1，不 1，也 0。句子B：我 1，喜欢 2，看 2，电视 1，电影 1，不 2，也 1。 第四步，写出词频向量。12句子A：[1, 2, 2, 1, 1, 1, 0]句子B：[1, 2, 2, 1, 1, 2, 1] 到这里，问题就变成了如何计算这两个向量的相似程度, 说明：”余弦相似度”是一种非常有用的算法，值越大就表示越相似,只要是计算两个向量的相似程度，都可以采用它． 参考：TF-IDF与余弦相似性的应用（二）：找出相似文章 七.关键词提取 （１）TF-IDF:提取文中的关键词：词频乘以权重。 （２）”词频”（Term Frequency，缩写为TF）就是词在文中出现的次数或者百分比求得方法如下图： （３）权重即IDF（”逆文档频率”（Inverse Document Frequency，缩写为IDF））就是对应词的权重，其设置原则就是：修饰词、停用词、常见词、非常见词的权重越来越大，求得方法如下图： （４）参考：TF-IDF之关键词提取","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"语义解析","slug":"NLP/语义解析","permalink":"http://yoursite.com/categories/NLP/语义解析/"}],"tags":[{"name":"相似度","slug":"相似度","permalink":"http://yoursite.com/tags/相似度/"},{"name":"关键词抽取","slug":"关键词抽取","permalink":"http://yoursite.com/tags/关键词抽取/"}]},{"title":"word embeding、词向量、语言模型、word2vec","slug":"word embeding、词向量、语言模型、word2vec","date":"2018-11-19T00:41:36.000Z","updated":"2018-11-19T05:03:04.000Z","comments":true,"path":"2018/11/19/word embeding、词向量、语言模型、word2vec/","link":"","permalink":"http://yoursite.com/2018/11/19/word embeding、词向量、语言模型、word2vec/","excerpt":"本文主要是区别word embeding(词嵌入)、词向量、语言模型、word2vec之间的关系,word embeding是一种将将词转换到其他空间的思想,词向量是这种转换／思想的载体,语言模型则是词向量的逻辑实现,word2vec则是语言模型的代码实现即工具包,因此使用word2vec可以得到词向量.","text":"本文主要是区别word embeding(词嵌入)、词向量、语言模型、word2vec之间的关系,word embeding是一种将将词转换到其他空间的思想,词向量是这种转换／思想的载体,语言模型则是词向量的逻辑实现,word2vec则是语言模型的代码实现即工具包,因此使用word2vec可以得到词向量. 一.word embeding 二.词向量 三.语言模型 3.1 作用 3.2 类型 3.3 统计语言模型 3.4 神经网络语言模型 3.4.1 种类 3.4.2 实现工具 四.参考 一.word embeding 一种思想，就是将词转换到其他空间； 二.词向量词向量：将词转换到其他空间的表现形式就是词向量；词向量的两种类型：oneHot表示(热表示)–单纯表示词语、分布式表示–联系了上下文对该词语进行表示； （１）oneHot表示：以稠密的低维向量表示每个词 ①不能表示词向量间的位置信息,只是单纯的进行词向量增加构成句子的向量，－－使用共现矩阵来解决，原理就是某个词的意思跟它临近的单词是紧密相关的,至于窗口是多少,自己可以设置,但是没有解决向量维数问题.(因为是矩阵) ②向量的维数会随着句子的增加而增加，采用分布式表示解决． ③任意两个词之间都是孤立的，无法表示出在语义层面上词语词之间的相关信息 （２）分布式表示：以稠密的低维向量表示每个词,单独看其中一维的话没什么含义，但是组合到一起的vector就表达了这个词的语义信息（粒度上看的话，不止词，字、句子乃至篇章都可以有分布式表示； 三.语言模型3.1 作用 如何判断一个句子是否流畅？例如 我在学习 而不是 我玩学习 ，语言模型可以解决这个问题。模型是刻画某个词（下文称“目标词”）与其上下文之间的关系，用概率进行说明,通过训练语言模型可以得到词向量,之后进行概率求解,选取概率最大的词,之后判断所补充的词是否让句子通顺. 3.2 类型代表性的有两类模型:语言模型包括神经网络语言模型和统计语言模型. 3.3 统计语言模型 统计语言模型statistical language model就是给你几个词，在这几个词出现的前提下来计算某个词出现的（事后）概率． 统计语言模型的典型代表有n 元文法 (N-gram) ，但该模型有两个明显的缺陷：其一，在计算概率时只考虑到前 n-1 个词；其二，没有考虑到词语之间的相似性。例如，由 The cat is walking in the bedroom. 可以推测出 A dog was running in a room这句子的概率。因为这两句话在语义和语法结构上相似。 3.4 神经网络语言模型 基于神经网络的分布表示一般称为词向量、或分布式表示（distributed representation），为了解决统计语言模型的缺陷，Yoshua Bengio 等人提出了神经网络语言模型 。该语言模型： （１）词表中的每个词都对应一个m 维的特征向量，维数一般设为 300300 ，是远远小于词表大小的。当两个向量的相差不是很大时，则这两个向量代表的词相似。 （２）根据一段词序列中的每个特征向量可以得到这段序列的联合概率，即 P(S)P(S) 。 （３）在该模型中，神经网络的参数和词向量是同时进行训练的。 3.4.1 种类12345a) Neural Network Language Model ，NNLMb) Log-Bilinear Language Model， LBLc) Recurrent Neural Network based Language Model，RNNLMd) Collobert 和 Weston 在2008 年提出的 C&amp;W 模型e) Mikolov 等人提出了 CBOW（ Continuous Bagof-Words）和 Skip-gram 模型 CBOW也是神经网络语言模型的一种，顾名思义就是根据某个词前面的C个词或者前后C个连续的词，来计算某个词出现的概率。Skip-Gram Model相反，是根据某个词，然后分别计算它前后出现某几个词的各个概率。具体如下图： 总概率定义为： 3.4.2 实现工具 SENNA:C&amp;W模型的实现工具; Word2Vec:CBOW模型与Skip-Gram模型实现工具（１）word2vec word2vec是一个可以通过对数据进行训练，将词表达成向量形式的工具.可以求出分布式表示类型的词向量的工具包，是实现了两种神经网络语言模型的工具包，一个是CBOW模型，一个是Skip-Gram模型，因为通过训练语言模型可以得到词向量,因此使用word2vec工具包可以直接得到词向量. 四.参考 ❋Word Embedding与Word2Vec ❋词嵌入来龙去脉 word embedding、word2vec ❋深度学习 之 词向量(Word Embedding)篇 ：word2vec","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"名词辨析","slug":"NLP/名词辨析","permalink":"http://yoursite.com/categories/NLP/名词辨析/"}],"tags":[{"name":"word2vec","slug":"word2vec","permalink":"http://yoursite.com/tags/word2vec/"},{"name":"word embeding","slug":"word-embeding","permalink":"http://yoursite.com/tags/word-embeding/"}]},{"title":"常见问题及其设置（二）","slug":"常见问题及其设置（二）","date":"2018-11-18T06:01:07.000Z","updated":"2018-11-18T06:16:32.000Z","comments":true,"path":"2018/11/18/常见问题及其设置（二）/","link":"","permalink":"http://yoursite.com/2018/11/18/常见问题及其设置（二）/","excerpt":"本文主要是对软件安装及其一些常见配置的总结,也是常见问题及其设置系列第二篇,文中也参考了大量网友的智慧结晶,特此说声谢谢.","text":"本文主要是对软件安装及其一些常见配置的总结,也是常见问题及其设置系列第二篇,文中也参考了大量网友的智慧结晶,特此说声谢谢. 一.小键盘的home开启 二.坚果云解锁文件 三.调整屏幕亮度 四.表格分页增加续表 五.奇偶页的页眉设置 六.论文首页不显示页码 七.类图 八.右键添加cmder 九.安装tensorFlow与keras 十.腾讯云图床 十一.图床迁移 十二.快速将图片复制到剪切板 十三.文件夹备注 13.1 预前知识: 13.2 步骤 一.小键盘的home开启 关闭Num Lock键就是home键,end键也是同样道理,insert键也是如此; 二.坚果云解锁文件 坚果云锁定文件解锁 三.调整屏幕亮度 ctrl+alt+up/down 四.表格分页增加续表 表格分页增加续表 五.奇偶页的页眉设置 注意点： ❋记住是全文文档下的奇偶页不同； ❋页眉的第几章，可以自己加上去，有时候章数不对，看下当前的页眉有没有链接到前一页； ❋ctrl+backSpace：可以应对分节符的困扰 ❋ctrl+enter是分页 ❋奇偶页的页眉设置在不同章节下，需要用到分节符，记住是分节符不是分页符，同时不同章节要取消链接到前一页的页眉 六.论文首页不显示页码 ❋双击首页页码–》页码–》页面底端–》选择合适的格式即可（以节为单位） ❋之后点击首页不同，或者删除首页的页码即可 七.类图UML类图画法及其之间关系UML类图使用说明 八.右键添加cmder 在窗口输入cmder /register all然后回车即可 九.安装tensorFlow与keras tensorFlow1.12 ===&gt;keras2.2.4版本12345pip install tensorflowpip install keras测试:import tensorflow as tf tf.__version__测试:import keras 结果如下: 十.腾讯云图床 PicGo 十一.图床迁移 ipic+ipic mover(mac os下的软件),配置如下: 十二.快速将图片复制到剪切板 (1)对于图片:share快速截屏:ctrl+shift+X (2)之后快速复制图片到剪切板:ctrl+shift+c (3)设置gif图片上传: ❋设置打开历史图片快捷键 ❋操作如下: ❋使用ctrl+H打开历史图片,选中,之后右键复制文件,而不是图片 (4)屏幕录制(gif)–ctrl+shift+G (5)屏幕录制(MP4)–alt+printScreen,对于以后的项目展示,有比较大的帮助. 十三.文件夹备注13.1 预前知识: 了解desktop.ini的备注作用; py文件打包成exe; 13.2 步骤 (1)pip install pyinstaller; (2)pyinstaller -F 需要打包的py文件:其中-F说明只生成exe,其他文件不生成 (3)生成的.exe文件就在dist文件下，点击即可运行 (4)具体操作如下: (5)注意: ❋路径最好不要有中文; ❋设置应用程序快捷键:先将应用程序进行改名,之后添加到环境变量之中,Path变量之中.注意是软件所在的目录,不是软件本身 ❋如果不及时显示文件夹备注则需要进行:文件夹刷新具体操作如下:","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/工具/"},{"name":"小问题","slug":"工具/小问题","permalink":"http://yoursite.com/categories/工具/小问题/"}],"tags":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/tags/工具/"},{"name":"小问题","slug":"小问题","permalink":"http://yoursite.com/tags/小问题/"}]},{"title":"动图形象理解LSTM","slug":"动图理解LSTM","date":"2018-11-17T05:22:21.000Z","updated":"2019-01-22T13:50:44.560Z","comments":true,"path":"2018/11/17/动图理解LSTM/","link":"","permalink":"http://yoursite.com/2018/11/17/动图理解LSTM/","excerpt":"本文主要讲述的是LSTM的原理,结合动图进行阐述,遗忘门确定前一个步长中哪些相关的信息需要被保留；输入门确定当前输入中哪些信息是重要的，需要被添加的,被添加到细胞状态c；输出门确定下一个隐藏状态h应该是什么，细胞状态的哪些信息需要在下次输入被用到.其中隐藏状态h与输入文本的向量相加作为输入变量．其中激活函数Sigmoid用来进行信息选择,Tanh激活函数则是用来输入信息的规约,便于激活函数Sigmoid进行信息选择.","text":"本文主要讲述的是LSTM的原理,结合动图进行阐述,遗忘门确定前一个步长中哪些相关的信息需要被保留；输入门确定当前输入中哪些信息是重要的，需要被添加的,被添加到细胞状态c；输出门确定下一个隐藏状态h应该是什么，细胞状态的哪些信息需要在下次输入被用到.其中隐藏状态h与输入文本的向量相加作为输入变量．其中激活函数Sigmoid用来进行信息选择,Tanh激活函数则是用来输入信息的规约,便于激活函数Sigmoid进行信息选择. 一.短时记忆 二.LSTM 和 GRU 三.本质 四.RNN 述评 4.1 激活函数 Tanh 五.LSTM 5.1 核心概念 5.2 Sigmoid 5.3 遗忘门 5.4 输入门 5.5 细胞状态 5.6 输出门 六.代码示例 6.1 过程解释： 七.总结 本文转至：动图形象理解LSTM LSTM ——是一种特殊 RNN 类型，可以学习长期依赖信息。LSTM 由Hochreiter &amp; Schmidhuber (1997)提出，并在近期被Alex Graves进行了改良和推广。在很多应用问题，LSTM 都取得相当巨大的成功，并得到了广泛的使用。 然而LSTM结构复杂，初学者难于理解，本文通过动图形象直观的理解LSTM. 一.短时记忆 RNN 会受到短时记忆的影响。如果一条序列足够长，那它们将很难将信息从较早的时间步传送到后面的时间步。 因此，如果你正在尝试处理一段文本进行预测，RNN 可能从一开始就会遗漏重要信息。 在反向传播期间，RNN 会面临梯度消失的问题。 梯度是用于更新神经网络的权重值，消失的梯度问题是当梯度随着时间的推移传播时梯度下降，如果梯度值变得非常小，就不会继续学习。 因此，在递归神经网络中，获得小梯度更新的层会停止学习—— 那些通常是较早的层。 由于这些层不学习，RNN 可以忘记它在较长序列中看到的内容，因此具有短时记忆。 二.LSTM 和 GRU LSTM 和 GRU 是解决短时记忆问题的解决方案，它们具有称为“门”的内部机制，可以调节信息流。 这些“门”可以知道序列中哪些重要的数据是需要保留，而哪些是要删除的。 随后，它可以沿着长链序列传递相关信息以进行预测，几乎所有基于递归神经网络的技术成果都是通过这两个网络实现的。 LSTM 和 GRU 可以在语音识别、语音合成和文本生成中找到，你甚至可以用它们为视频生成字幕。对 LSTM 和 GRU 擅长处理长序列的原因，到这篇文章结束时你应该会有充分了解。下面我将通过直观解释和插图进行阐述，并避免尽可能多的数学运算。 三.本质 让我们从一个有趣的小实验开始吧。当你想在网上购买生活用品时，一般都会查看一下此前已购买该商品用户的评价。 当你浏览评论时，你的大脑下意识地只会记住重要的关键词，比如“amazing”和“awsome”这样的词汇，而不太会关心“this”、“give”、“all”、“should”等字样。如果朋友第二天问你用户评价都说了什么，那你可能不会一字不漏地记住它，而是会说出但大脑里记得的主要观点，比如“下次肯定还会来买”，那其他一些无关紧要的内容自然会从记忆中逐渐消失。 而这基本上就像是 LSTM 或 GRU 所做的那样，它们可以学习只保留相关信息来进行预测，并忘记不相关的数据。 四.RNN 述评 为了了解 LSTM 或 GRU 如何实现这一点，让我们回顾一下递归神经网络。 RNN 的工作原理如下；第一个词被转换成了机器可读的向量，然后 RNN 逐个处理向量序列。 处理时，RNN 将先前隐藏状态传递给序列的下一步。 而隐藏状态充当了神经网络记忆，它包含相关网络之前所见过的数据的信息。 让我们看看 RNN 的一个细胞，了解一下它如何计算隐藏状态。 首先，将输入和先前隐藏状态组合成向量， 该向量包含当前输入和先前输入的信息。 向量经过激活函数 tanh之后，输出的是新的隐藏状态或网络记忆。 4.1 激活函数 Tanh 激活函数 Tanh 用于帮助调节流经网络的值。 tanh 函数将数值始终限制在 -1 和 1 之间。 当向量流经神经网络时，由于有各种数学运算的缘故，它经历了许多变换。 因此想象让一个值继续乘以 3，你可以想到一些值是如何变成天文数字的，这让其他值看起来微不足道。 tanh 函数确保值保持在 -1~1 之间，从而调节了神经网络的输出。 你可以看到上面的相同值是如何保持在 tanh 函数所允许的边界之间的。 这是一个 RNN。 它内部的操作很少，但在适当的情形下（如短序列）运作的很好。 RNN 使用的计算资源比它的演化变体 LSTM 和 GRU 要少得多。 五.LSTM LSTM 的控制流程与 RNN 相似，它们都是在前向传播的过程中处理流经细胞的数据，不同之处在于 LSTM 中细胞的结构和运算有所变化。 这一系列运算操作使得 LSTM具有能选择保存信息或遗忘信息的功能。咋一看这些运算操作时可能有点复杂，但没关系下面将带你一步步了解这些运算操作。 5.1 核心概念 LSTM 的核心概念在于细胞状态以及“门”结构。细胞状态相当于信息传输的路径，让信息能在序列连中传递下去。你可以将其看作网络的“记忆”。理论上讲，细胞状态能够将序列处理过程中的相关信息一直传递下去。 因此，即使是较早时间步长的信息也能携带到较后时间步长的细胞中来，这克服了短时记忆的影响。信息的添加和移除我们通过“门”结构来实现，“门”结构在训练过程中会去学习该保存或遗忘哪些信息。 5.2 Sigmoid 门结构中包含着 sigmoid 激活函数。Sigmoid 激活函数与 tanh 函数类似，不同之处在于 sigmoid 是把值压缩到 0~1 之间而不是 -1~1 之间。这样的设置有助于更新或忘记信息，因为任何数乘以 0 都得 0，这部分信息就会剔除掉。同样的，任何数乘以 1 都得到它本身，这部分信息就会完美地保存下来。这样网络就能了解哪些数据是需要遗忘，哪些数据是需要保存。 接下来了解一下门结构的功能。LSTM 有三种类型的门结构：遗忘门、输入门和输出门。 5.3 遗忘门 遗忘门的功能是决定应丢弃或保留哪些信息。来自前一个隐藏状态的信息和当前输入的信息同时传递到 sigmoid 函数中去，输出值介于 0 和 1 之间，越接近 0 意味着越应该丢弃，越接近 1 意味着越应该保留。 5.4 输入门 输入门用于更新细胞状态。首先将前一层隐藏状态的信息和当前输入的信息传递到 sigmoid 函数中去。将值调整到 0~1 之间来决定要更新哪些信息。0 表示不重要，1 表示重要。 其次还要将前一层隐藏状态的信息和当前输入的信息传递到 tanh 函数中去，创造一个新的侯选值向量。最后将 sigmoid 的输出值与 tanh 的输出值相乘，sigmoid 的输出值将决定 tanh 的输出值中哪些信息是重要且需要保留下来的。 5.5 细胞状态 下一步，就是计算细胞状态。首先前一层的细胞状态与遗忘向量逐点相乘。如果它乘以接近 0 的值，意味着在新的细胞状态中，这些信息是需要丢弃掉的。然后再将该值与输入门的输出值逐点相加，将神经网络发现的新信息更新到细胞状态中去。至此，就得到了更新后的细胞状态。 5.6 输出门 输出门是GRU(Gate 门控循环单元)中不显式存在的门，用处是将最终记忆与隐状态分离开来。细胞状态中的信息(最终记忆)不是全部都需要存放到隐状态中，隐状态是个很重要的使用很频繁的东西，LSTM中每个gate都需要隐状态的参与。 输出门用来确定下一个隐藏状态的值，隐藏状态包含了先前输入的信息。首先，我们将前一个隐藏状态和当前输入传递到 sigmoid 函数中，然后将新得到的细胞状态传递给 tanh 函数。 最后将 tanh 的输出与 sigmoid 的输出相乘，以确定隐藏状态应携带的信息。再将隐藏状态作为当前细胞的输出，把新的细胞状态和新的隐藏状态传递到下一个时间步长中去。 让我们再梳理一下。遗忘门确定前一个步长中哪些相关的信息需要被保留；输入门确定当前输入中哪些信息是重要的，需要被添加的；输出门确定下一个隐藏状态应该是什么。 六.代码示例 对于那些懒得看文字的人来说，代码也许更好理解，下面给出一个用 python 写的示例。 6.1 过程解释：12345671.首先，我们将先前的隐藏状态和当前的输入连接起来，这里将它称为 combine；2.其次将 combine 丢到遗忘层中，用于删除不相关的数据；3.再用 combine 创建一个候选层，候选层中包含着可能要添加到细胞状态中的值；4.combine 同样要丢到输入层中，该层决定了候选层中哪些数据需要添加到新的细胞状态中；5.接下来细胞状态再根据遗忘层、候选层、输入层以及先前细胞状态的向量来计算；6.再计算当前细胞的输出；7.最后将输出与新的细胞状态逐点相乘以,实现细胞状态与新的隐藏状态进行分离。 七.总结 总而言之，RNN适用于处理序列数据和预测任务，但会受到短期记忆的影响。LSTM通过引入门结构来减弱短期记忆影响的演化变体，其中门结构可用来调节流经序列链的信息流。目前，LSTM经常被用于语音识别、语音合成和自然语言理解等多个深度学习应用中。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"LSTM","slug":"深度学习/LSTM","permalink":"http://yoursite.com/categories/深度学习/LSTM/"}],"tags":[{"name":"LSTM","slug":"LSTM","permalink":"http://yoursite.com/tags/LSTM/"},{"name":"RNN","slug":"RNN","permalink":"http://yoursite.com/tags/RNN/"}]},{"title":"实体识别方法--感知机框架","slug":"实体识别方法--感知机框架","date":"2018-11-12T03:01:43.000Z","updated":"2018-11-13T04:34:04.000Z","comments":true,"path":"2018/11/12/实体识别方法--感知机框架/","link":"","permalink":"http://yoursite.com/2018/11/12/实体识别方法--感知机框架/","excerpt":"本文主要讲述了实体识别一个方法–感知机标注框架,该框架可以实现增量学习,自定义标注类型,同时分词速度也很快.并且已经有集成的接口,能够使得开发成本也大大降低.","text":"本文主要讲述了实体识别一个方法–感知机标注框架,该框架可以实现增量学习,自定义标注类型,同时分词速度也很快.并且已经有集成的接口,能够使得开发成本也大大降低. 一.实验方法选择 1.1 BiLstm+CRF 1.2 感知机标注框架 二.语料库的准备 三.实验过程 3.1 中文分词模型 3.1.1 模型的训练 3.1.2 模型评估 3.1.3 模型的加载,使用及其增量更新 3.2 词性标注模型 3.2.1 模型的的训练及保存 3.2.2 模型的使用 3.3 命名实体模型 3.3.1 模型的训练 3.3.2 模型的使用 3.4 词法分析器 四.其他 4.1 模型的加载与保存 4.2 语料库(训练集,验证集)和模型保存路径 4.3 模型的知识(重点) 4.3.1 模型的内容及其存放位置 4.3.2 模型训练文件的存放位置 一.实验方法选择 本文中标签与标记是同一概念,实体识别的方法有很多:CRF,BiLstm+CRF,感知机标注框架 1.1 BiLstm+CRF 两种实体类型：人物(Person)和机构(Organization)。同时假设采用BIO标注体系。因此会有五种实体标签：12345B-PersonI-PersonB-OrganizationI-OrganizationO B代表实体的开始部分,I代表实体的其他部分,O代表其他部分.从下往上看: 第一层：表示层： 将每个句子表示为词向量和字向量。 第二层：BiLSTM层： 输入词向量和字向量到模型中的BiLSTM层，该层的输出是句子的每个词的所有标签的各自得分。 【注】此处的标签的各自得分充当的是CRF模型中的非归一化的发射概率。(也就是没有将概率约定在0-1之间)在本例中就是五种标签的各自得分，如B-Person(1.5),I-Person(0.9),B-Organization(0.1),I-Organization(0.08),O(0.05)。 第三层：CRF层： 该层使用BiLSTM层的输出——每个词的所有标签的各自得分，由于没有考虑到前后标签的关联,因此CRF层主要计算标签之间的转移概率来体现前后标签的关联,如在当前标签下,通过计算下一个标签的转移概率,转移概率大的标签则作为当前标签的下一个标签. 虽然准确率有较大提升,但是由于需要重新构建(算法及其语料库),成本较高,并且不支持在线学习. 1.2 感知机标注框架 感知机标注框架有结构化感知机标注框架(多线程中使用,默认)和平均化感知机框架(常在单线程中使用),已经集成在HANLP分词器当中,优点就是读取的速度快;能自定义实体所属的标签/记;最重要的是能够进行增量学习,并且在线保存对应的模型. 感知机标注框架其主要是有三部分组成:中文分词(CWS),词性标注(POS),和实体识别,这三者就构成了词法分析器.(注,最终目的还是进行实体识别) 二.语料库的准备 语料库使用的是关于企业的标注集,形式如下,标注集采用1998人民日报的标注格式,形式如下: 三.实验过程 本文以中文分词模型的训练,保存,加载,学习,使用,更新进行说明,后面的模型的使用方法与之前类似. 3.1 中文分词模型3.1.1 模型的训练使用流程为: 新建分词模型+进行模型的训练,保存.123456789101112131415161718public class trainCWS &#123; @Test public void testcws() throws IOException &#123; /** *新建分词器,向上类型转为 PerceptronTrainer类型 */ PerceptronTrainer cwstrainer = new CWSTrainer(); /** * 进行模型的训练,参数主要包括:训练集,验证集,模型保存路径,模型压缩比,迭代次数,线程数 * 视语料与任务的不同，迭代数、压缩比和线程数都可以自由调整，以保证最佳结果: */ PerceptronTrainer.Result train_result = cwstrainer.train(cws_trainingFile,cws_developFile,cws_modelSavePath); /* *模型训练时的准确率 */ System.out.println(train_result.getAccuracy()); &#125;&#125; 3.1.2 模型评估 主要使用evaluate方法进行评估．123456789101112131415public class evaluateCWSModel &#123; @Test public void evaluatecwsmodel() throws IOException &#123; /** * 测试模型的精确度,使用验证集;之前训练模型测试精度的时候使用训练集 */ LinearModel model = new LinearModel(cws_modelSavePath); CWSTrainer cwstrainer = new CWSTrainer(); //训练集作为语料库 double[] prf = cwstrainer.evaluate(cws_developFile,model); System.out.println(\"准确率Precision为:\"+prf[0]); System.out.println(\"召回率Recall为:\"+prf[1]); System.out.println(\"F值为:\"+prf[2]); &#125;&#125; 3.1.3 模型的加载,使用及其增量更新使用流程为: 模型的加载+测试初始模型(分词不正确)+学习分词+再次使用模型测试(分词正确)+模型保存.123456789101112131415161718192021public class testCWS &#123; @Test public void testcws() throws IOException &#123; /* *模型加载 */ PerceptronSegmenter segmenter = new PerceptronSegmenter(cws_modelSavePath); List&lt;String&gt; segment_sentence = segmenter.segment(\"今天是下雨天\"); System.out.println(segment_sentence); /** * 学习分词 */ segmenter.learn(\"今天 是 下雨天\"); List&lt;String&gt; segment_sentence1 = segmenter.segment(\"今天是下雨天\"); System.out.println(segment_sentence1); /** * 模型更新 */ segmenter.getModel().save(cws_modelSavePath); &#125;&#125; 输出结果为:1234//下面这个是因为:这是在模型已经更新的情况下训练的.[今天, 是, 下雨天]//说明学习成功[今天, 是, 下雨天] 3.2 词性标注模型3.2.1 模型的的训练及保存1234567891011121314151617public class PosTrainer &#123; @Test public void postrainer() throws IOException &#123; /** * 新建词性标注模型 */ PerceptronTrainer postrainer = new POSTrainer(); /** * 模型训练及保存 */ PerceptronTrainer.Result posmodel = postrainer.train(pos_trainingFile,pos_developFile,pos_modelSavePath,0.1,5,4); /** * 模型训练时的准确率 */ System.out.println(\"模型训练时的准确率为:\"+posmodel.getAccuracy()); &#125;&#125; 模型评估就不在细说了,重点说下,模型的使用; 3.2.2 模型的使用12345678public class testPosModel &#123; @Test public void testposmodel() throws IOException &#123; PerceptronPOSTagger postagger = new PerceptronPOSTagger(pos_modelSavePath); System.out.println(Arrays.toString(postagger.tag(\"中国 交响乐团 谭利华 在 布达拉宫 广场 演出\".split(\" \")))); &#125;&#125; 输出结果为:12//词性的测试[nz, ns, n, p, r, ns, n] 3.3 命名实体模型3.3.1 模型的训练主要流程： 实体识别模型建立,添加自定义标注如companyName,训练模型.123456789101112131415161718192021public class NER &#123; @Test public void ner() throws IOException &#123; /** * 先定义好实体识别的标注集,下文就添加了companyName这个自定义实体, * 标注集,因此最好在模型定义阶段就定义好 */ PerceptronTrainer nertrainer = new NERTrainer() &#123; @Override protected TagSet createTagSet() &#123; NERTagSet tagSet = new NERTagSet(); tagSet.nerLabels.add(\"companyName\"); tagSet.nerLabels.add(\"prizeName\"); return tagSet; &#125; &#125;;//这个是分号说明是服务于NERTrainer的 PerceptronTrainer.Result nerModel = nertrainer.train(ner_trainingFile,ner_developFile, ner_modelSavePath,0.1,4,4); &#125;&#125; 3.3.2 模型的使用123456789101112public class testNer &#123; @Test public void testner() throws IOException &#123; /** * 命名实体识别器的输入不再是纯文本，而是分词结果与词性标注结果： * 如果不是的话,则需要先进行 */ PerceptronNERecognizer perceptronnerecognizer = new PerceptronNERecognizer(ner_modelSavePath); String[] ner_result = perceptronnerecognizer.recognize(\"天津 渤海银行 的 总部\".split(\" \"), \"ns n n nr p ns n\".split(\" \")); System.out.println(Arrays.toString(ner_result)); &#125;&#125; 输出结果为:1[B-companyName, E-companyName, O, O] 对应标注集: 3.4 词法分析器 词法分析器则是由三个模型进行整合和,主要是通过PerceptronLexicalAnalyzer(分词模型的路径,词性标注模型的路径,实体识别的路径),具体实现如下图:12345678910111213141516171819202122232425public class Analyse &#123; public static final String SENTENCE = \"天津渤海银行公司位于哪里\"; @Test public void analyse() throws IOException &#123; /** * 整合模型 */ PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer(cws_modelSavePath, pos_modelSavePath, ner_modelSavePath); /** * 允许使用自定义词典 */ analyzer.enableCustomDictionaryForcing(true); //学习完之后注释,得到的结果还是一样,说明模型学习到了,注意是---先学习后分词// analyzer.learn(\"[天津/ns 渤海/ns 银行/n 公司/n]/companyName 位于/v 哪里/r\"); //分析句子 Sentence result1 = analyzer.analyze(SENTENCE); //更新学习到的分词 analyzer.getPerceptronSegmenter().getModel().save(cws_modelSavePath); //更新学习到的标注 analyzer.getPerceptronPOSTagger().getModel().save(pos_modelSavePath); //更新学习到的实体识别 analyzer.getPerceptionNERecognizer().getModel().save(ner_modelSavePath); System.out.println(result1); &#125;&#125; 输出结果为:1[天津/ns 渤海银行/companyName 公司/ns]/companyName 位于/v 哪里/r 其中渤海银行/companyName在自定义词典当中已经设置,因此会优先盖过标注集的词性,但要注意要使得这个companyName成功标记,那么就需要在实体识别模型新建的时候就要将这个自定义标记添加到里面去.1234567891011PerceptronTrainer nertrainer = new NERTrainer() &#123; @Override protected TagSet createTagSet() &#123; NERTagSet tagSet = new NERTagSet(); tagSet.nerLabels.add(\"companyName\"); tagSet.nerLabels.add(\"prizeName\"); return tagSet; &#125; &#125;; 否则,该标记则不能进行识别.各个模型在使用阶段最好明确是否需要使用自定义字典. 四.其他4.1 模型的加载与保存模型加载:123PerceptronSegmenter segmenter = new PerceptronSegmenter(cws_modelSavePath);PerceptronPOSTagger postagger = new PerceptronPOSTagger(pos_modelSavePath);PerceptronNERecognizer perceptronnerecognizer = new PerceptronNERecognizer(ner_modelSavePath); 模型更新:123456//更新学习到的分词analyzer.getPerceptronSegmenter().getModel().save(cws_modelSavePath);//更新学习到的标注analyzer.getPerceptronPOSTagger().getModel().save(pos_modelSavePath);//更新学习到的实体识别analyzer.getPerceptionNERecognizer().getModel().save(ner_modelSavePath); 4.2 语料库(训练集,验证集)和模型保存路径 最好保存在一个类如config类,便于修改1234567891011121314151617181920public class Config &#123; /** * 分词的训练集,验证集,模型保存路径 */ public static final String cws_trainingFile = \"E:/Project/Pratice/src/main/java/com/example/demo/model/perceptron/data/199801.txt\"; public static final String cws_developFile = \"E:/Project/Pratice/src/main/java/com/example/demo/model/perceptron/data/199801.txt\"; public static final String cws_modelSavePath = \"E:/Project/Pratice/src/main/java/com/example/demo/model/perceptron/cws.bin\"; /** * 标注的训练集,验证集,模型保存路径 */ public static final String pos_modelSavePath = \"E:/Project/Pratice/src/main/java/com/example/demo/model/perceptron/pos.bin\"; public static final String pos_trainingFile = \"E:/Project/Pratice/src/main/java/com/example/demo/model/perceptron/data/199801.txt\"; public static final String pos_developFile = \"E:/Project/Pratice/src/main/java/com/example/demo/model/perceptron/data/199801.txt\"; /** * 实体识别 */ public static final String ner_trainingFile = \"E:/Project/Pratice/src/main/java/com/example/demo/model/perceptron/data/199801.txt\"; public static final String ner_developFile = \"E:/Project/Pratice/src/main/java/com/example/demo/model/perceptron/data/199801.txt\"; public static final String ner_modelSavePath = \"E:/Project/Pratice/src/main/java/com/example/demo/model/perceptron/ner.bin\";&#125; 4.3 模型的知识(重点)4.3.1 模型的内容及其存放位置 模型文件指词典、训练后的中文分词器、POS标注器.后面可以根据自己的需要,添加分类器,实体识别器等等,默认情况是它们位于HANLP数据/data包下“models”目录之中。可以将此目录复制到Eclipse项目目录或者IDEA模块目录之下,也可以自己在训练的时候就指定模型文件的存储目录,其中HANLP最后的模型文件是以.bin的形式编码进行说明的. 4.3.2 模型训练文件的存放位置 模型的训练代码文件可以使用命令行的形式,也可以使用编码的形式,可以将训练代码文件存在main目录下或者test目录下.接下来就可以编程调用了.HANLP提供了一系列中文处理工具，其中中文分词、词性标注、实体名识别等基础功能已经封装在HANLP类之中。","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"实体识别","slug":"NLP/实体识别","permalink":"http://yoursite.com/categories/NLP/实体识别/"}],"tags":[{"name":"词性标注","slug":"词性标注","permalink":"http://yoursite.com/tags/词性标注/"},{"name":"分词","slug":"分词","permalink":"http://yoursite.com/tags/分词/"}]},{"title":"I/O流系列之readLine()(二)","slug":"IO流系列之readLine()(二)","date":"2018-10-21T16:21:47.000Z","updated":"2018-11-13T02:29:00.000Z","comments":true,"path":"2018/10/22/IO流系列之readLine()(二)/","link":"","permalink":"http://yoursite.com/2018/10/22/IO流系列之readLine()(二)/","excerpt":"本文就是专门讲的是readLine()的数据读取策略,只要出现,readLine()就会进行下一行的读取,那么就需要一个变量来接收readLine()读取的结果–String类型.","text":"本文就是专门讲的是readLine()的数据读取策略,只要出现,readLine()就会进行下一行的读取,那么就需要一个变量来接收readLine()读取的结果–String类型. BufferedReader.readLine()只要是读取了就会跳到下一行,因此最好用用一个字符串变量接收br.readLine()值,否则再次使用的时候会出现跳行读取,也容易形成空行读取,造成Null,如:12345678910while (br.readLine()!=null)&#123; //读取的是第一行 //读取的是第二行,如果文本刚好只有一行,那么这个就会造成null. String[] dict = br.readLine().split(\":\"); System.out.println(dict); int index = Integer.parseInt(dict[0]); String word = dict[1]; dictMap.put(word,index); &#125; return dictMap; &#125; 所以第一二行改为:12345String line=\"\"; while ((line=br.readLine())!=null)&#123; //line保存第一行的读取 //因为是值传递方式,line一直保存的结果 String[] dict = ; line.split(\":\"); br.readLine()使用经常会出现null指针的情况 ,原因:br.readLine()!=null说明已经读取了文件的一行数据,如果再次通过br.readLine().split(&quot;:&quot;);读取时应该是第二行的数据.所以,如果文件总共只有一行,从读取到输出就会进行两行的读取,会出现null的情况","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"I/O流","slug":"java/I-O流","permalink":"http://yoursite.com/categories/java/I-O流/"}],"tags":[{"name":"readLine()","slug":"readLine","permalink":"http://yoursite.com/tags/readLine/"}]},{"title":"I/O流系列之文件读取(一)","slug":"IO流系列之文件读取(一)","date":"2018-10-21T16:13:30.000Z","updated":"2018-11-13T04:01:20.000Z","comments":true,"path":"2018/10/22/IO流系列之文件读取(一)/","link":"","permalink":"http://yoursite.com/2018/10/22/IO流系列之文件读取(一)/","excerpt":"本文主要是对java中I/O流的接口与类之间的关系,做一个总结,I/O流主要包括输入流与输出流,其中输入流为:java.io.Reader和java.io.InputStream两个接口,输出流:java.io.Writer和java.io.OutputStream.","text":"本文主要是对java中I/O流的接口与类之间的关系,做一个总结,I/O流主要包括输入流与输出流,其中输入流为:java.io.Reader和java.io.InputStream两个接口,输出流:java.io.Writer和java.io.OutputStream. 一.总体架构 二.fileInputStream,InputStreamReader,FileReader 2.1 InputStreamReader类 2.2 BufferedReader类: 一.总体架构 ❋BufferedReader,BufferedWriter是高效字符流; ❋java.io.Reader和java.io.InputStream组成了Java输入类。 ❋java.io.Writer和java.io.OutputStream组成了Java输出类。 二.fileInputStream,InputStreamReader,FileReader fileInputStream FileReader 读取方式 按字节读取 字符读取 读取结果 byte[]数组 String或者char(java中都是16位) 读取的数据 二进制数据 字符 是否与编码有关 编码无关 涉及编码 所属类别 java.io.InputStream java.io.Reader.InputStreamReader 优点 快速,高效,没有乱码 直观,容易理解 缺点 按字节读取,不直观,不利于阅读 涉及到乱码问题 使用场景 非纯文本文件居多 只在纯文本文件 2.1 InputStreamReader类 (1)作用：是用来进行字节流与字符流转换的. (2)其实该类也可以直接读取文件,FileReader因为就是该类的子类,方法也是从那里继承的. (3)是Reader接口的实现类,通过设置字符编码,将读取到的字节流转换成字符流的核心类,转换方法是该类独有,子类也不继承.如果字符编码没有设置,则按系统ANSI字符集的默认编码读取.(简体中文系统为GB2312),最后得到的是InputStreamReader对象或者父类-Reader的对象; 2.2 BufferedReader类: (1)作用:是InputStreamReader类的子类,用来设置字符流的缓冲区大小的，提高文件读取效率. (2)从字符输入流中读取文本，缓冲各个字符，从而实现字符、数组和行的高效读取。可以指定缓冲区的大小，或者可使用默认的大小。大多数情况下，默认值就足够大了。 (3)通常，Reader 所作的每个读取请求都会导致对底层字符或字节流进行相应的读取请求。因此，建议用 BufferedReader 包装所有其 read() 操作可能开销很高的 Reader（如 FileReader 和 InputStreamReader）。例如，1BufferedReader in= new BufferedReader(new FileReader(\"foo.in\")); 该方法将缓冲指定文件的输入。如果没有缓冲，**则每次调用 read() 或 readLine() 都会导致从文件中读取字节，并将其转换为字符后返回，而这是极其低效的**。具体的一个文件读取代码如下:123456789101112131415161718192021222324252627package com.example.qa.Process.Load;import java.io.*;import java.util.HashMap;import static com.example.qa.Process.Load.loadClassifyModel.rootPath;public class loadIndexQuestion &#123; public String getPattern(String s, int index) throws IOException &#123; //新建文件对象 File file = new File(rootPath + s); //按字节流方式读取文件--new FileInputStream(file) //new InputStreamReader(字节流对象,字符编码)=字符流对象 Reader isr = new InputStreamReader(new FileInputStream(file), \"GB2312\"); //将读取的字符流存入缓冲区,减少直接读取文件的次数,提高性能 BufferedReader br = new BufferedReader(isr); String line=\"\"; HashMap&lt;Integer, String&gt; index2Pattern = new HashMap&lt;Integer, String&gt;(); //声明:按字符流读取文件,结果一般为string(常用)或者char,都是16位(java中) //缓冲区中按行读取,br.readLine()方法注意, while ((line=br.readLine())!=null)&#123; String[] fileSplit=line.split(\":\"); int patternIndex = Integer.parseInt(fileSplit[0].toString()); String pattern = fileSplit[1]; index2Pattern.put(patternIndex,pattern); &#125; String StructQuery = index2Pattern.get(index); return StructQuery; &#125;&#125;","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"I/O流","slug":"java/I-O流","permalink":"http://yoursite.com/categories/java/I-O流/"}],"tags":[{"name":"I/O流体系结构","slug":"I-O流体系结构","permalink":"http://yoursite.com/tags/I-O流体系结构/"},{"name":"InputStreamReader","slug":"InputStreamReader","permalink":"http://yoursite.com/tags/InputStreamReader/"}]},{"title":"集合系列之集合转换与合并(六)","slug":"集合系列之集合转换与合并(六)","date":"2018-10-21T16:05:15.000Z","updated":"2018-11-13T03:02:20.000Z","comments":true,"path":"2018/10/22/集合系列之集合转换与合并(六)/","link":"","permalink":"http://yoursite.com/2018/10/22/集合系列之集合转换与合并(六)/","excerpt":"集合转换，在项目开发当中用的很频繁，本文专门总结了集合之间的转化，主要是数组与列表的转换，因为数组有个大小固定的缺陷，需要转换成list集合进行弥补,在转换的过程中主要有两个注意的地方:基本数据类型数组的转换,与AsList方法长度修改问题.","text":"集合转换，在项目开发当中用的很频繁，本文专门总结了集合之间的转化，主要是数组与列表的转换，因为数组有个大小固定的缺陷，需要转换成list集合进行弥补,在转换的过程中主要有两个注意的地方:基本数据类型数组的转换,与AsList方法长度修改问题. 一.说明 二.原因 三.Array与ArrayList 3.1 区别 3.2 Array转List (1) int[]的转换问题 (2) asList问题 3.3 List转Array 四.合并 五.参考文献 一.说明 基于效率和类型检验，应尽可能使用Array，无法确定数组大小时才使用ArrayList！ 二.原因(1)List长度动态增加解决Array长度固定问题 主要是将数组Array与ArrayList/List之间的转换,因为Array数组在初始化的时候就需要将数组的大小固定之后不可更改,所以如果需要增加数据的话,就会带来转换问题,而List刚好可以解决长度固定问题,其中数组Array与ArrayList,因为底层都是数组,所以ArrayList可以想象成一种“会自动扩增容量的Array因此转换起来难度更低,也更常用;(2)集合类型作用范围比较明确: List/Set集合重在存储,Map集合重在查找. 三.Array与ArrayList3.1 区别 Array ArrayList 实例化 声明的同时必须进行实例化(至少长度要确定下来),如:int[] arr=new int[3] 不强制实例,可以单独声明,如: ArrayList myList = new ArrayList(); 对象类型 要同类型 可以不同类型,统一封装成Objective类型对象 存放的地址 地址连续的 可以不连续 对象大小 固定 不固定 3.2 Array转List 因为数组Array与ArrayList,因为底层都是数组,所以ArrayList可以想象成一种“会自动扩增容量的Array因此转换起来难度更低,也更常用．总结来说使用的是Arrays.asList(数组名)即可,但其间涉及到如下问题: (1) int[]的转换问题123456789public class App &#123; public static void main(String[] args) &#123; int[] intarray = &#123;1, 2, 3, 4, 5&#125;; List&lt;int[]&gt; list = Arrays.asList(intarray); System.out.println(list); System.out.println(\"List的长度为:\"list.size()); &#125;&#125; 输出为:12[[I@66d3c617]List的长度为:1 造成上述现象: 因为集合(List是集合之一)只存储对象类型/引用类型数据1List&lt;int[]&gt; list = Arrays.asList(intarray); 因为int[]是一个基本类型数组,本质是一个数组,所以是一个对象,因此List集合能接收这个数组,但是不能接收数组里面的元素,因为他们不属于对象/引用类型,而是基本类型数据-int型数据,因此List的长度只为1,就是这个原由.所以List&lt;int[]&gt; list = Arrays.asList(intarray);虽说接收成功了,但是解析不成功,为此我们需要将int[]里面的数据类型转换成对象类型数据,恰好int有包装类对象,可以使用这个包装类中的方法自动或者手动封装为对象类型,因此有两种修改方法:手动封装:12Integer[] intarray = &#123;1, 2, 3, 4, 5&#125;;List&lt;Integer&gt; list = Arrays.asList(intarray); 或者自动封装,保持int[]数组,但是List集合元素变为Integer类型(因为能自动对int型数组元素进行自动封装为Interger对象类型,都变成对象)12//将里面的元素变为IntegerList&lt;Integer&gt; list = Arrays.asList(intarray); (2) asList问题 asList返回的的是数组的视图,因此是定长的,不可修改List的长度(也就是add,remove之类修改长度的操作失效)注意: 其实通过asList方法得到的List是只读的。因此下面这些代码会报错误:12345678910public List&lt;String&gt; getsearchcondition(String sentence) throws IOException &#123; String query = new queryExtension().queryextension(sentence); String[] StructQuery = query.split(\" \"); //asList方法得到的List是只读的。 List&lt;String&gt; lastQuery=Arrays.asList(StructQuery); String strIndex = String.valueOf(index); lastQuery.add(strIndex); return lastQuery; &#125;&#125; 因此我们可以在新建个集合来接收这个asList方法得到的List数组,方法如下:1List&lt;String&gt; lastQuery= Lists.newArrayList(Arrays.asList(StructQuery)); 最终运行结果为:1[zzxp, 年龄, 0] 3.3 List转Array主要使用arrayList.toArray(列表名)即可,但注意: (1)前提List存储的元素类型要相同,如:[xian, d, 3]否则不能转换成数组Array,成功结果为:{ &quot;xian&quot;, &quot;d&quot;, &quot;3&quot; } (2)其中[xian, d, 3]表面看起来是由int和字符串,但是系统会统一会转换成字符串. 四.合并 ❋假设list1与list2都是实例化的相同类型的List–list1.add(list2) ❋假设arr1与arr2都是实例化的相同类型的Array–String[] ll = ArrayUtils.addAll(arr1,arr2) 五.参考文献 ❋java中Array和ArrayList区别 ❋ArrayList和Array之间的转换","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"集合","slug":"java/集合","permalink":"http://yoursite.com/categories/java/集合/"}],"tags":[{"name":"集合转换","slug":"集合转换","permalink":"http://yoursite.com/tags/集合转换/"},{"name":"AsList","slug":"AsList","permalink":"http://yoursite.com/tags/AsList/"}]},{"title":"集合系列之集合输出(五)","slug":"集合系列之集合输出(五)","date":"2018-10-21T16:00:50.000Z","updated":"2018-11-13T03:02:00.000Z","comments":true,"path":"2018/10/22/集合系列之集合输出(五)/","link":"","permalink":"http://yoursite.com/2018/10/22/集合系列之集合输出(五)/","excerpt":"集合的输出,历来是项目开发的基本功,为此本章专门针对不同集合的输出方式进行了总结,输出方式有两种:逐个输出和整体输出.","text":"集合的输出,历来是项目开发的基本功,为此本章专门针对不同集合的输出方式进行了总结,输出方式有两种:逐个输出和整体输出. 一.Array数组 1.1 逐个输出 1.2 整体输出 二.List/Set输出 2.1 逐个输出 2.2 整体输出 三.Map输出 3.1 逐个输出 (1)增强for (2)迭代器 一.Array数组1.1 逐个输出增强for输出:123456//Integer是元素的类型Integer[] dd=&#123;1,2,5&#125;for(Integer i:dd)&#123; System.out.println(i);&#125; 1.2 整体输出直接System.out.println()输出即可,代码如下:123int[] intArray = new int[] &#123;1, 2, 3, 4, 5&#125;;System.out.println(Arrays.toString(intArray));//输出: &#123;1, 2, 3, 4, 5&#125; 二.List/Set输出2.1 逐个输出增强for输出:123456List&lt;String&gt; list = new ArrayList&lt;String&gt;(); list.add(\"a\"); list.add(\"b\"); for(String ss:list) &#123; System.out.println(ss); &#125; 2.2 整体输出直接System.out.println()输出即可,代码如下:1234List&lt;String&gt; list = new ArrayList&lt;String&gt;(); list.add(\"a\"); list.add(\"b\"); System.out.println(list); 三.Map输出3.1 逐个输出主要有两种常用方法:增强for与迭代器输出1234//value是Character-char的包装类 Map&lt;String, Character&gt; map =new LinkedHashMap&lt;String, Character&gt;(); map.put(\"一\", '1'); map.put(\"二\", '2'); (1)增强for123 for (String string : map.keySet();) &#123; System.out.println(string+\" \"+map.get(string));&#125; (2)迭代器123456 Iterator&lt;String&gt; iterator = set.iterator(); while(iterator.hasNext())&#123; String str=iterator.next(); Integer integer = map.get(str); System.out.println(str+\" \"+integer);&#125; 其中LinkedHashMap是有序的,HashMap是无序的","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"集合","slug":"java/集合","permalink":"http://yoursite.com/categories/java/集合/"}],"tags":[{"name":"集合输出","slug":"集合输出","permalink":"http://yoursite.com/tags/集合输出/"},{"name":"迭代器输出","slug":"迭代器输出","permalink":"http://yoursite.com/tags/迭代器输出/"}]},{"title":"集合系列之集合格式(四)","slug":"集合系列之集合格式(四)","date":"2018-10-21T15:56:22.000Z","updated":"2018-11-13T03:01:10.000Z","comments":true,"path":"2018/10/21/集合系列之集合格式(四)/","link":"","permalink":"http://yoursite.com/2018/10/21/集合系列之集合格式(四)/","excerpt":"本节是根据本人在开发当中涉及到的一些常用知识点(各集合形式)进行介绍,针对性较强,遗漏部分会在后期的文章中进行说明.","text":"本节是根据本人在开发当中涉及到的一些常用知识点(各集合形式)进行介绍,针对性较强,遗漏部分会在后期的文章中进行说明. 一.List/列表的形式–[]: 2.1 组成部分 2.2 ArrayList与LinkedList 二.Array/数组的形式–{} 三.Map形式 四.set形式 一.List/列表的形式–[]:2.1 组成部分实现类: ArrayList ,LinkedList,Vectord.都是List接口的实现类.123List&lt;String&gt; str=['physics', 'chemistry', ''1997', '2000'];Linked list : [D, A, E, B, C, F, G]向量vector:[0,0,0,1,0,1]之类的,还有double[],int[]之类的 2.2 ArrayList与LinkedList ArrayList LinkedList 字面含义 数组表 链表 底层结构 与Vector一样,都是数组数据结构 链表数据结构 优点 根据索引查询快 增删,修改快 缺点 修改慢,因为数组元素移动成本较高 修改方便,只需要修改左右两侧节点的地址 场景 查找 修改 结构 基于数组实现的List 双向链表 线程安全 不安全 不安全 注: 正因为 ArrayList与LinkedList都有各自的优缺点,所以后面出现了一种新的集合-Map,结合了两种集合的优点. Vector:虽然是线程安全,多数方法都被synchronized修饰的List实现；但现在用到比较少. 二.Array/数组的形式–{}1String[] strings=&#123;\"1\",\"2\",\"3\"&#125;; 三.Map形式1&#123;key1=value, key2=value&#125; 实现类:LinkedHashMap是有序的,HashMap是无序的 四.set形式1234list = [0,2,2,4]//不重复set = [0,2,4]map = &#123;a=1, b=2, c=3&#125; 实现类:sortSet是有序的,HashSet是无序的 其中set集合可以设置为有序–使用sortSet类对象存储:这里的“有序”，“无序”指的是存放元素是是否会记住元素存放的顺序，并非对元素进行“排序”.","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"集合","slug":"java/集合","permalink":"http://yoursite.com/categories/java/集合/"}],"tags":[{"name":"集合形式","slug":"集合形式","permalink":"http://yoursite.com/tags/集合形式/"},{"name":"集合类型","slug":"集合类型","permalink":"http://yoursite.com/tags/集合类型/"}]},{"title":"集合系列之集合工具类(三)","slug":"集合系列之集合工具类(三)","date":"2018-10-21T15:48:14.000Z","updated":"2018-11-13T03:01:32.000Z","comments":true,"path":"2018/10/21/集合系列之集合工具类(三)/","link":"","permalink":"http://yoursite.com/2018/10/21/集合系列之集合工具类(三)/","excerpt":"因为集合工具类是静态类,不需要进行实例化对象就可以进行方法的调用.集合工具类除了java内置的,还有GUAVA这个工具类插件,使用体验也是非常不错.","text":"因为集合工具类是静态类,不需要进行实例化对象就可以进行方法的调用.集合工具类除了java内置的,还有GUAVA这个工具类插件,使用体验也是非常不错. 一.各类型对象的工具类 二.Conllections与Collection 三.Arrays与Array 四.Map与Maps 一.各类型对象的工具类 任何对JDK集合框架有经验的程序员都熟悉和喜欢java.util.Collections包含的工具方法。Guava沿着这些路线提供了更多的工具方法：适用于所有集合的静态方法。 集合接口 属于**JDK还是Guava** 对应的**Guava**工具类 Collection JDK Collections2：不要和java.util.Collections混淆 List JDK Lists Set JDK Sets SortedSet JDK Sets Map JDK Maps SortedMap JDK Maps Queue JDK Queues Multiset Guava Multisets Multimap Guava Multimaps BiMap Guava Maps Table Guava Tables 二.Conllections与Collection Conllection是一个java.util包下的集合接口–(java.util.Collection) Collections是个java.util下的类，它包含有各种有关集合操作的静态方法。此类不需要实例化，就像一个工具类。如Collections的排序方法sort –Collections.sort(list); 三.Arrays与Array Arrays:java.util下的类-工具类,是拥有静态方法操作数组的工具类,不需要实例化:直接:123int[] inits = new int[10] ; //将数组全部赋值为2 Arrays.fill(inits , 2) ; 此静态类专门用来操作Array对象 ，提供搜索、排序、复制等静态方法。123equals()：比较两个array是否相等。array拥有相同元素个数，且所有对应元素两两相等。sort()：用来对array进行排序。binarySearch()：在排好序的array中寻找元素 四.Map与Maps Map是一个k-v结构的接口 Maps:则为com.google.common.collect下的工具类,主要是对Map集合进行数据操作,具体实现不作过多介绍.","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"集合","slug":"java/集合","permalink":"http://yoursite.com/categories/java/集合/"}],"tags":[{"name":"集合工具类","slug":"集合工具类","permalink":"http://yoursite.com/tags/集合工具类/"},{"name":"Guava","slug":"Guava","permalink":"http://yoursite.com/tags/Guava/"}]},{"title":"集合系列之接口关系(二)","slug":"集合系列之接口关系(二)","date":"2018-10-21T15:41:20.000Z","updated":"2018-11-13T03:02:46.000Z","comments":true,"path":"2018/10/21/集合系列之接口关系(二)/","link":"","permalink":"http://yoursite.com/2018/10/21/集合系列之接口关系(二)/","excerpt":"本章主要是讲集合接口的各种关系，其中Map接口与Collection接口只是实现了Iterable的接口,从而调用iterator(),生成迭代器/iterator对象是其实现类.","text":"本章主要是讲集合接口的各种关系，其中Map接口与Collection接口只是实现了Iterable的接口,从而调用iterator(),生成迭代器/iterator对象是其实现类. 一.接口关系图 二.依照实现接口分类 一.接口关系图说明下: Map与Collection接口只是实现了Iterable的接口,从而调用iterator(),生成迭代器/iterator对象是其实现类.各个集合接口关系如下图: 二.依照实现接口分类12345678910实现Map接口的有：EnumMap、IdentityHashMap、HashMap、LinkedHashMap、WeakHashMap、TreeMap实现List接口的有：ArrayList、LinkedList实现Set接口的有：HashSet、LinkedHashSet、TreeSet实现Queue接口的有：PriorityQueue、LinkedList、ArrayQueue依据底层实现的数据结构分类：底层以数组的形式实现：EnumMap、ArrayList、ArrayQueue底层以链表的形式实现：LinkedHashSet、LinkedList、LinkedHashMap底层以hash table的形式实现：HashMap、HashSet、LinkedHashMap、LinkedHashSet、WeakHashMap、IdentityHashMap底层以红黑树的形式实现：TreeMap、TreeSet底层以二叉堆的形式实现：PriorityQueue stack,HashTale退出历史舞台","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"集合","slug":"java/集合","permalink":"http://yoursite.com/categories/java/集合/"}],"tags":[{"name":"集合关系","slug":"集合关系","permalink":"http://yoursite.com/tags/集合关系/"},{"name":"分类","slug":"分类","permalink":"http://yoursite.com/tags/分类/"}]},{"title":"集合关系系列之基本接口(一)","slug":"集合系列之基本接口(一)","date":"2018-10-21T15:28:41.000Z","updated":"2018-11-13T03:00:32.000Z","comments":true,"path":"2018/10/21/集合系列之基本接口(一)/","link":"","permalink":"http://yoursite.com/2018/10/21/集合系列之基本接口(一)/","excerpt":"java集合有两种：Collection系列集合,Map系列集合,Iterator接口是用来遍历集合的,无论Map还是Collection集合都可以,因为他们都实现了Iterable接口,因此可以调用iterator()方法,生成iterator对象/迭代器，从而进行遍历．","text":"java集合有两种：Collection系列集合,Map系列集合,Iterator接口是用来遍历集合的,无论Map还是Collection集合都可以,因为他们都实现了Iterable接口,因此可以调用iterator()方法,生成iterator对象/迭代器，从而进行遍历． java中集合有两个基本的接口:Collection接口与Map接口,其中Collection是单列集合,Map是双列集合(K-V,一个变量有两个部分). Iterator接口/迭代器是用来遍历集合的(以Collection接口为例),然后Collection的实现类对象采用iterator()方法生成该迭代器.其中iterator()是属于java.lang.Iterable顶级接口的唯一方法,是Iterable接口专门用来生成迭代器的.同时Collection的实现类对象实现了Iterable接口,所以Collection的实现类对象可以使用iterator()方法得到一个迭代器.具体方法如下123456789101112131415161718192021222324252627public class SDF&#123; public static void main(String[] args) &#123; //Collection的实现类对象 Collection c = new HashSet&lt;&gt;(); c.add(\"ele1\"); //调用Iterable接口的iterator()方法生成迭代器 Iterator it1 = c.iterator(); System.out.println(it1.hasNext()); while (it1.hasNext()) &#123; System.out.println(\"---&gt;\" + it1.next()); &#125; //第二次遍历同一个Iterator对象,结果为空, //因为迭代器会保留当前位置信息,所以结果为空. //这也就是为什么不让对象直接使用Iterator接口里面的方法 //而是需要使用iterator()方法为不同的对象分配不同的全新迭代器, //因为这样各自对象使用的迭代器才不会相互影响, //可以记录各自迭代器的当前位置. System.out.println(it.hasNext()); while (it.hasNext()) &#123; System.out.println(\"---&gt;\" + it.next()); &#125; &#125;&#125; 输出结果为:123true---&gt;ele1false 总结: (1)Collection(为了存储)–&gt;iterator(遍历Collection对象) ,Map(存储之后的查找) ; (2)Iterable接口专门实现Iterator接口/迭代器的对象,这样作是为了减少各自对象使用的迭代器的相互影响.参考文献: ❋Java集合Collection与Iterator ❋Java中Iterable和Iterator的辨析","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"集合","slug":"java/集合","permalink":"http://yoursite.com/categories/java/集合/"}],"tags":[{"name":"Collection","slug":"Collection","permalink":"http://yoursite.com/tags/Collection/"},{"name":"Map","slug":"Map","permalink":"http://yoursite.com/tags/Map/"}]},{"title":"数据类型系列之数据操作(二)","slug":"java数据类型系列之数据操作(二)","date":"2018-10-21T15:17:17.000Z","updated":"2018-11-13T02:31:30.000Z","comments":true,"path":"2018/10/21/java数据类型系列之数据操作(二)/","link":"","permalink":"http://yoursite.com/2018/10/21/java数据类型系列之数据操作(二)/","excerpt":"本文主要是介绍了基于数据的一些操作，主要有初始化，类型获取，长度测量等，其中对象数组，由于已经有人总结得很好，我就不必重复造轮子了．","text":"本文主要是介绍了基于数据的一些操作，主要有初始化，类型获取，长度测量等，其中对象数组，由于已经有人总结得很好，我就不必重复造轮子了． 一.数组全部置为0 二.java对象数组的使用 三.对象类型初始化 四.类型获取 五.数据的长度测量 5.1 length 5.2 length() 5.3 size() 六.double[]与Double[] 七.其他 一.数组全部置为01234567double[] vector = new double[voc.size()]; /** * 向量初始化--对应特征词汇表,防止其他方式出现异常 */ for (int i = 0; i &lt;vector.length ; i++) &#123; vector[i]=0; &#125; 二.java对象数组的使用 对象数组与基本类型数组在运用上几乎一模一样，唯一差别在于，前者数组中的元素是引用地址，后者的则为直接数据；例如：12Person [] staff=new Person[100];int [] num=new int[10]; 具体参考:java 对象数组的使用 三.对象类型初始化 对象类型默认初始化为null，必须要手动初始化。如字符串是属于对象类型,默认值为null,但我们需要进行初始化赋值,代码如下1String str=\"\"; 四.类型获取getClass()与getSimpleName() new testClass().getClass()–对应new testClass().getClass()–类的全称 new testClass().getClass().getSimpleName()–对应:testClass-类的简称 五.数据的长度测量 其实主要就是Java中length、length()、size()的区别/使用范围. 5.1 lengthlength–针对数组,数组当中的一个属性;1double[] arr=new double[sentence.length]; 5.2 length()length()–针对字符串的1int lh=\"ssde\".length(); 5.3 size() size()–针对集合数据类型的–List,Map,set,Queue size()方法最后要找的其实还是数组的length属性；因为集合数据类型的底层还是数组; 六.double[]与Double[] 包装类:便于对数据的操作;类型的转换(Double与double,int与Integer可以自动转换) 但double[]与Double[]尚且不能做到自动转换,否则会报错,下面例子:123//Double[]这个类型会报错,因为Vectors.dense的方法类型是double[]Double[] vector = sen2Arr.sentenceToArray(str);LabeledPoint train_one = new LabeledPoint(0.0, Vectors.dense(vector)); 七.其他 (1)小可自动转大，大转小会失去精度。 (2)字符串很多地方都要用到，但是由于string类生成的字符串不可修改，因此用stringbuffer或者stringbuild类进行字符串的修改。","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"数据类型","slug":"java/数据类型","permalink":"http://yoursite.com/categories/java/数据类型/"}],"tags":[{"name":"对象数组","slug":"对象数组","permalink":"http://yoursite.com/tags/对象数组/"},{"name":"数据长度","slug":"数据长度","permalink":"http://yoursite.com/tags/数据长度/"}]},{"title":"数据类型系列之综述(一)","slug":"java数据类型系列之综述(一)","date":"2018-10-21T13:19:20.000Z","updated":"2018-11-13T02:32:12.000Z","comments":true,"path":"2018/10/21/java数据类型系列之综述(一)/","link":"","permalink":"http://yoursite.com/2018/10/21/java数据类型系列之综述(一)/","excerpt":"本文主要是对java的数据类型进行总的阐述,其分为:基本数据类型与引用/对象数据类型,集合则是引用/对象数据类型的容器,只能存储对象类型的数据,不能存储基本数据类型的数据,但可以将基本数据类型的数据通过包装类的方法转换成对象类型,之后进行存储,泛化","text":"本文主要是对java的数据类型进行总的阐述,其分为:基本数据类型与引用/对象数据类型,集合则是引用/对象数据类型的容器,只能存储对象类型的数据,不能存储基本数据类型的数据,但可以将基本数据类型的数据通过包装类的方法转换成对象类型,之后进行存储,泛化 一.数据类型分类及其默认值 二.区别 三.栈内存与堆内存区别 四.java集合 一.数据类型分类及其默认值分为:基本数据类型与引用/对象数据类型说明: 类:具有某些共同特征的实体的集合,是抽象概念;默认值是null; 接口:抽象方法的接口,也是抽象概念,实现类需要实现其全部抽象方法.默认值是null; 数组:相同数据类型的集合,默认值是null; double型比float型存储范围更大，精度更高，所以通常的浮点型的数据在不声明的情况下都是double型的。 二.区别 基本数据类型 引用/对象数据类型 内存方面 栈中存数据 栈上存引地址,堆中存地址对应的数据 指向方面 变量名指向具体的数值 变量名指向数据的引用地址 对象直接的判断 使用== 对象1.equals(对象2),==是判断地址的 赋值 直接赋值 new方法;String,包装类可以直接赋值; 表达和操作能力 简单的字符或数字 表达且操作任何复杂的数据结构 三.栈内存与堆内存区别 堆和栈是虚拟出来的概念 他们在内存中都只是一块区域,只是他们的存取规则被人为的限制了,一般来说,把类似指针的地址数据放在同一块内存区域这块区域遵守栈的存取规则我们就称之为栈 然后把地址对应的数据放在同一块内存区域并遵守堆的存取规则,我们称之为堆,其实他们物理上都是一样的东西 只是我们附加上了一些概念 最后之所以这么做 是为了优化存取速度.1Object obj=new Objec() (1)栈内存: ❋存放地址数据(对象名可以认为是地址数据)/值;–便于查找 ❋因为基本数据类型没有什么引用数据,那么直接存值即可.(2)堆内存: 地址对应的数据,如:存放new出来的对象(包括属性,方法之类的)–成本低便于存储. 四.java集合 数据类型可以分为:引用/对象数据类型 ,基本数据类型 java集合则是只能存放引用数据类型／对象类型,是引用数据类型的容器,即存放的是对象的引用，不能存放基本数据类型. java集合存放的使得对象的引用.**每个集合元素都是一个引用变量，放在栈内存中,但实际对应的数据内容都放在堆内存或者方法区里面. 为了解决基本数据类型不能存放的问题,可以通过包装类((Integer,String)之类)把基本类型转为对象数据类型**.加之有自动拆箱与封装功能,基本数据类型的对象与包装类的对象转换十分方便.想把基本数据类型存入集合中，直接存就可以了，系统会自动将其装箱成封装类，然后加入到集合当中。 手动包装:12345int i = 10; Integer in = new Integer(i);//手动将i包装为Integer类型对象 HashSet set = new HashSet();//定义一个Set集合 set.add(in);//将包装类对象加入该集合 System.out.println(set);//打印结果 自动包装：1234int i = 10; HashSet set = new HashSet(); set.add(i);//系统会自动将i装箱为Integer类型的对象然后再存入set集合中 System.out.println(set);","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"数据类型","slug":"java/数据类型","permalink":"http://yoursite.com/categories/java/数据类型/"}],"tags":[{"name":"对象类型","slug":"对象类型","permalink":"http://yoursite.com/tags/对象类型/"},{"name":"栈内存","slug":"栈内存","permalink":"http://yoursite.com/tags/栈内存/"}]},{"title":"依赖注入系列之自定义注解(三)","slug":"依赖注入系列之自定义注解(三)","date":"2018-10-21T13:04:49.000Z","updated":"2018-11-13T03:12:42.000Z","comments":true,"path":"2018/10/21/依赖注入系列之自定义注解(三)/","link":"","permalink":"http://yoursite.com/2018/10/21/依赖注入系列之自定义注解(三)/","excerpt":"作为注解系列第三篇(之前是bean,现在是annotation(注解名))，重点讲解了自定义注解的组成结构与应用，自定义注解其实本身也是个类，属于注解类的范畴，之后，重点介绍了注解的应用，可以作为数据库sql语句的载体，之后使用反射机制实现对查询语句的获取，降低开发者的开发成本．","text":"作为注解系列第三篇(之前是bean,现在是annotation(注解名))，重点讲解了自定义注解的组成结构与应用，自定义注解其实本身也是个类，属于注解类的范畴，之后，重点介绍了注解的应用，可以作为数据库sql语句的载体，之后使用反射机制实现对查询语句的获取，降低开发者的开发成本． 一.声明 二.作用 三.结构 四.@Target 五.@Retention 六.@Documented 七.@Inherited 八.场景应用 8.1 预前知识 8.2 自定义注解实现数据插入 九其他 9.1 占位符赋值 9.2 value()方法 9.3 源码级注解与运行级注解 一.声明 (1)自定义注解可以看成是一个类; (2)“元注解”只能用来注解 “普通注解”,“普通注解”只能用来注解“代码”,自定义注解是“普通注解”,元注解包括:@Retention,@Target,@Inherited,@Document. 二.作用 主要是通过反射机制获取注解的属性值加以应用． 注解定义阶段设置好结构信息(有哪些属性),之后在注解阶段:进行属性值的设置,之后通过解析框架来解析注解,获取对应的属性值加以应用,例如sql语句的插入(这也就是为什么Neo4j仅仅通过@Query就可以实现对数据的查询)–好好屡屡 三.结构 自定义注解public @interface xxx 其中@interface不是interface接口，是一个注解类，是jdk1.5之后加入的，java没有给它新的关键字，所以就用@interface 这么个东西表示了，下面以建立一个自定义注解--@DoSomething来进行说明:12345678@Document@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.TYPE&#125;)@Inheritedpublic @interface DoSomething &#123; public String value(); public String name() default \"write\";&#125; 这个注解类包括： 四.@Target这个注解用于什么地方(类、方法、property，还是方法入参)，具体有如下情况：12345678@Target(ElementType.TYPE) //接口、类、枚举、注解@Target(ElementType.FIELD) //字段、枚举的常量@Target(ElementType.METHOD) //方法@Target(ElementType.PARAMETER) //方法参数@Target(ElementType.CONSTRUCTOR) //构造函数@Target(ElementType.LOCAL_VARIABLE)//局部变量@Target(ElementType.ANNOTATION_TYPE)//注解@Target(ElementType.PACKAGE) ///包 五.@RetentionRetention:保留(1)作用: 被它所注解的注解保留多,由编译器决定(2)属性: 主要是通过属性value的值来设定，是RetentionPolicy枚举类型的,值有:SOURCE,CLASS,RUNTIME三个.都是用来明确注解的生命周期长度,长短顺序为: SOURCE &lt; CLASS &lt; RUNTIME ，所以前者能作用的地方后者一定也能作用. 注解级别 存在范围 主要用途 SOURCE 源码级别 注解只保留在源文件，当Java文件编译成class文件的时候，注解被遗弃 如果只是做一些检查性的操作，比如 @Override 和 @SuppressWarnings，则可选用 SOURCE 注解 CLASS 字节码级别 注解被保留到class文件，但jvm加载class文件时候被遗弃，这是默认的生命周期 如果要在编译时进行一些预处理操作，比如生成一些辅助代码（如 ButterKnife）,生成额外的文件，如XML，Java文件等就用 CLASS注解； RUNTIME 运行时级别 注解不仅被保存到class文件中，jvm加载class文件之后，仍然存在 如果需要在运行时去动态获取注解信息，就是在运行时反射获取相关信息,那只能用 RUNTIME 注解 六.@Documented 被@Documented注解的自定义注解(以@DoSomething说明),会将该注解及其该注解的类连同打印到javaDoc文档当中.12345678910111213//最好导入注解包,要不然注解不添加到文档中import java.lang.annotation.Documented ;@Documented@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.TYPE&#125;)@Inheritedpublic @interface DoSomething &#123; public String value(); public String name() default \"write\";&#125;@DoSomething(\"walidake\")public class UseAnnotation &#123;&#125; 在当前文件所属的文件夹下进行文档生成命令:1javadoc -encoding \"GBK\" -d doc UseAnnotation.java 有时需要对文件以特定编码读取-encoding &quot;GBK&quot;,设置文档的存放目录(directory)-d doc:说明文档是在当前文件夹建立一个新文件夹doc进行存储.操作过程如下图:说明: 如果出现javadoc不是内部命令,说明在环境变量path中没有配置java命令,才导致命令运行不成功. 七.@Inherited 允许子类继承父类中的注解：被自定义注解类注解的类,它的子类可以继承该自定义注解(包括特性及其属性值)． 八.场景应用8.1 预前知识 应用讲解之前,主要说明下,对注解的一些操作方法:| 方法名 | 用法 || ———————————————- | ———————————————————— || Annotation getAnnotation(Class annotationType) | 获取注解在其上的annotationType || Annotation[] getAnnotations() | 获取所有注解 || isAnnotationPresent(Class annotationType) | 判断当前元素是否被annotationType注解 || Annotation[] getDeclareAnnotations() | 与getAnnotations() 类似，但是不包括父类中被Inherited修饰的注解 | 8.2 自定义注解实现数据插入Step 1 :声明自定义注解。12345@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface Insert &#123; public String value();&#125; Step 2 : 在规定的注解使用范围内使用我们的注解123456public interface UserMapper &#123; //联想Neo4j的@Query注解,异曲同工 @Insert(\"insert into user (name,password) values (?,?)\") public void addUser(String name,String password);&#125; Step 3 : 通过method.getAnnotation(Insert.class).value()使用反射解析自定义注解，得到其中的sql语句1234567891011//检查是否被@Insert注解修饰if (method.isAnnotationPresent(Insert.class)) &#123; //检查sql语句是否合法 //method.getAnnotation(Insert.class).value()取得@Insert注解value中的Sql语句 //check(sql语句,sql语句的类型--insert注解对应insert操作) sql = checkSql(method.getAnnotation(Insert.class).value(), Insert.class.getSimpleName()); //具体的插入数据库操作 //自定义方法insert(sql语句,为占位符赋值)实现插入 insert(sql, parameters);&#125; Step 4 : 根据实际场景调用Step 3的方法12UserMapper mapper = MethodProxyFactory.getBean(UserMapper.class);mapper.addUser(\"walidake\",\"665908\"); 运行结果： 本节主要以自定义注解完成插入操作来进行说明,最主要的其实就是根据自定义注解的类型及其名称使用反射机制获取自定义注解的值.完成对应的插入操作. 九其他9.1 占位符赋值上面insert的插入语句涉及到给占位符赋值的的操作,用PreparedStatement这类完成PreparedStatement.setObject:给占位符赋值的操作123PreparedStatement pstm = connection.prepareStatement(\"insert into t_user values (?,?)\");pstm.setString(1,\"yang\");pstm.setString(2,\"12456\"); 其中setObject中的Object与t_user表中列的字段类型一致的，不如第1列在表中的数据类型是“varchar2”的，那就为setString 9.2 value()方法(1)当注解有value()方法时，在进行注解的时候不需要指明具体名称。如:1234567891011@Documented@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.TYPE&#125;)@Inheritedpublic @interface DoSomething &#123; public String value(); public String name() default \"write\";&#125;@DoSomething(\"walidake\")public class UseAnnotation &#123;&#125; 上述代码中@DoSomething(&quot;walidake&quot;)其实就是等价于@DoSomething(value=&quot;walidake&quot;)。 9.3 源码级注解与运行级注解(1)源码级注解 注解只存在源码中,在源码编写时期对注解进行解析(例如@Override之类的,在代码编写阶段,就通过对该注解进行解析,然后对代码进行分析,检测),并不会过多影响到运行效率(因为已经将注解解析好了).—开发成本高–Android等对效率性能要求较高的平台.(2)运行级别注解 运行时才对代码进行解析,(虽然注解从源码阶段一直保留到了jvm阶段)注解存在源码(java文件)，字节码(class文件)与Java虚拟机（JVM）中,在程序运行时对注解进行反射获取相关信息,影响程序运行速度.–但开发成本低,web项目较多. Spring框架的注解方式是运行时注解，效率不高。为啥不使用源码级别框架,对开发成本的节省，开发效率的提高，虽然运行效率稍微低一些，但开发效率大大提高参考文献： ❋用自定义注解做点什么 ❋自定义注解之运行时注解","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"注解","slug":"java/注解","permalink":"http://yoursite.com/categories/java/注解/"}],"tags":[{"name":"Documented","slug":"Documented","permalink":"http://yoursite.com/tags/Documented/"},{"name":"Inherited","slug":"Inherited","permalink":"http://yoursite.com/tags/Inherited/"}]},{"title":"泛型系列之通配符与关键字(二)","slug":"泛型系列之通配符与关键字(二)","date":"2018-10-21T12:52:39.000Z","updated":"2018-11-13T02:58:54.000Z","comments":true,"path":"2018/10/21/泛型系列之通配符与关键字(二)/","link":"","permalink":"http://yoursite.com/2018/10/21/泛型系列之通配符与关键字(二)/","excerpt":"本文是泛型系列的第二篇,重点是对泛型的关键字与通配符进行介绍,泛型T或者?的类型必须要对象类型,如:List,基本数据类型不能作为泛型的类型,如”List“是错误的,但可以使用他们对应的包装类型(是对象类型的一种),如:List,这个才是正确类型.","text":"本文是泛型系列的第二篇,重点是对泛型的关键字与通配符进行介绍,泛型T或者?的类型必须要对象类型,如:List,基本数据类型不能作为泛型的类型,如”List“是错误的,但可以使用他们对应的包装类型(是对象类型的一种),如:List,这个才是正确类型. 一.主要类型 二.extends与super 三.?与T 四.&lt;T extends Comparable&lt;? super T&gt;&gt; 五.参考文献 一.主要类型说明下： (1)?代表类型不固定,不唯一,符合即可; (2)T则是暂时不固定,当确定类型(也就是T的具体类型值)之后,这个T的类型就唯一了,固定了. (3)泛型T或者?必须要为对象类型123456List&lt;String&gt; —- 参数化的类型 List&lt;E&gt; —- 泛型(记住) ---List集合中的各个元素存放的是T类型的数据List&lt;?&gt; —- 无限制通配符类型(记住) &lt;E extends SomeClass&gt; —- 有限制类型参数 List &lt;? extends SomeClass&gt;—- 有限制通配符类型 &lt;T extends Comparable&lt;T&gt;&gt; —– 递归类型限制 二.extends与super以&lt;? extends Fruit&gt;与&lt;? super Fruit&gt;为例 ❋extends:扩展到(上界),求的是子类–取东西较多 ❋super:继承于(下界),?求的是父类–存东西较多参考文献: 三.?与T以&lt;? extends Fruit&gt;与&lt;T extends Fruit&gt;为例 ❋本质上是类型固不固定的问题; ❋T是一种类名,代表某种类型,当确定下来之后(例如设置T为Apple类型),之后的操作都是对Apple类型数据的操作. ❋?并不是一个类名，它只是一个通配符,只要?是Fruit类型及其子类,都可以进行数据的操作 四.&lt;T extends Comparable&lt;? super T&gt;&gt; ❋Comparable&lt;? super T&gt;:把它看成一个实现了Comparable接口的类型,类型下限是T,就是求T的父类. ❋extends表明:泛型T(类型)的上限类型,上限类型是实现Comparable的类型,这个类型又是T类型的父类;例如下面这个:&lt;Apple extend Comparable&lt;Fruit&gt;&gt;就成立,T是Apple,?则是Fruit,是Apple的父类. 五.参考文献 ❋&lt;T extends Comparable&lt;? super T&gt;&gt; ❋&lt;? extends T&gt;和&lt;? super T&gt;","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"泛型","slug":"java/泛型","permalink":"http://yoursite.com/categories/java/泛型/"}],"tags":[{"name":"关键字","slug":"关键字","permalink":"http://yoursite.com/tags/关键字/"},{"name":"通配符","slug":"通配符","permalink":"http://yoursite.com/tags/通配符/"}]},{"title":"CrudRepository 接口","slug":"CrudRepository 接口说明","date":"2018-10-21T12:47:04.000Z","updated":"2018-11-13T02:18:44.000Z","comments":true,"path":"2018/10/21/CrudRepository 接口说明/","link":"","permalink":"http://yoursite.com/2018/10/21/CrudRepository 接口说明/","excerpt":"CrudRepository 接口是用来操作Neo4j的重要接口,因此对其的基本方法需要有一个基本的认识,为此,特地从网上摘取相关的说明文档,以便日后回忆,同时谢谢网友的热心分享.","text":"CrudRepository 接口是用来操作Neo4j的重要接口,因此对其的基本方法需要有一个基本的认识,为此,特地从网上摘取相关的说明文档,以便日后回忆,同时谢谢网友的热心分享. 本文转自:SpringData中的CrudRepository接口,谢谢分享. CrudRepository 接口提供了最基本的对实体类的添删改查操作12345678910(1).T save(T entity);//保存单个实体 (2).Iterable&lt;T&gt; save(Iterable&lt;? extends T&gt; entities);//保存集合 (3).T findOne(ID id);//根据id查找实体 (4).boolean exists(ID id);//根据id判断实体是否存在 (5).Iterable&lt;T&gt; findAll();//查询所有实体,不用或慎用! (6).long count();//查询实体数量 (7).void delete(ID id);//根据Id删除实体 (8).void delete(T entity);//删除一个实体 (9).void delete(Iterable&lt;? extends T&gt; entities);//删除一个实体的集合 (10).void deleteAll();//删除所有实体,不用或慎用! 上述方法,都是系统定义好了,如无扩展需求,则不需要进行覆写.","categories":[{"name":"Neo4j","slug":"Neo4j","permalink":"http://yoursite.com/categories/Neo4j/"},{"name":"Spring-boot","slug":"Neo4j/Spring-boot","permalink":"http://yoursite.com/categories/Neo4j/Spring-boot/"}],"tags":[{"name":"CrudRepository","slug":"CrudRepository","permalink":"http://yoursite.com/tags/CrudRepository/"}]},{"title":"IDEA使用(二)","slug":"IDEA使用(二)","date":"2018-10-21T12:36:15.000Z","updated":"2018-11-13T02:26:16.000Z","comments":true,"path":"2018/10/21/IDEA使用(二)/","link":"","permalink":"http://yoursite.com/2018/10/21/IDEA使用(二)/","excerpt":"本文的IDEA使用经验是基于现有项目开发而来的,主要是对现有操作方式的进行速度提升,提升用户体验,可能文中有些叙述不对,欢迎大家指正.","text":"本文的IDEA使用经验是基于现有项目开发而来的,主要是对现有操作方式的进行速度提升,提升用户体验,可能文中有些叙述不对,欢迎大家指正. 一.IDEA 技巧 二.IDEA文件的删除 三.GroupId和ArtifactId怎么填写 四.其他 一.IDEA 技巧 cosole中查找错误定位:ctrl+F输入exception 打开快捷方式:alt+insert== 页面切换:alt+ -&gt;/&lt;- 批量修改变量名:ctrl+shift+alt+j–同时也可以注释 关闭页面: ctrl+F4,shift+点击页面栏 最近文件:ctrl+E–recent 大小写转化:ctrl+shift+U 类文件结构页面: ctrl+H 查找类:ctrl+N 展示当前类的所有方法:ctrl+F12 取消注释Ctrl-Shift-/: 显示方法简洁信息:ctrl+Q 查看方法的参数配置(光标在括号里面):ctrl+P ; 创建成员变量–选中变量–&gt;alt+insert–local variable之类的 换行: (1)自动添加分号并跳到下一行:ctrl+shift+enter (2)向下一行:shift+enter;当前行的任意位置都可以 (3)向上一行:ctrl+alt+enter:当前的行的任意位置都可以 某行复制:将鼠标定位到那行,然后直接ctrl+c,复制成功; 跳出括号 (1)使用tab键跳出括号: (2)跳出括号: shift+);跳出双引号:shift+” 撤销上一步:ctrl+z 恢复撤销上一步:ctrl+shift+z–类似word中的ctrl+y 快速生成实体类:插件GsonFormat快速生成JSon实体类 快速构造方法,alt+Insert IDEA的git项目克隆: (1)vcs-&gt;checkout from version control–&gt;git ,粘贴项目地址即可. (2)记住是https地址不是,SSH地址 package.json:对包的说明,采用json形式二.IDEA文件的删除 IDEA删除文件时出现如下图: (1)Safe delete ( with usage search):相当于代码重构了，编辑器会查找项目代码里跟这个文件相关的代码，有的话你就斟酌更改，以防你因为删除了这个文件而导致程序运行出错. (2)Search in comments and string” 选项, 如果选中 , 会同时搜寻注解中是否有相关文件. 如果有,一样会提醒 ; 如果没有相关文件,会完成删除. (3)建议使用Safe delete,避免误删除或删除后留下冗余代码 (4)如果有相关文件会提醒有几处被使用,并且列出以下三个按钮:如下图: ❋ Delete Anyway 确认删除 ❋ View usages 查看哪里被使用 ❋ Cancel 取消删除 三.GroupId和ArtifactId怎么填写 (1)groupid和artifactId被统称为“坐标”是为了保证项目唯一性而提出的,就是一个项目坐标 (2)groupid:域–org,com,cn;如公司名称–apache,zxp (3)artifactId:项目名–testQA (4)cn.zxp.项目名 四.其他 (1)一般源码当中的的参数是大写的,说明是类型–&gt;例如T","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/工具/"},{"name":"IDEA","slug":"工具/IDEA","permalink":"http://yoursite.com/categories/工具/IDEA/"}],"tags":[{"name":"快捷键","slug":"快捷键","permalink":"http://yoursite.com/tags/快捷键/"},{"name":"删除","slug":"删除","permalink":"http://yoursite.com/tags/删除/"}]},{"title":"乱码系列之编码异常(二)","slug":"乱码系列之编码异常(二)","date":"2018-10-21T12:23:51.000Z","updated":"2018-11-13T03:05:20.000Z","comments":true,"path":"2018/10/21/乱码系列之编码异常(二)/","link":"","permalink":"http://yoursite.com/2018/10/21/乱码系列之编码异常(二)/","excerpt":"本文是乱码系列的第二篇,在该篇中我们会对Integer.parseInt()这个让我头疼了很久的异常进行重点解析,还有在生成javaDoc说明文档的时候,出现的一些乱码异常.","text":"本文是乱码系列的第二篇,在该篇中我们会对Integer.parseInt()这个让我头疼了很久的异常进行重点解析,还有在生成javaDoc说明文档的时候,出现的一些乱码异常. 一.Integer.parseInt()异常; 1.1 原因: 1.2 解决方法: 二.出现非法字符：“\\ufeff” 一.Integer.parseInt()异常;1.1 原因: (1)数字字符串有空格–对对应数组元素采取trim方式; (2)数字没有空格依然报错,那就很有可能是编码错误:可能UTF-8编码在此处不合适,可能Integer.parseInt()是根据系统的原先默认编码(ANSI编码)来读取,同时在简体中文系统中,ansi编码的默认字符编码是GB2312,在日文操作系统下，ANSI 编码代表 JIS 编码, 1.2 解决方法: 修改文件编码:改为字节流读取.设置字符编码. 1.将utf-8编码的文件改为ansi编码文件-可以另存为ansi编码,进行覆盖, 2.并修改编码变换的乱码,乱码如下: 3.不使用FileReader的方式读取文件,采用InputStreamReader方式,(字节流的方式),指定编码,代码如下:123File file = new File(rootPath + s); InputStreamReader isr = new InputStreamReader(new FileInputStream(file), \"GB2312\"); BufferedReader br = new BufferedReader(isr); 这步没有做的话,会导致中文乱码 4.重新运行即可得到结果: 二.出现非法字符：“\\ufeff” 说明该文件是utf-8带Bom格式的,因此需要转换成不带Bom格式的;方法如下:1将文件编码更改为UTF-16，再改回UTF-8即可 或者改为其他编码如GBK,在读取文件的时候,指定编码格式,如下面:1javadoc -encoding \"GBK\" -d doc UseAnnotation.java 即可.","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"乱码","slug":"java/乱码","permalink":"http://yoursite.com/categories/java/乱码/"}],"tags":[{"name":"parseInt","slug":"parseInt","permalink":"http://yoursite.com/tags/parseInt/"},{"name":"非法字符","slug":"非法字符","permalink":"http://yoursite.com/tags/非法字符/"}]},{"title":"乱码系列之编码知识(一)","slug":"乱码系列之编码知识(一)","date":"2018-10-21T12:23:46.000Z","updated":"2018-11-13T03:06:30.000Z","comments":true,"path":"2018/10/21/乱码系列之编码知识(一)/","link":"","permalink":"http://yoursite.com/2018/10/21/乱码系列之编码知识(一)/","excerpt":"项目开发当中，乱码是常有的事情，特别是在进行文件读取的时候经常出现乱码，因此，解决好乱码问题之前，先看下有关编码的知识．","text":"项目开发当中，乱码是常有的事情，特别是在进行文件读取的时候经常出现乱码，因此，解决好乱码问题之前，先看下有关编码的知识． 一.ASCII编码 二.Unicode标准 三.UTF-8 四.UTF-16 五.BOM 六.参考文献 一.ASCII编码 ASCII编码:主要是英文字符;0-127:常见字符集,128-255:之后增加的字符–扩展字符集;ANSI编码集:主要是对ASCII码的扩展,其中0-127:常见字符集与ASCII码相同;之后根据不同国家对其的修改不同而不同,因此,ANSI编码是众多国家对ASCII码的扩展字符编码集合,是一个总称;中国的GB2312字符编码就是属于ANSI编码集. 二.Unicode标准 Unicode标准:就是为了解决:由于各国对ASCII码的无序修改导致,不同国家之间的字符编码转换困难,推出的一套集成全部国家字符的编码标准,因为是重新建立一套的标准,除了0-127:常见字符集是一样,其余与其他国家的字符编码是完全不一样,没有继承,兼容,导致各国热情不高; 三.UTF-8 UTF-8:Unicode为了兼容各国字符,导致每个字符所占的字节数都为2个,但是有些字符就不需要这么多字节,0-127:英文常见字符集只需要1个字节,其实由于硬盘容量较大,对硬盘方面影响较少,但是对于数据传输,影响就很明显,因此,就开发了一个基于可变字节长(根据是否是常见英文字符设置不同的字节)的字符编码–utf-8–一开始专门用来进行数据传输.utf:unicode translation format–unicode标准的转换格式或者理解为数据传输格式,8代表,英文常见字符集的时候,只用一个字节(8位)保存,其余则用两个字节保存. 说明:Unicode则是编码的标准,utf-8,utf-16,utf-32是一种实现Unicode编码标准的存储格式,与GB2312之类的字符编码一样. 四.UTF-16 UTF-16:都用两个字节保存,因此对所有字符的支持力度最好(utf-32优点浪费内存了),所以作为Unicode标准的默认编码,因此在内存中使用,用来更好地支持字符显示.与此同时,utf-8字符编码则是在数据传输,硬盘存储(省内存)用的比较多. 五.BOM BOM:BOM——Byte Order Mark，就是字节序标记,使用BOM来标记文本文件的编码方式,utf-8带bom的文件:当系统读取该文件的bom,就知道以utf-8字符编码格式读取;但是带BOM的utf-8编码的文件是以:\\ufeff开头,就会导致很多编码问题的存在,因此在开发的过程之中,最好是用不带Bom的utf8 六.参考文献 ❋编码格式简介","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"编码","slug":"java/编码","permalink":"http://yoursite.com/categories/java/编码/"}],"tags":[{"name":"ANSI","slug":"ANSI","permalink":"http://yoursite.com/tags/ANSI/"},{"name":"编码","slug":"编码","permalink":"http://yoursite.com/tags/编码/"}]},{"title":"异常及其解决方法(一)","slug":"异常及其解决方法","date":"2018-10-21T12:00:28.000Z","updated":"2018-11-13T03:13:30.000Z","comments":true,"path":"2018/10/21/异常及其解决方法/","link":"","permalink":"http://yoursite.com/2018/10/21/异常及其解决方法/","excerpt":"本文主要是记录本人在项目开发过程之中遇到的一些异常的解决方法,很多都是很不小心的造成的,看样子自己的代码能力需要很大的提升.","text":"本文主要是记录本人在项目开发过程之中遇到的一些异常的解决方法,很多都是很不小心的造成的,看样子自己的代码能力需要很大的提升. 一.返回值与返回类型不一致 二.文件地址重复 三.测试类最好以匿名对象调用方法 四.List,Map最好初始化 五.结果个数不符 六.IOException 七.spark的SparkContext最后需要关闭 八.返回问题 九.抽象类/接口不能直接实例化 十.没有添加对应的bean 十一.没有建立无参构造函数 一.返回值与返回类型不一致 Cannot return a value from a method with void result type–&gt;返回值与返回类型不一致 二.文件地址重复 String ScoreQuestion = LoadQuestionPattern.LoadFile(rootPath+&quot;【0】年龄&quot;);其方法中12345678public static String LoadFile(String s) throws IOException &#123; /** *先将文件生成javaBean对象,便于操作; */ //File file= new File(rootPath+\"s\");这种错误还在犯, //不应该,s本来已经是字符串,不要添加字符串 File file= new File(rootPath+s); &#125; 会造成地址重叠:1D:\\java\\HANLP\\1.6.8\\data\\question\\D:\\java\\HANLP\\1.6.8\\data\\question\\【0】年龄 三.测试类最好以匿名对象调用方法 在测试类文件文件中,最好不要直接定义,如下形式:1private sentence2Array arr; 否则会报错,需对sentence2Array类的对象arr进行实例化–new sentence2Array()实例化,或者用一个变量来接收实例化对象. 四.List,Map最好初始化 List,Map之类的数据类型,在本类中是全局变量 Map数据类型在用来接收新的数据的时候(写入时),最好初始化代码如下：1Map&lt;String, Integer&gt; vocabulary = new HashMap&lt;String, Integer&gt;(); 否则会出现put方法失效的结果,put结果为Null,导致NPE. 读取集合类型数据的的时候就最好设置为全局变量. 五.结果个数不符 当结果个数与预期不符的,那就需要检查循环语句,if/while区别开来.if只有一次 六.IOException输入输出异常:往往是字符流、文件读或者写时发生的异常,比如: (1)往硬盘某个文件写入某个数据,但是文件不存在–写入异常的一种情况; (2)从硬盘某个文件读取文件–某个文件不存在–读取异常的一种说明: Input流写入内存;output从内存(闪存/硬盘)输出到屏幕或者某个文件中 七.spark的SparkContext最后需要关闭12//记得关闭 sc.close(); 要不然就不会有返回值,会报空指针异常 八.返回问题1234567public String answer(String sentence) throws IOException &#123;if (answer == null) &#123; System.out.println(\"对不起,答案尚未找到 = \" + answer); &#125; else &#123; return answer; &#125; &#125; 还是会报错,那是因为System.out.println(&quot;对不起,答案尚未找到 = &quot; + answer);这句–这句的answer也是画蛇添足,多此一举,因为这句是输出,不是返回.因此改为:return &quot;对不起,答案尚未找到 &quot;即可;注意return回来的数据类型要符合返回值的类型. 九.抽象类/接口不能直接实例化接口为:123public interface answerQuestion &#123; public String answer(String sentence) throws IOException;&#125; 会提示:1'answerQuestion' is abstract; cannot be instantiated` 因此不能直接实例化,需要它的实现类帮助.new answerQuestionImpl()即可. 十.没有添加对应的bean12`Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sessionFactory' defined in class path resource` 上述代码出现了BeanCreationException说明没有添加对应的bean,(可能在某方面调用了)注意多观察:caused by后面的说明. 十一.没有建立无参构造函数1java.lang.NoSuchMethodException: com.example.qa.Entity.entity.Food.&lt;init&gt;() –说明缺少对Food类的无参构造()–因此在实体类中,有参,无参构造都要写出来","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"异常处理","slug":"java/异常处理","permalink":"http://yoursite.com/categories/java/异常处理/"}],"tags":[{"name":"实例化","slug":"实例化","permalink":"http://yoursite.com/tags/实例化/"},{"name":"返回问题","slug":"返回问题","permalink":"http://yoursite.com/tags/返回问题/"}]},{"title":"开发心得(一)","slug":"开发心得","date":"2018-10-21T11:51:55.000Z","updated":"2018-11-13T03:03:30.000Z","comments":true,"path":"2018/10/21/开发心得/","link":"","permalink":"http://yoursite.com/2018/10/21/开发心得/","excerpt":"本文主要是最近项目开发以来的一些经验和体会,感觉基础确实很不牢固,可能是代码敲得少的原因,写下这篇心得,让自己能够尽快查漏补缺,赶上去.","text":"本文主要是最近项目开发以来的一些经验和体会,感觉基础确实很不牢固,可能是代码敲得少的原因,写下这篇心得,让自己能够尽快查漏补缺,赶上去. IDEA中源代码的类,可以导入到测试包当中,要不然怎么叫测试呢? 测试方法的时候,不一定需要main方法,在对应的方法加上@Test即可 在进行的测试时候,如果出现问题,用最好用F7进行单步测试; null是小写的 java.util中的类大部分是静态类(Maps,Lists,Arrays之类) 写程序,当遇到自定义方法的时候,先定义,之后在进行设计(很重要,这个是很重要的思想) 定义变量的时候,先写变量名,之后alt+insert(节省时间) Integer之类的包装类型的初始值为null 以后写程序的时候,每写一个模块最好进行测试,要不然出了问题不知道该怎么排查; 当使用的全局变量不提示的时候,自己可以手动导入所在的包; 使用快捷键快速设置接口的实现类的时候,参数最好预先设置好,节省时间.以下是借鉴网友的开发心得,觉得挺不错:12345678910写代码的四点： 1.明确需求。要做什么？ 2.分析思路。要怎么做？（1,2,3……） 3.确定步骤。每一个思路要用到哪些语句、方法和对象。 4.代码实现。用具体的语言代码将思路实现出来。学习新技术的四点： 1.该技术是什么？ 2.该技术有什么特点？（使用需注意的方面） 3.该技术怎么使用？（写Demo） 4.该技术什么时候用？（在Project中的使用场景）","categories":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/categories/随笔/"},{"name":"开发心得","slug":"随笔/开发心得","permalink":"http://yoursite.com/categories/随笔/开发心得/"}],"tags":[{"name":"测试","slug":"测试","permalink":"http://yoursite.com/tags/测试/"},{"name":"设计思想","slug":"设计思想","permalink":"http://yoursite.com/tags/设计思想/"}]},{"title":"词性纠正或者识别错误","slug":"词性或者识别错误","date":"2018-10-21T11:44:41.000Z","updated":"2018-11-13T02:57:36.000Z","comments":true,"path":"2018/10/21/词性或者识别错误/","link":"","permalink":"http://yoursite.com/2018/10/21/词性或者识别错误/","excerpt":"词性纠正和识别错误是NLP处理当中是很平常的,一般主要的方法有两种:直接修改词所在词典的词性,还有一种就是使用优先级更高的词典进行覆盖,第二种带来的后遗症少,通常采用第二种.","text":"词性纠正和识别错误是NLP处理当中是很平常的,一般主要的方法有两种:直接修改词所在词典的词性,还有一种就是使用优先级更高的词典进行覆盖,第二种带来的后遗症少,通常采用第二种.操作步骤： (1)词性分类不正确的–看该词所在的词典,之后进行修改,或者直接修改核心词典CoreNatureDictionary.txt (2)不能识别的–增加自定义词典辅助识别,设置词性(在内置自定义词典中); (3)修改词典后,删除缓存(主要是为了加快读取速度的,不能实现实时更新缓存)出现如下情况: 注意事项： 词性最好转换成String类型来进行比对; 字符串最好以””方式初始化,否则在分词的时候会出现”Nullzxpp 今年 多大了”系统获取不到zzxp的词性","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"异常处理","slug":"NLP/异常处理","permalink":"http://yoursite.com/categories/NLP/异常处理/"}],"tags":[{"name":"词性标注","slug":"词性标注","permalink":"http://yoursite.com/tags/词性标注/"},{"name":"词性纠正","slug":"词性纠正","permalink":"http://yoursite.com/tags/词性纠正/"}]},{"title":"初始化与实例化","slug":"初始化与实例化","date":"2018-10-21T10:59:33.000Z","updated":"2018-11-13T02:57:08.000Z","comments":true,"path":"2018/10/21/初始化与实例化/","link":"","permalink":"http://yoursite.com/2018/10/21/初始化与实例化/","excerpt":"初始化有包含类的初始化和对象的初始化等,实例化一般是指对象的实例化,三者顺序为:类的初始化,类的实例化(新建对象),对象初始化.","text":"初始化有包含类的初始化和对象的初始化等,实例化一般是指对象的实例化,三者顺序为:类的初始化,类的实例化(新建对象),对象初始化. 一.概念 二.总结 一.概念 1.先说实例化：对类进行对象的构建：Car car=new Car();需要为对象分配内存空间–着重于动作; 2.再说初始化:初始化有包含类的初始化和对象的初始化等． 对象的初始化，主要是赋值操作:默认赋值,自定义赋值.在Car类实例化了一个对象car之后,可以分成两方面: (1)默认赋值:那么该对象就会拥有所属类的默认状态(拥有类的方法,属性及其默认值) (2)自定义赋值:car.price=13322223;就对car对象进行了一个初始化.–着重于状态 二.总结 (1)类的初始化先于类的实例化，然后再是对象初始化; (2)实例化主要是为对象分配内存空间; (3)初始化主要进行赋值,最常见的=就是一个赋值操作.","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"知识点","slug":"java/知识点","permalink":"http://yoursite.com/categories/java/知识点/"}],"tags":[{"name":"实例化","slug":"实例化","permalink":"http://yoursite.com/tags/实例化/"},{"name":"初始化","slug":"初始化","permalink":"http://yoursite.com/tags/初始化/"}]},{"title":"依赖注入系列之注入实现(二)","slug":"依赖注入系列之注入实现(二)","date":"2018-10-17T06:36:30.000Z","updated":"2018-11-13T03:12:16.000Z","comments":true,"path":"2018/10/17/依赖注入系列之注入实现(二)/","link":"","permalink":"http://yoursite.com/2018/10/17/依赖注入系列之注入实现(二)/","excerpt":"该篇是依赖注入的第二篇，注入实现的方式有两种,一种是配置注入,根据配置文件设置数据的依赖与注入信息;还有一种是进行注解注入,通常是以Spring框架下的注解来进行表述的,其中配置注入的优先级高于注解注入,因为后期维护成本低的原因.","text":"该篇是依赖注入的第二篇，注入实现的方式有两种,一种是配置注入,根据配置文件设置数据的依赖与注入信息;还有一种是进行注解注入,通常是以Spring框架下的注解来进行表述的,其中配置注入的优先级高于注解注入,因为后期维护成本低的原因. 一.原因 二.基本流程 2.1 容器的信息设置 2.2 注入方式与获取方式 三.配置注入 3.1 setter方法上注入 3.3 构造器注入 四.注解注入 4.1 bean的注册策略 4.1.1 Spring注解 4.1.2 java注解 4.2 配置文件中设置扫描与注解 4.2.1 配置文件中开启 4.2.2 @SpringBootApplication方式开启 4.2.3 javaConfig类中开启 4.3 bean对象的生成策略 4.3.1 xml中配置scope 4.3.2 @Scope配置 4.3.3 Scope配置值讲解 4.4 bean对象的获取策略 4.4.1 @AutoWired获取bean对象 4.4.2 @Resource获取bean对象 4.4.3 required属性 4.5 getBean()获取bean对象 4.6 bean的注入结果测试 五.参考文献 一.原因 主要是简化对某个组件(方法，属性，类)的调用;因为直接注入就可以完成任务.(可以注解注入或者配置注入)－－简化程序． 二.基本流程声明: bean就是一个类,不是所谓的对象,下文有时说bean对象可以理解为类的对象.其实bean有时候更多针对的是自定义类型的类.因为自定义类型的类通过设置bean,才真正提高处理自定义类型数据的效率(好好理解),因为基本类型的方法已经设置好了.注入的过程如下: 都是需要将注入对象/依赖对象对应的实现类注册到Spring容器中,形成对应的bean/容器,然后根据不同的注入方式使用不同的获取方式,来获取对应的容器对象,期间Spring会对相应的bean自动进行new()方法,得到相应的bean对象并赋值.上述过程涉及到:容器的信息设置,获取方式 2.1 容器的信息设置其中容器的id/名称与类型的设置可以参考以下情况: (1)方法上注解: 方法名是容器id;返回值类型是容器类型(默认) (2)变量上注解: 对象名是容器id,对象的实现类才是容器的类型(默认)–因为Spring会自动new()方法,因此需要容器的类型是实现类. (3)在类上注解: 类名是容器id,类的全名是容器的类型; 2.2 注入方式与获取方式 其中获取方式是因注入方式的不同而不同,主要有两种注入方式:配置注入–硬编码(最开始的);注解注入–软编码. (1)其中配置注入:不需要开启扫描,注解,默认自动开启.直接进行测试类运行即可获取对应的bean对象; (2)注解注入:则需要在配置文件中设置开启扫描,注解功能,之后使用@autoWired或@Resource获取bean对象.引申下: ❋硬编码:以主要通过修改配置文件的形式实现业务逻辑的修改;Spring就是典型的例子.–后期成本低,使用的优先级高 ❋软编码:主要通过修改代码文件实现业务逻辑的修改.–后期成本高,优先级低 三.配置注入 含义：设置bean间的引用关系来设置类与不同类的对象的引用关系,并将内容存放在配置文件中,因此该注入方法叫做配置注入.xml注入/配置注入,不需要扫描，直接在程序运行时，即可实时获取.主要是用在两个地方:构造函数,setter方法上． 3.1 setter方法上注入123456789101112package com.bless.springdemo.action; public class SpringAction &#123; //注入对象springDao private SpringDao springDao; //一定要写被注入对象的set方法 public void setSpringDao(SpringDao springDao) &#123; this.springDao = springDao; &#125; public void ok()&#123; springDao.ok(); &#125; &#125; 上述代码:我们可以看到:SpringAction类要引用对象springDao,它是属于SpringDao类的,我们可以使用配置文件设置引用关系,内容如下:12345678910111213&lt;!--为引用和被引用双方创建bean,用bean来配置引用关系&gt;&lt;!--创建了名为springAction的bean,属性有springDao这个对象 :实际上就是设置好了引用关系--&gt;&lt;!--name对应类名;class对应类的全名--&gt;&lt;bean name=\"springAction\" class=\"com.bless.springdemo.action.SpringAction\"&gt; &lt;!--(1)依赖注入,配置当前类中相应的属性--&gt; &lt;!--property 标签中name是对象名, ref则是对象所属的bean的id名--&gt; &lt;property name=\"springDao\" ref=\"springDao\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--因为对象是类创建的,因此需要配置能够创建springDao这个对象的类,这里是指实现类--&gt;&lt;bean name=\"springDao\" class=\"com.bless.springdemo.dao.impl.SpringDaoImpl\"&gt;&lt;/bean&gt; spring将SpringDaoImpl对象实例化为springDao,并且调用SpringAction的setSpringDao方法将springDao这个对象注入.说明下: (1)bean中的name对应的是名字: ❋在类上(如SpringAction),对应的是类名; ❋在对象中,对象的是对象名 (2)bean中的class对应的是类型: ❋在类上(如SpringAction),对应的类的全名; ❋在对象中则为实现该对象的类 (3)property的name是对象名;ref则为对象所属的bean的id名; 3.3 构造器注入构造函数如下:123456789101112131415public class SpringAction &#123; //注入对象springDao private SpringDao springDao; //注入对象user,属于User类 private User user; public SpringAction(SpringDao springDao,User user)&#123; this.springDao = springDao; this.user = user; System.out.println(\"构造方法调用springDao和user\"); &#125; public void save()&#123; user.setName(\"卡卡\"); springDao.save(user); &#125; &#125; 然后配置文件如下:12345678&lt;!--配置bean,配置后该类由spring管理--&gt; &lt;bean name=\"springAction\" class=\"com.bless.springdemo.action.SpringAction\"&gt; &lt;!--(2)创建构造器注入,如果主类有带参的构造方法则需添加此配置--&gt; &lt;constructor-arg ref=\"springDao\"&gt;&lt;/constructor-arg&gt; &lt;constructor-arg ref=\"user\"&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; &lt;bean name=\"springDao\" class=\"com.bless.springdemo.dao.impl.SpringDaoImpl\"&gt;&lt;/bean&gt; &lt;bean name=\"user\" class=\"com.bless.springdemo.vo.User\"&gt;&lt;/bean&gt; 讲解方式也与setter方法一样,不再赘述; 最后待全部配置好了,之后使用无需配置扫描开关,程序运行即可实现,依赖对象的自动注入. 四.注解注入注解注入的方式分词两步: ❋容器注册(创建对应容器,交给Spring管理); ❋容器对象获取(获取对应容器的对象,进行注入) 4.1 bean的注册策略 既然是注解注入的方式,那么bean的注册主要是由Spring注解和java注解(javaConfig类)完成.其实也可以由applicationContext.xml完成,但不属于注解注入的范畴. 4.1.1 Spring注解主要是注解@Compoment及其分支． 通过这个注解可以讲所定义的JAVAbean或POJO类注册到Spring容器当中，并且根据所涉及的JAVAbean所处的层不同，还可以将@Component特化成一下几个注解：@Repository，用于DAO层，通常用于标记pojo类；@Service，用于业务逻辑层，通常用来标记JAVAbean，@Controller，用于控制表示层，通常也是用于标记JAVAbean的。使用方式也几乎一致,以@Repository注解与@Compoment注解为例:在Person类中:1234567891011121314151617181920212223242526272829303132package bjtu.wellhold.testSpring;@Repository(\"person\")public class Person &#123; @value(\"张山\") private String name; @value(\"19\") private int age; @Resource(\"myschool\") private School school; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getSchool() &#123; return school; &#125; public void setSchool(School school) &#123; this.school = school; &#125; @Override public String toString() &#123; return \"Person [name=\" + name + \", age=\" + age + \",\"+school.toString(); &#125; &#125; School类对象在Person类中被引用,Person类还是把它当成一个类的成员看待(Setter,getter,toString方法都有),这点需要注意.因此，在School类中:1234567891011121314//注册了一个名为myschool的bean,类型是School--通常指实现类型@Component(\"myschool\")public class School&#123; private String name; public void setName(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public String toString() &#123; return \"school=\"+name; &#125; &#125; 上面两个类的注解等价于,在配置文件中编写:12345678&lt;bean id=\"person\" class=\"bjtu.wellhold.testSpring.Person\"&gt;&lt;property name=\"name\" value=\"张山\"&gt;&lt;/property&gt;&lt;property name=\"age\" value=\"19\"&gt;&lt;/property&gt;&lt;!--ref对应的是bean的id,name为对象名--&gt;&lt;property name=\"school\" ref=\"myschool\"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!--class对应的是对象的实现类型--&gt;&lt;bean id=\"myschool\" class=\"bjtu.wellhold.testSpring.School\"&gt; 因为name与age都是基本数据类型,不是用户自定义类型,也就没有必要创建的必要,但是school则是一个自定义类型的对象. 4.1.2 java注解 主要是在JavaConfig包下,设置不同的类来实现不同bean的注册.除了用applicationContext.xml进行容器/bean注册之外,其实也可以用JavaConfig进行配置1234567891011@Configurationpublic class MyjavaConfig&#123;@Bean(\"person\")public Person initialPerson()&#123; return new Person(\"张山\",\"19\");&#125;@Bean(\"myschool\")public School initialSchool()&#123; return new School(\"实验中学\");&#125;&#125; 4.2 配置文件中设置扫描与注解 那么已经实现了容器的注册,那么如何获取呢,在获取之前,需要开启自动扫描(也会隐式开启注解功能),主要有三种方法:配置文件中开启,SpringbootApplication注解方式开启,javaConfig类中开启. 4.2.1 配置文件中开启 在配置文件(bean.xml/ApplicationContent.xml/Application.xml)中设置扫描与注解,具体方式如下:1234//打开注解功能: &lt;context:annotation-config /&gt; //打开扫描注解的功能,同时会隐式启用注解功能,因此值打开这个即可 &lt;context:component-scan base-package=\"bjtu.wellhold.testSpring\"/&gt; 4.2.2 @SpringBootApplication方式开启如果项目运行了Springboot,可以使用:123456789//@SpringBootApplication启动自动扫描功能.import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 4.2.3 javaConfig类中开启主要是添加如下:12345678910111213141516import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;//@Configuration是第一位@Configuration//basePackages的地址为配置文件所在的包@ComponentScan(basePackages = \"com.cn.config\")public class MyjavaConfig&#123;@Bean(\"person\")public Person initialPerson()&#123; return new Person(\"张山\",\"19\");&#125;@Bean(\"myschool\")public School initialSchool()&#123; return new School(\"实验中学\");&#125;&#125; 4.3 bean对象的生成策略 bean的对象在被获取之前,我们需要对bean生成对象进行设置,我们把这节简称为bean对象的生成策略.该节内容主要是围绕@scope这个注解进行说明,也可以直接在xml中对相应的bean进行配置. 4.3.1 xml中配置scope方式就是设置scope属性值12345&lt;bean id=\"userAction\" class=\"com.alibaba.project.user.UserAction\" scope=\"prototype\"&gt; &lt;property name=\"service\"&gt; &lt;ref bean=\"userService\"/&gt; &lt;/property&gt;&lt;/bean&gt; 4.3.2 @Scope配置@Scope配置属性值123456@Service(\"scUserService\")@Scope(\"prototype\")public class ScUserServiceImpl implements ScUserService &#123; public static final Logger log = Logger.getLogger(ScUserServiceImpl.class); //TODO 业务代码...&#125; 说明: 当有@Scope(BeanDefination.SCOPE_PROTOTYPE)修饰时，推荐使用getApplicationContext().getBean()去获取对应的对象 4.3.3 Scope配置值讲解 (1).singleton单例模式(默认)，每个bean(根据id区分每个bean),只会创建一个bean对象,大家可以通过调用共享这个bean对象. (2)、prototype:多例模式，每次请求注入特定的bean,Spring都会重新创建新的bean对象 (3)、request:在一次请求范围内，创建一个bean实例 –请求阶段(request) (4)、session:在一个会话范围内，创建一个bean实例 –会话阶段(session) (5)、globle-session:在servletContext范围内，创建一个实例–应用程序阶段(application) 4.4 bean对象的获取策略 本质上通过java的反射机制,得到bean/类,之后使用new方法进行对象创建并赋值,然后注入给注解的对象. bean对象的获取其实就是bean对象的数据注入的重要步骤,具体如何注入我们不深究,我们重点讲解此方面.现在常用的获取bean对象的方式是采用@AutoWired/@Resource方式(都包含bean对象的获取与数据注入两方面),getBean()方式,其中@AutoWired是优先根据类型进行获取,@Resource则是优先根据名称获取bean对象,之后进行数据注入,上述两种都是隐式获取bean对象,而getBean()可以同时根据名称类型进行显式查找bean对象,也可以任选一种,getBean()只包含bean对象获取,不包含数据注入.至于如何判断bean的名称和类型,请参考2.1 容器信息的设置.说明: @Resource是J2EE对Spring注解(@Compoment系列注册类注解)的支持， @Autowired是Spring本身支持的注解 4.4.1 @AutoWired获取bean对象(1)AutoWired的获取策略 不添加任何信息的时候,默认采用按类型获取相应bean对象,之后进行注入;如果获取不到,则按bean名称进行获取,在获取不到则报异常(NullPointerEXception–NPE); 指定按名称获取bean的时候,需要使用@Qualifier进行完成,获取不到则报错,代码如下;1234// @AutoWired public String Search(@Qualifier(\"question\") Question question )&#123; &#125; 指定按类型获取,获取不到也是直接报错 4.4.2 @Resource获取bean对象 不添加任何信息的时候,默认采用对象名/方法名/类名进行获取bean对象,获取不到则退而求其次,根据类型进行获取bean对象,如果还查不到会报异常. 一旦指定了@Resource的name属性，就只能按名称装配了,查不到就会报异常 4.4.3 required属性作用: 设置找不到bean对象是否会报异常@Resource(required=false): 没有查到也不会报空指针异常(NullPointerException–NPE),@AutoWired同理 4.5 getBean()获取bean对象 常用于主程序当中,因为getBean()的使用要加载所有的bean容器,并且是显式获取bean对象(只包含查找bean对象,不包含数据注入)12345//加载配置文件(例如开启激活) ApplicationContext cfg = new ClassPathXmlApplicationContext(\"Config.xml\"); //显式获取指定bean的对象 Person person = cfg.getBean(\"person\", Person.class); System.out.println(person); 4.6 bean的注入结果测试 那么如何测试依赖对象是否已经查找并注入成功,我们可以新建一个testUnit类进行测试,采用applicationContext的实例的getBean()方法获取bean的对象.123456789101112package bjtu.wellhold.testSpring;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class testUnit &#123; public static void main(String[] args) &#123; //进行配置文件的加载 ApplicationContext cfg = new ClassPathXmlApplicationContext(\"Config.xml\"); //显式获取指定bean的对象 Person person = cfg.getBean(\"person\", Person.class); System.out.println(person); &#125;&#125; 五.参考文献 ❋ spring中scope作用域(转) ❋ Spring之基于注解的注入 ❋ 使用spring注解——定义bean和自动注入 ❋ spring 四种依赖注入方式以及注解注入方式 ❋ spring bean scope简要说明，有代码示例 ❋ Spring：基于注解的依赖注入的使用","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"注解","slug":"java/注解","permalink":"http://yoursite.com/categories/java/注解/"}],"tags":[{"name":"注解注入","slug":"注解注入","permalink":"http://yoursite.com/tags/注解注入/"},{"name":"配置注入","slug":"配置注入","permalink":"http://yoursite.com/tags/配置注入/"}]},{"title":"java知识点<二>","slug":"java知识点(二)","date":"2018-10-17T05:47:24.000Z","updated":"2018-11-13T02:32:38.000Z","comments":true,"path":"2018/10/17/java知识点(二)/","link":"","permalink":"http://yoursite.com/2018/10/17/java知识点(二)/","excerpt":"java知识点的第二篇,内容都是一些比较易混淆的知识点，主要有：java的实例化对象的顺序,内存的分区,还有静态变量与对象变量的区别,POJO,class,JavaBean关系(重点),最后还说了下公有与私有的区别.","text":"java知识点的第二篇,内容都是一些比较易混淆的知识点，主要有：java的实例化对象的顺序,内存的分区,还有静态变量与对象变量的区别,POJO,class,JavaBean关系(重点),最后还说了下公有与私有的区别. 一.java实例化对象加载顺序 二.POJO,class,JavaBean关系 三.静态变量与对象变量 四.引申: 4.1 内存的分区: 4.2 公有与私有的理解 五.@SuppressWarnings 一.java实例化对象加载顺序实例化某一个类的对象的时，块和构造器的相对运行顺序: (1)配置文件–&gt; (2)父类静态块(含有静态成员的变量或者方法)–&gt; (3)子类静态块–&gt; (4)父类普通块–&gt; (5)父类构造器–&gt; (6)子类普通块(普通变量定义/声明)–&gt; (7)子类构造器(优先调用父类的构造方法)–&gt; (8)@autowired(这个时候是生成普通变量所属类的bean对象,给注解对应的普通变量进行赋值,帮助子类构造器完成实例化对象)–&gt; (9)普通方法(因为生成了类的对象–使用构造方法). 二.POJO,class,JavaBean关系1.POJO:不包含业务逻辑的单纯用来存储数据的java类;2.javabean/bean:遵循以下约定的java类: (1)要有无参构造方法,可以不需要有参构造方法(对属性进行实例化赋值); (2)属性获取设置只可以使用setter,getter方法实现; (3)可序列化(实现java.io.Serializable接口) (4)具体的;3.class:单纯的表示一个类,可以是抽象的,具体的;是无约束条件的Bean4.范围:POJO&lt;javaBean&lt;class 三.静态变量与对象变量 静态变量/类变量 实例变量/对象变量 调用方式 可以不需要实例化,直接通过类名调用, 也可以通过对象名调用。只是强调这个变量是共享. 只可以通过对象名调用 生命周期 随着类的加载–&gt;类的消失 对象的创建至消失 共享范围 类的所有对象,(加上public,则为全局静态变量,为整个项目共享–少用) 只有对象才能使用. 存放位置 全局区（静态区） 堆内存区 值是否可以改变 静态变量其实也可以改变,只不过是强调共享. 可以 四.引申:4.1 内存的分区: (1)栈区:函数的参数与局部变量值,由编译器分配与释放(对应的方法结束后,对其进行回收); (2)堆区:存放的是实例变量之类的值,可以由程序员分配(自己设定何种变量),待程序结束之后,程序员手动释放或者系统定时自动回收; (3)全局区/静态区:未初始化,全局变量与静态变量是相邻的,初始化后放在一块;由程序员分配(自定义). (4)常量区:存放字符串常量; (5)代码区:存放函数体;说明:(1)到(5)生命周期越来越长. 4.2 公有与私有的理解其实私有与公有的定义可以这样理解,以属性值的公有与私有说明: 私有:获取的属性值,因对象的不同而不同; 公有:获取的属性值,所以对象都是一样的 五.@SuppressWarnings@SuppressWarnings(“resource”)–用来压制资源泄露警告的。比如使用io类，最后没有关闭@SuppressWarnings(“unused”)–在使用IDE如eclipse的时候，当定义了一个变量如int a=0;但是你后面根本就没有使用到这个变量，这一行的前面会有一个黄色的警告标志，将鼠标移动到上面会提示“这个变量从未被使用”，用上面的标注后就没有这个提示了12345678910111213141516171819all----抑制所有警告boxing----抑制装箱、拆箱操作时候的警告cast----抑制映射相关的警告dep-ann----抑制启用注释的警告deprecation----抑制过期方法的警告fallthrough----抑制在switch中缺失breaks的警告finally----抑制finally模块没有返回的警告hiding----incomplete-switch----忽略没有完整的switch语句nls----忽略非nls格式的字符null----忽略对null的操作rawtypes----使用generics时忽略没有指定相应的类型restriction----serial----忽略在serializable类中没有声明serialVersionUID变量static-access----抑制不正确的静态访问方式警告synthetic-access----抑制子类没有按最优的方法访问内部类的警告unchecked----抑制没有进行类型检查操作的警告unqualified-field-acces----抑制没有权限访问的域的警告unused----抑制没被使用过的代码额警告 抑制单类就@SuppressWarings（“XXX”）抑制多类就@SuppressWarings（value={“XXX”,“XXX”}）","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"知识点","slug":"java/知识点","permalink":"http://yoursite.com/categories/java/知识点/"}],"tags":[{"name":"变量","slug":"变量","permalink":"http://yoursite.com/tags/变量/"},{"name":"Bean","slug":"Bean","permalink":"http://yoursite.com/tags/Bean/"}]},{"title":"java知识点<一>","slug":"java知识点(一)","date":"2018-10-16T06:11:37.000Z","updated":"2018-11-13T02:33:08.000Z","comments":true,"path":"2018/10/16/java知识点(一)/","link":"","permalink":"http://yoursite.com/2018/10/16/java知识点(一)/","excerpt":"本文主要是涉及java的一些基本概念,打算作为一个系列,作为查漏补缺,尽量用比较精辟,简短,通俗易懂的话语来解释,当然这些也离不开网友的宝贵经验,特此感谢.","text":"本文主要是涉及java的一些基本概念,打算作为一个系列,作为查漏补缺,尽量用比较精辟,简短,通俗易懂的话语来解释,当然这些也离不开网友的宝贵经验,特此感谢. 1.SDK 2.pop3,SMTP,IMAP 3.JPA 4.三层架构与MVC 5.ORM,OGM,SDN,POJO,JPA 5.1 概念 5.2 关系 6.SessionFactory 7.Map&lt;String,Object&gt; 8.page,request,session,Application的作用域范围 9.Page,Pageable的区别与联系 10.model,ModelMap,ModelAndView 11.spring中实体层,DAO层,service层,controller(控制层),view层 12.MVVM (1)原因 (2)概念 (3)产品 13.前端实用框架avalonJS入门 14.GET，POST，PUT，DELETE 15.dom 16.int与integer 17.vm文件 18.map与HashMap 1.SDK 软件开发工具包（缩写：SDK、外语全称：Software Development Kit）一般都是一些软件工程师为特定的软件包、软件框架、硬件平台、操作系统等建立应用软件时的开发工具的集合 2.pop3,SMTP,IMAP这个是电子邮件的三种协议其中： ①pop3是收取邮件 ②smtp是发送邮件,这两个是配套一起使用的 ③imap是邮件收发一起都包含的 3.JPA JPA是Java Persistence API的简称(就是一种映射)，中文名Java持久层API,是JDK 5.0注解或XML描述对象－关系表的映射关系,就是定义的对象(属性/实体)与数据库当中的属性/实体进行映射 4.三层架构与MVC三层架构与MVC 5.ORM,OGM,SDN,POJO,JPA5.1 概念 (1)ORM:Object/Relation Mapping–对象/关系数据库映射 (2)OGM:Object/Graph Mapping–对象/图数据库映射 (3)POJO:plain Ordinary Java Object–简单java对象 (4)SDN:Spring Data Neo4j–为访图数据库提供接口(是在建立映射之后) (5)JPA:Java Persistence API–Java持久层API–主要是为访问关系数据库提供接口(是在建立映射之后) 5.2 关系 创建一个POJO对象,对象建模,然后不同的存储库/数据库访问接口(SDN/JPA)使用对应的映射(ORM,OGM)实现对象与数据库当中元素的转换(映射),最终实现CRUD的功能.以SDN使用OGM映射机制为例,只需要添加简单的注解就可以实现对象与图数据库的映射 6.SessionFactory 存储着连接数据库的配置信息和映射信息(java对象与数据库),所以相当于一个连接池(里面有很多的连接信息–连接数据库的配置信息和映射信息),有时候有多个连接,因此存在多个session–(与数据库的会话),相当于Connection 7.Map&lt;String,Object&gt; 实际上String就是Key(Key必须是String类型),Value是Object类型,说明Value可以存储任何类型的数据12实际上就是定义了名为aa的Map容器,这个容器规定了Key和Value的类型Map&lt;String,Object&gt; aa=Map&lt;String,Object&gt; 名为aa的Map容器主要是为了调用put与get方法,例如aa.put(&quot;user&quot;, obj),可以认为是把Object类型的对象–obj放入到aa这个Map容器当中,对象obj在容器中的代号是”user”(也就是Key为user,Value是对象obj),最后取出语句就是:aa.get(user),根据这个名为user的key,取出对应的value–obj. 8.page,request,session,Application的作用域范围 (1)page:就是在本页面有效; (2)request:就是在请求发送直至服务器回应结束,这个周期当中有效,可能中间会经过多个跳转页面.但是手动刷新之后,又是一个新的request. (3)session:一次会话的有效期,例如从浏览器开启直至关闭,在此期间,访问某个网站的服务器,其都知道是同一个人请求.这就是一次会话的过程. (4)Application :从应用的开启到关闭,存活时间最长,例如服务器的开启与关闭,电脑的开启与关闭. 好好屡屡:从页面之间–&gt;多个页面–&gt;会话–&gt;应用 page,request,session,Application区别 9.Page,Pageable的区别与联系 Page与Pageable 10.model,ModelMap,ModelAndView作用：装载数据,传递数据.{从控制类（以controller为例）向页面（jsp页面）传值}。 model,ModelMap,ModelAndView联系 11.spring中实体层,DAO层,service层,controller(控制层),view层在项目中，控制器完成的工作主要是流程控制，及请求相应，根据对应的结果返回对应的视图，具体的业务代码代码不应该写到控制器里，而是交给service负责实现。比如要验证一个用户名是否存在，应该在控制器里调用Service层的验证的方法，由service去验证用户名是否存在，而service再调用dao层，dao层跟底层的数据库打交道，查询出结果返回service层，service层返回结果给控制器层。控制层根据结果返回对应的资源。如果涉及到到了dao的操作，都是由service去调用dao的。个人认为，所谓的耦合是指在编译时就确定了类型，如果要更改，得全部修改。而采用面向接口编程，运行时才知道具体实现的是那个类。控制器在运行时才知道具体实现的是哪一个service，service在运行的时候才知道具体实现的是哪一个dao。dao层，service层，controller层，entriy层的理解关于service层的作用，有点疑惑，求各路前辈指点迷津 12.MVVM(1)原因 由于前端开发混合了HTML、CSS和JavaScript，而且页面众多，所以，代码的组织和维护难度其实更加复杂，这就是MVVM出现的原因。 现在，随着前端页面越来越复杂，用户对于交互性要求也越来越高，想要写出Gmail这样的页面，仅仅用jQuery是远远不够的。MVVM模型应运而生。 MVVM最早由微软提出来，它借鉴了桌面应用程序的MVC思想，在前端页面中，把Model用纯JavaScript对象表示，View负责显示，两者做到了最大限度的分离。 (2)概念model-view-ViewModel:model负责逻辑;view:负责显示;ViewModel:负责逻辑与显示的同步,(把Model和View关联起来的就是ViewModel。ViewModel负责把Model的数据同步到View显示出来，还负责把View的修改同步回Model) (3)产品avalonjs 是国内大神司徒正美开发的一个迷你MVVM框架,可以轻松实现视图与数据的隔离和双向绑定. 13.前端实用框架avalonJS入门前端实用框架avalonJS入门 14.GET，POST，PUT，DELETE HTTP中的GET，POST，PUT，DELETE就对应着对这个资源的查，改，增，删4个操作。而我们在实际操作的时候，就get和post两种用的比较多。而post方法又比get方法要安全的多，因为get方法会将参数都放在url上（就是典型的get方法），而且get方法传送的数据量较小，不能大于2KB；另，用get方法传参到后台，会造成中文乱码等问题 15.dom dom树其实就是html/xml文档的一种标签总体结构. 16.int与integer integer是int的封装类:Integer a = new Integer(1),其中1是整型.但要记住:123Integer a = new Integer(1);Integer b = new Integer(1);int c = 1; a= = c为true。b= = c为true,但是a = =b为false,因为a和c,b与c比较的是数值,a,b比较的是内存地址.不同的内存地址,指向同一个值–1. 17.vm文件 基于java的velocity模板引擎的一种页面控制文件，你可以用文本编译器打开，能够看到他只是一些类似html的语句和一种叫VLT的语句构成的。 18.map与HashMap map是接口,而HashMap则是接口map的实现类,加之map接口的实例化不能用new方法,因此有:Map map=new HashMap()","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"知识点","slug":"java/知识点","permalink":"http://yoursite.com/categories/java/知识点/"}],"tags":[{"name":"java","slug":"java","permalink":"http://yoursite.com/tags/java/"},{"name":"知识点","slug":"知识点","permalink":"http://yoursite.com/tags/知识点/"}]},{"title":"常见问题及其设置<一>","slug":"软件安装及其设置(一)","date":"2018-10-16T05:47:53.000Z","updated":"2018-11-13T03:06:52.000Z","comments":true,"path":"2018/10/16/软件安装及其设置(一)/","link":"","permalink":"http://yoursite.com/2018/10/16/软件安装及其设置(一)/","excerpt":"本文主要整理一些常见的安装／配置小问题，由于涉及到的方面比较多，因此打算作为一个系列进行整理，文中参考了不少网友的经验，在此说声谢谢，同时也希望我的整理能够帮助他人．","text":"本文主要整理一些常见的安装／配置小问题，由于涉及到的方面比较多，因此打算作为一个系列进行整理，文中参考了不少网友的经验，在此说声谢谢，同时也希望我的整理能够帮助他人． 1.powershell/cmd换字体 1.1下载powershell6 1.2下载Windows Management Framework5.1组件 1.3增加字体 1.4添加相应的软件目录到环境目录, 2.代替记事本的软件–emeditor (已经下好了安装包) 2.1 下载安装包及其密钥 2.2 将软件所在目录添加到环境变量 3.eclipse+maven环境配置 4.eclipse 安装各种版本spring插件-STS 5.notepad++ markdown实时预览,高亮显示 6.各程序的快捷键 7.修改右键菜单选项 8.天津大学邮箱登录outLook 9.手心输入法快捷键 10.提高鼠标灵敏度 11.a recent hardware or software change might be the cause 12.PDF解密 13.UML的用例图箭头指向 14.UML序列图 15.重置cmd 16.系统变量、用户变量、环境变量,ClassPath，path，JAVA_HOME 17.Spark在window下的安装 18.代理服务器出现错误(导致上不了网) 19.shell界面理解 20.windows界面只显示时间不显示日期 21.设置网页锚点 1.powershell/cmd换字体1.1下载powershell61http://down1.greenxf.com:8010/系统工具/系统其他/winpowershell(www.greenxf.com).zip 1.2下载Windows Management Framework5.1组件Windows Management Framework 5.1地址 1.3增加字体教程方法Windows7更改替换cmd（powershell)字体完全方法教程 1.4添加相应的软件目录到环境目录, 最好是系统环境变量的Path变量 2.代替记事本的软件–emeditor (已经下好了安装包)2.1 下载安装包及其密钥EmEditor下载地址 2.2 将软件所在目录添加到环境变量 最好是系统变量的Path路径中 3.eclipse+maven环境配置eclipse+maven环境配置 4.eclipse 安装各种版本spring插件-STS地址下载 5.notepad++ markdown实时预览,高亮显示Notepad++ 添加MarkdownViewerPlusPlus插件Notepad++上使用Markdown及其高亮Highlight设置 6.各程序的快捷键 先将应用程序进行改名,之后添加到环境变量之中,Path变量1234xshell----xs;eDiary----dyEverEdit----eePowerShell---pwsh 7.修改右键菜单选项使用右键管家 8.天津大学邮箱登录outLook 9.手心输入法快捷键123❋--fh①--yqszhexo的yaml设置--ifmm 10.提高鼠标灵敏度提高鼠标灵敏度 11.a recent hardware or software change might be the cause说明需要修复分区引导使用U盘进入pe系统,点击修复图标,之后选择特定的启动盘,之后重启即可. 12.PDF解密PDF解密 13.UML的用例图箭头指向 UML之用例图箭头方向UML建模——用例图（Use Case Diagram） 14.UML序列图UML序列图 15.重置cmd进入注册表： 在注册表编辑器左边的文件夹目录里面找到:HKEY_CURRENT_USER \\ Console \\ %SystemRoot%_system32_cmd.exe删除文件夹 %SystemRoot%_system32_cmd.exe即可 16.系统变量、用户变量、环境变量,ClassPath，path，JAVA_HOME环境变量粗识 17.Spark在window下的安装Spark在Windows下的环境搭建java+scala+Spark+Hadoop，期间还有环境变量的设置。 18.代理服务器出现错误(导致上不了网)代理服务器出现问题 19.shell界面理解 就是用户和系统/软件的一个交互界面,也是与内核交互的一种接口,用户只需要输入若干命令,就可调用系统/软件功能完成特定的操作.例如: 输入spark-shell 进入如下shell界面: 使用scala语言进行交互,操作.加减乘除等等. 20.windows界面只显示时间不显示日期 可能是任务栏太窄了,拉长些就可以. 21.设置网页锚点 主要通过复制个人网站的目录导航的链接实现锚点,具体实现如下:","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/工具/"},{"name":"小问题","slug":"工具/小问题","permalink":"http://yoursite.com/categories/工具/小问题/"}],"tags":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/tags/工具/"},{"name":"小问题","slug":"小问题","permalink":"http://yoursite.com/tags/小问题/"}]},{"title":"IDEA使用(一)","slug":"IDEA使用(一)","date":"2018-10-16T05:19:38.000Z","updated":"2018-11-13T04:04:58.000Z","comments":true,"path":"2018/10/16/IDEA使用(一)/","link":"","permalink":"http://yoursite.com/2018/10/16/IDEA使用(一)/","excerpt":"这篇主要是讲述本人在使用IDEA遇到的一些问题及其解决方法,以便节省开发时间,该文参考了许多网友的方法、经验,于此,对其表达感谢,这个系列会不定时更新.","text":"这篇主要是讲述本人在使用IDEA遇到的一些问题及其解决方法,以便节省开发时间,该文参考了许多网友的方法、经验,于此,对其表达感谢,这个系列会不定时更新. 1.IDEA2018.2.1的破解方法: 2.IDEA配置MAVEN 3.IDEA代码注释斜体 4.IDEA的MAVEN更新 5.IDEA进行http测试 6.Eclipse项目导入到IDEA之中 7.IDEA项目进行打包 8.IDEA快捷键 9.小技巧 9.1代码颜色 9.2光标位置 9.3 方法跳转 9.4 光标与括号 10.配置自定义浏览器 11.jQuery的引入 12.手动添加jar到本地仓库 13.快速创建基于某个接口的类 14.快速进行错误定位: 15.消除历史文件 16.类文件结构查看: 1.IDEA2018.2.1的破解方法:1http://www.ddooo.com/softdown/127543.htm#dltab 重要步骤: (1)先将补丁包放到安装目录的bin目录下; (2)之后用记事本打开“idea64.exe.vmoptions”和“idea.exe.vmoptions”这两个文件，再后面添加破解文件的路径:1-javaagent:D:\\JetBrains\\IntelliJ IDEA 2018.2.1\\bin\\JetbrainsCrack-3.1-release-enc.jar (3)生成快捷方式:之后生成”idea64.exe”的快捷方式,(重点),最好不直接用安装时生成的快捷方式. (4)使用激活码激活:验证激活的时候,使用激活码: 上述安装包及其破解文件,在文件夹中有. 2.IDEA配置MAVEN主要步骤: (1)设置MAVEN版本及其本地仓库(setting.xml) (2)设置MAVEN镜像(配置了本地apache-maven setting.xml中的软件源)参考文献:IntellIJ IDEA 配置 Maven 以及 修改 默认 Repository下载地址 3.IDEA代码注释斜体idea多行注释如何取消文档注释 的斜体 4.IDEA的MAVEN更新idea更新maven依赖包 一般是采用第一种方法比较快速 5.IDEA进行http测试idea中使用restclient测试接口发送http请求 6.Eclipse项目导入到IDEA之中【Intellij IDEA】eclipse项目导入 7.IDEA项目进行打包idea将maven项目打包成war包的方式，以及使用war包 8.IDEA快捷键 要求 快捷键 备注 项目运行 ctrl+shift+F10 或者在右上角的绿色运行 项目重新运行 ctrl+F5 代码复制粘贴一行 ctrl+D 窗口切换 ctrl+tab 任何窗口 底部工具栏窗口切换 alt+数字 删除行 ctrl+X 可选定,默认是注释一行 注释一行 ctrl+/ 可选定,默认是注释一行 向上回车一行 ctrl+alt+enter 新建包 alt+insert 重命名 shift+f6 快速下一行 ctrl+D(down) 格式化 ctrl+alt+L 代码折叠展开/收拢 ctrl+shift+ +/- 错误定位(上/下) shift+F2/F2 项目重构 ctrl+F9 代码调试参考:问句向量化那节 9.小技巧9.1代码颜色 ❋红色代码:说明字母拼写错误; ❋灰色代码:说明尚未使用; ❋波浪线:说明字符不符合规范,可以添加到字典当中. 9.2光标位置 当一行代码假如没有分号,光标会缩进; 9.3 方法跳转 连续点击跳转,会进行循环跳转 9.4 光标与括号 IDEA中,在引用某个方法的时候,如果该方法有参数,该软件会进入到括号内,没有的话则在括号外面. 10.配置自定义浏览器 Intellij IDear配置自定义浏览器预览效果 11.jQuery的引入Idea添加JQuery库及其说明文件库文件需要自己下载:地址 12.手动添加jar到本地仓库手动添加jar到本地仓库 13.快速创建基于某个接口的类基本步骤:1file -&gt; settings -&gt; Editor -&gt; Intentions -&gt; Declaration -&gt; Implement abstract class or interface 用alt+enter实现: 14.快速进行错误定位:错误分为:语法错误(编译之前,不能运行);逻辑错误:可以运行,结果不一样快捷键:F2–&gt;下一个,shift+F2–&gt;上一个右键点击感叹号图标会出现,以下两个选项: (1)Go the high priority problems only 在优先级较高的错误间切换 (2)Go to next problem 永远跳向下一个错误和警告，不管优先级的问题 红线代表有错误的位置(主要是语法错误,). 错误的地方会有红杠线,点击红杠线,会跳转到的错误的文件(文件定位),之后可以使用F2进行定位(行内定位)具体操作步骤如下图: 具体参考:Intellij IDEA_错误与调试 15.消除历史文件 16.类文件结构查看:点击左下角:Structure,放在图标上,会有提示:","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/工具/"},{"name":"IDEA","slug":"工具/IDEA","permalink":"http://yoursite.com/categories/工具/IDEA/"}],"tags":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/tags/工具/"},{"name":"IDEA","slug":"IDEA","permalink":"http://yoursite.com/tags/IDEA/"}]},{"title":"交互实现(五)","slug":"交互实现(五)","date":"2018-10-16T04:39:34.000Z","updated":"2018-11-13T03:03:08.000Z","comments":true,"path":"2018/10/16/交互实现(五)/","link":"","permalink":"http://yoursite.com/2018/10/16/交互实现(五)/","excerpt":"交互实现是提升问答系统的用户体验重要一步,本文涉及到restful测试与可视化界面测试,期间出现了不少问题,但终究解决,写下此文便于回忆.","text":"交互实现是提升问答系统的用户体验重要一步,本文涉及到restful测试与可视化界面测试,期间出现了不少问题,但终究解决,写下此文便于回忆. 一准备过程 二.restful风格交互 2.1 实现方法 2.2 代码调试 2.2.1 Could not autowire 2.2.2 语义解析之后的查询 2.2.3 容器缺失 三.可视化界面交互 3.1 实现方法 3.2 代码调试 3.2.1 页面跳转正常,但查询失败 3.2.2 静态资源文件添加 3.2.3 页面跳转问题: 3.2.4 request.getparameter()获取不到值 四.注意事项 4.1 Controller排错三部曲: 4.2 Springboot错误详情页: 4.3 开发技巧 4.3.1 类导入原则: 4.3.2 全局变量的导入方法 4.3.3 方法调用 4.3.4 List列表输出 一准备过程 由于交互实现,涉及到:实体类Person的设计,连接及其查询Neo4j的实现方法等内容,篇幅有限,不在赘述,需要的话可以查看:可视化查询Neo4j数据库. 二.restful风格交互2.1 实现方法 我们可以基于之前可视化查询Neo4j数据库的代码,稍加改动即可,改动后的结果如下:1234567891011@Controller@RequestMapping(\"/question\")public class QuestionController &#123; @Autowired questionServiceImpl quest; @RequestMapping(\"/query\") public ModelAndView getAnswer(@RequestParam(\"question\") String question) throws Exception &#123; String result=quest.answer(question1); return result; &#125;&#125; 测试地址:http://localhost:8080/question/query/?question=zzxp的年龄是多少结果如下:说明:虽然参数是String,但是还是写成这样:question=zzxp的年龄是多少 2.2 代码调试上面的代码遇到的坑还是比较多,可能是代码写得少的原因,现在总结下: 2.2.1 Could not autowire 寓意为:不能自动装配,可能是没有实现接口的抽象方法．在编写QuestionController.java中,出现:12@AutowiredquestionService questionservice(红色波浪线); 提示Could not autowire. No beans of &#39;questionService&#39; type found.,因为questionService是没有具体实现方法的接口,不能直接实例化,因此也不能调用那些方法,因此会报错(之后在，进行调试的时候，发现确实没有实现该接口),但是PersonRepository则是有具体实现方法的接口,因此在自动装配的时候,代码如下:12@AutowiredPersonRepository personRepository; 不报错.而不是网上所说的忽略,因为IDEA语法较为严格,所以智能. 2.2.2 语义解析之后的查询例如: 原句:zzxp的年龄是多少经过语义解析之后,变为zzxp 年龄,在开发查询方法的时候,我们可以让:zzxp作为输入,结果返回的是年龄,在PersonRepository.java中,具体代码如下:12@Query(\"match (t:Person) where t.name=&#123;name1&#125; return t.age\")String getPersonAge(@Param(\"name1\") String name1); 2.2.3 容器缺失 没有为某个类添加对应的注解,因此会出现容器缺失的情况： (1)特定类的容器缺失–添加类对应层次的容器Consider defining a bean of type &#39;com.example.demo.Service.questionServiceImpl&#39; in your configuration.–对应要注入的实现类没有加注解(最好对应的接口也加上)，如dao层 @Repository 如service层 @Service–这个搞了很久 (2)接口缺少具体实现方法You generally autowire to your interface type and not impl.–调用的接口是否有具体的实现类。 (3)String类的容器缺失–很容易犯错Consider defining a bean of type &#39;java.lang.String&#39; in your configuration--@Autowired下面，没有直接紧邻类,就会报错.参考:Spring mvc 报错：No qualifying bean of type [java.lang.String] found for dependency: 三.可视化界面交互3.1 实现方法后台QuestionController如下:12345678910111213141516171819public class QuestionController &#123; @Autowired questionServiceImpl quest; /** * 记住不是HttpServletResponse response,而是HttpResponse response */ @RequestMapping(\"/index1\") public String index(HttpServletRequest request, HttpResponse response)&#123; return \"/question/query\"; &#125; @RequestMapping(\"/query\") public ModelAndView getAnswer(HttpServletRequest request, HttpResponse response) throws Exception &#123; String question1=request.getParameter(\"question\"); String result=quest.answer(question1); ModelAndView mv=new ModelAndView(\"/question/query\"); mv.addObject(\"answer\",result); return mv; &#125;&#125; 前台query.html如下:123456789101112&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:th=\"http://www.thymeleaf.org\"&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;form action=\"/question/query\"&gt; &lt;input id=\"question\" name=\"question\"/&gt; &lt;button type=\"submit\" value=\"查询\"&gt;查询&lt;/button&gt; &lt;br&gt; 你好,结果是:&lt;span th:text=\"$&#123;answer&#125;\"&gt;&lt;/span&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 实现思路如下: 通过页面跳转(/question/index1–&gt;/question/query),进入到query.html,之后在query.html之中填入问句,点击发送,形成表单数据并发送,然后用/query映射的方法获取表单数据,取出对应的问句,对问句进行处理,处理结果用ModelAndView返回给query.html,该页面,使用thymeleaf框架获取结果值.结果如下: 3.2 代码调试3.2.1 页面跳转正常,但查询失败可以连接到前台,但是查找失败的情况,前台界面具体见下图:在查询的过程之中.后台console同时也显示如下异常:123java.lang.NullPointerException: null at com.example.demo.service.impl.PersonServiceImpl.findByName(PersonServiceImpl.java:23) ~[classes/:na] at com.example.demo.controller.PersonController.search(PersonController.java:47) ~[classes/:na] 结合上面代码显示,都是findByName()方法导致查找失败,如果方法本身没有错误的话,那么就需要对调用这个方法的对象进行判断,这个对象是否存在,调用对象是否合法,最后我们在PersonServiceImpl类中找到如下代码:1PersonRepository personRepository; 虽然定义了这个接口对象,但是没有自动装配-也就是没有设置setter与getter方法,那么就无法使用接口当中的方法/属性,说明是违法调用.才会导致错误,因此需要添加setter与getter方法,那么最简单的方法就是在对象前面添加@AutoWired,最后得到结果: 3.2.2 静态资源文件添加 在之前的文章已经提及,因为IDEA对静态资源文件(代码文件不需要)不是实时更新,如静态文件有变动,需要在进行项目进行运行之前,先进行构建/重构,重构之前,先进行静态资源文件添加:把resource文件夹下的文件/文件夹添加到target/classes文件夹下,具体操作见下图: 3.2.3 页面跳转问题:(1)误用@RestController @RestController再进行页面跳转的时候会出现:(2)跳转路径与对应的资源文件 如果设置了/question/query跳转路径,那么在templates文件夹下需要新建question文件夹,里面存放query.html,否则会报错.因此跳转路径与资源文件要一一对应(3)跳转页面的路径设置问题: html页面最好都在templates文件夹下,以templates文件夹为根目录(系统默认的),路径为/,具体设置如下:(4)静态资源文件添加方式 最好直接在资源管理器替换,不在IDEA里面替换,容易出问题,因为IDEA可能还会改动原有的路径 3.2.4 request.getparameter()获取不到值 request.getparameter 获取不到值可能是form表单中的action的路径问题(本项目由于在IDEA进行静态资源文件添加,导致原有的路径改变,因此出错),或者是传递的内容格式问题. 四.注意事项4.1 Controller排错三部曲: 第一步:不用输入,直接返回字符串;–看是否连接 第二步:不用输入参数,自己先将参数变为变量,然后对其进行处理看下,是否存在问题.–测试数据的处理方法是否存在问题. 第三步:设置方法参数,看下结果 4.2 Springboot错误详情页: ❋Required String parameter &#39;question&#39; is not present–地址栏缺少参数,或者跳转路径错误 ❋No enable message–服务器内部查找方法错误,导致查找失败,显示没有可用的信息. 4.3 开发技巧4.3.1 类导入原则: 一般情况下:不同包的类才需要专门注明,在同一文件夹下的各类,不用相互导入–因为大家的直接父类相同. 4.3.2 全局变量的导入方法 import static com.example.demo.classification.modelIndex 4.3.3 方法调用(1)如果接口本身有具体实现方法，可以直接:12@AutowiredPersonRepository personRepository; 在springboot当中,一般接口的调用会在类中,不一定是该接口的实现类.(2)如果是调用接口本身无具体实现方法:12接口名 接口对象名=new 实现类名()Interface inter = new InterfaceImpl() 4.3.4 List列表输出list类型可以直接用System.out.println()方法输出;[XXX,XXY,XXX]","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"交互","slug":"NLP/交互","permalink":"http://yoursite.com/categories/NLP/交互/"}],"tags":[{"name":"restful","slug":"restful","permalink":"http://yoursite.com/tags/restful/"},{"name":"可视化界面","slug":"可视化界面","permalink":"http://yoursite.com/tags/可视化界面/"}]},{"title":"抽象类,接口,普通类","slug":"抽象类,接口,普通类","date":"2018-10-15T16:25:11.000Z","updated":"2018-11-13T02:55:22.000Z","comments":true,"path":"2018/10/16/抽象类,接口,普通类/","link":"","permalink":"http://yoursite.com/2018/10/16/抽象类,接口,普通类/","excerpt":"抽象类与接口的关系,之前接触过,但比较模糊,特此总结下,接口侧重功能实现,但是抽象类侧重从属关系,其实抽象类就是接口与普通类的混合版,架起了接口与普通类的桥梁,因为类继承抽象类，抽象类可以实现多个接口．","text":"抽象类与接口的关系,之前接触过,但比较模糊,特此总结下,接口侧重功能实现,但是抽象类侧重从属关系,其实抽象类就是接口与普通类的混合版,架起了接口与普通类的桥梁,因为类继承抽象类，抽象类可以实现多个接口． 一.相同点 二.不同点 三.联系 一.相同点 都有抽象方法 都可以被继承 都不能被实例化 继承的子类都必须实现抽象类或者接口定义的抽象方法 二.不同点 抽象类 接口 继承方式 extend implement 一个子类(实现类)继承个数 只能继承一个抽象类 可以实现多个接口 侧重点 侧重所属关系 侧重特定功能的实现 非抽象方法 可以有 只能为抽象方法 方法的权限 都可以 只能声明public,方法是public abstract类型 继承要求 抽象类中的非抽象的方法可以选择继承 ,抽象方法则是全部要实现(非抽象的方法–&gt;继承;抽象方法–&gt;实现) 全部要实现,因为都是公有的抽象方法 变量/常量 都可以 只能静态常量(不常用) 使用场合 需要统一的接口,有需要使用变量,区分不同的子类,协调子类, 总之:既需要统一的接口，又需要实例变量或缺省的方法的情况 多种特定功能的实现,但是, 纯粹的接口不能满足类与类之间的协调,有时需要变量进行协调. 说明： (1)类与接口的关系:实现; (2)类与类之间:继承 (3)接口与接口关系:继承 三.联系三者关系如下图:接口可以继承多个接口,但接口实现类必须实现所有接口的方法,这个要求间接弥补了java单继承的遗憾.但日常使用中,接口却用的更多.","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"知识点","slug":"java/知识点","permalink":"http://yoursite.com/categories/java/知识点/"}],"tags":[{"name":"接口","slug":"接口","permalink":"http://yoursite.com/tags/接口/"},{"name":"抽象类","slug":"抽象类","permalink":"http://yoursite.com/tags/抽象类/"}]},{"title":"问句还原(四)","slug":"问句还原(四)","date":"2018-10-15T14:43:50.000Z","updated":"2018-11-13T03:10:32.000Z","comments":true,"path":"2018/10/15/问句还原(四)/","link":"","permalink":"http://yoursite.com/2018/10/15/问句还原(四)/","excerpt":"问句还原是完成语义解析的重要一步,目前我们可以通过Map对象暂存问句的单词与自定义词性,之后根据问句分类的结果,得到结构化语句,之后读取Map对象,之后实行替换,最后实现结构化问句的还原.","text":"问句还原是完成语义解析的重要一步,目前我们可以通过Map对象暂存问句的单词与自定义词性,之后根据问句分类的结果,得到结构化语句,之后读取Map对象,之后实行替换,最后实现结构化问句的还原. 一.原因 二.实现过程 三注意事项 3.1 全局变量与数据共享 3.2 全局变量与局部变量 3.3 for循环 3.4 权限修饰符 3.5 测试技巧 3.5.1 测试运行当前程序: 3.5.2 测试一个方法: 一.原因 主要是为了方便之后的可视化查询.其过程:将原来被标注词性替换的单词还原.从这点,我们就可以推出:之前初步阶段的问句抽象,主要是为了减少相似问句的差异性,降低分类的难度.因为相同词性但是内容不同的的单词,若单纯比较问句的内容,不作处理的话,导致问句的分类准确性会大大降低.因此,后期的精度的提升,词性标注这块也是重点.例子: 小明的年龄是多少;小花的年龄是多少,词性标注后都是:name1的年龄是多少,其中name1是自定义标注.说明相似度100%,效果显而易见. 二.实现过程 说明下:(重点) (1)自定义词典里面的词性如(nm)是用来分词的; (2)自定义词性是为了抽象问句,便于问句的分类(再次抽象)和还原.–存放在内存之中(目前). 主要是将自定义词典的单词替换回问句的词性(自定义的)当中,自定义词典的单词和自定义词性,之前自定义词典的加载,已经说明了,是存放在Map&lt;String,String&gt;中,实现过程如下:12345678910111213141516171819202122232425262728293031public class queryExtension &#123; public String queryExtenstion(String question) throws Exception &#123; /** * 得到问句分类结果:索引编号为0,结构化语句为: name1 年龄. */ String queryPattern=new classification().queryClassify(question); //读取存放单词与自定义词性的Map对象--abstractMap, Set&lt;String&gt; set = abstractMap.keySet(); for (String key : set) &#123; /** * 如果句子模板中含有抽象的词性 */ if (queryPattern.contains(key)) &#123; /** * 则替换抽象词性为具体的值 */ String value = (String) abstractMap.get(key); queryPattern = queryPattern.replace(key, value); &#125; &#125; //得到还原问句; String extendedQuery = queryPattern; /** * 当前句子处理完，抽象map清空释放空间并置空，等待下一个句子的处理 */ abstractMap.clear(); abstractMap = null; System.out.println(\"句子还原为:\"+extendedQuery); return extendedQuery; &#125;&#125; 思路如下: (1)得到分类后的结构化语句–queryPattern (2)读取存放单词与自定义词性的Map对象(要使用全局变量,就是定义出加上static)–abstractMap (3)读取分类后的结构化语句–queryPattern并进行替换,将替换后的结果放在extendedQuery. (4)最后需要对原先词典进行清空,要不然会出现张冠李戴.–重点结果如下图: 三注意事项3.1 全局变量与数据共享 abstractMap是一个存储单词与自定义词性的Map对象,但是只存在于segment.java之中,但是在queryExtension.java之中需要调用该变量,为此,为了实现数据共享,我们可以设置全局变量来进行实现,同时该变量要为segment类的成员变量,合起来就是静态成员变量.具体设置见如下代码:123456public class segment &#123; //static+普通成员变量=静态成员变量=全局变量 static HashMap abstractMap = new HashMap&lt;String, String&gt;(); public String segment(String lineStr) &#123; ........ &#125; 除了在segment.java之中设置,由于有时IDEA会有些迟钝,因此,需要在queryExtension.java之中进行导入:import static com.example.demo.process.segment.abstractMap;,好好理解这个格式. 3.2 全局变量与局部变量 (1)局部变量:不加static的一般都是–为了数据安全 (2)全局变量:static 关键词(同一文件夹下的类间,全局变量是共享的)–为了数据共享 (3)场景:有时候当某个变量的值不能传递过来的时候,可以考虑使用全局变量,如上文的abstractMap变为全局变量. 3.3 for循环for(:)–&gt;这种形式用来处理不知道str长度的情况12345//str是一个集合,word与集合str中单元素,具有相同的数据类型,//所以通过for循环,会把从str集合取出来的中间元素给word. for(String word:str)&#123; lastQuery.add(word); &#125; 3.4 权限修饰符权限范围具体如下图: (1)public只是修饰符代表这个被修饰的类可以被公用， (2)protected修饰代表可以被本包及本包外有继承的类使用， (3)default:省略就是default，代表本包可以使用–如:void answer(){} (4)private代表封装，只有本类可以使用 3.5 测试技巧3.5.1 测试运行当前程序: 在测试中加上@Test,点击行号那栏的绿色三角形的运行按钮: 3.5.2 测试一个方法: (1)返回值设置为void;最后输出用System.out.println()方法输出, (2)方法参数消除,将其设置为方法内部变量;","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"语义解析","slug":"NLP/语义解析","permalink":"http://yoursite.com/categories/NLP/语义解析/"}],"tags":[{"name":"问句还原","slug":"问句还原","permalink":"http://yoursite.com/tags/问句还原/"},{"name":"语义解析","slug":"语义解析","permalink":"http://yoursite.com/tags/语义解析/"}]},{"title":"问句分类(三)","slug":"问句分类(三)","date":"2018-10-14T10:28:26.000Z","updated":"2018-11-13T03:10:02.000Z","comments":true,"path":"2018/10/14/问句分类(三)/","link":"","permalink":"http://yoursite.com/2018/10/14/问句分类(三)/","excerpt":"问句分类是语义解析的一种方式,虽然比较比较简单,但是直接粗暴有效,因此要掌握,问句分类涉及到:问题模版、特征词汇表、自定义词典的设置与加载,分类模型的训练和加载,自定义分词方法等,涉及到的知识点比较多,需要慢慢体会,对问题模版、特征词汇表、自定义词典的作用要有比较深刻的认识.","text":"问句分类是语义解析的一种方式,虽然比较比较简单,但是直接粗暴有效,因此要掌握,问句分类涉及到:问题模版、特征词汇表、自定义词典的设置与加载,分类模型的训练和加载,自定义分词方法等,涉及到的知识点比较多,需要慢慢体会,对问题模版、特征词汇表、自定义词典的作用要有比较深刻的认识. 一.各词典的作用 二.问句分类的流程 三.问句模版 3.1 年龄问题模版 3.2 性别-问题模版 四.分类模型 4.1 训练集的加载与标签设置 4.2 模型的训练 五.问句分类的方法 六.注意事项: 6.1 字符串转整型 6.2 测试类@Test 6.3 int,string,integer转换 6.4 字符串比较 6.5 系统找不到指定的文件 6.6 Map数据类型 一.各词典的作用 (1)自定义词典:是用来分词的; (2)特征词汇表:主要是用来 对分词之后的问句 进行结构化表示;其中特征词汇的选取–应该选取能够表示该问句的核心词/特征词或者独有词.这个是非常重要的 (3)问题模版词典:是用来训练集使用; (4)模版索引词典:主要是用来通过模型训练得到索引编号,之后找到对应的问句模版 二.问句分类的流程 问句分类是对问句进行更高层次的抽象,问句解析的过程如下图: 从上图我们可以得知分类模型主要做的是将抽象后的问句再进行抽象,得到结构化语言,目前的手段是通过分类模型进行训练.问句分类会涉及到:问句的向量化(涉及到特征词汇表的建立),问句模版的建立与加载,分类模型的建立与加载. 三.问句模版 问句模版是作为分类模型的训练集存在的,所以先对问题模版进行建立,为了提高准确性,我们设置设置两个问题模版:年龄-问题模版;与性别-问题模版. 3.1 年龄问题模版截图如下: 同时基于上述问法,我们需要尽可能的将能代表每个问法的关键词或者独有词,设置在特征词汇表vocabulary.txt当中,为的是在同一问题模版下,相似问句对应的向量的距离能够更接近,不同问句距离更远,便于后面分类模型的训练.,具体情况如下图: 3.2 性别-问题模版截图如下:同时也要在特征词汇表vocabulary.txt当中设置,如下图:注意： 上述两个模版处除了在特征词汇表添加单词,还需要添加自定义词典.因为有时候可能分词器借助系统预置的词典,可能会带来分词不准确,这就会导致问句向量也会出现差异,解决方法:可以自己在custom文件夹,添加若干个不同词性的词典.–根据词性设置词典. 四.分类模型 既然要进行问句分类,就需要对分类模型进行建立,当前项目所采用的是朴素贝叶斯分类模型,使用Spark的机器学习工具库进行建立.以问句模版作为训练集. 4.1 训练集的加载与标签设置 标签与问题模版-索引编号文件(question_classification.txt)是一一对应的,在设置标签之前,我们还需要对问句进行向量化,之后我们根据模版的个数设定标签的个数–分类的依据,为此,两个类别设置两个标签.我们设置的标签分别是:0.0和1.0,标签的设置如下:12345678910111213141516String scoreQuestions = loadFile(\"/【0】年龄.txt\"); /** * 读取后的问题字符串进行拆分抓取.自己还可以使用另一个问句模版.作为另一个标签的训练集 */ sentences = scoreQuestions.split(\"`\"); for (String sentence : sentences) &#123; double[] array = new sentenceToArrays().sentenceToArrays(sentence); /** * 为向量化的问句添加标签--就得到LabeledPoint */ LabeledPoint train_one = new LabeledPoint(0.0, Vectors.dense(array)); /** * 添加到训练集当中 */ train_list.add(train_one); &#125; 4.2 模型的训练12345JavaRDD&lt;LabeledPoint&gt; trainingRDD = sc.parallelize(train_list);//因为javaRDD是中转格式,需要最后转换成RDD格式,便于最大程度的发挥spark的性能. NaiveBayesModel nb_model = NaiveBayes.train(trainingRDD.rdd()); //资源关闭 sc.close(); 五.问句分类的方法 通过上述步骤,我们已经得到了训练好的模型,为此需要将上述步骤通过设置一个方法进行综合:1234567891011121314151617181920212223242526272829303132public class classificationTest &#123; /** * 利用加载的模型 * 就是将分词后的问句,转换成向量(根据特征词汇表),然后进行模型训练, * 得到index,然后使用K-V数据类型的get方法,得到Value值 ❋ * 贝叶斯分类器分类的结果，拿到匹配的分类标签号，并根据标签号返回问题的模板 */ @Test public void queryClassify() throws Exception &#123; String sentence=new segment().segment(\"zzxp的年龄是多少\"); /** * 调用问句向量 */ double[] testArray = new sentenceToArrays().sentenceToArrays(sentence); Vector v = Vectors.dense(testArray); /** * 对数据进行预测predict * 句子模板在 spark贝叶斯分类器中的索引【位置】 * 根据词汇使用的频率推断出句子对应哪一个模板 */ NaiveBayesModel nbModel=new loadClassifyModel().loadClassifierModel(); double index = nbModel.predict(v); //因为标签的数值类型是double型,索引编号是整型,因此需要转换 int modelIndex = ((int)index); System.out.println(\"分词结果:\" + sentence); System.out.println(\"问句向量化结果:\"+v); System.out.println(\"the model index is \" + modelIndex); //问题模版-索引编号在读取后存入Map类型当中,使用loadQuestionsPattern()实现 Map&lt;Integer, String&gt; modelPattern=new loadQuestionPattern().loadQuestionsPattern(); System.out.println(\"对应的问句模版是:\"+modelPattern.get(modelIndex)); &#125;&#125; 从上述代码可以得知,调用了: (1)自定义的分词方法segment() (2)问句向量化方法:sentenceToArrays() (3)上述–两个方法还包含词典加载方法. (4)模型训练方法:loadClassifierModel()–该方法包含问题模版加载的方法 (5)问题模版与索引文件的加载方法:loadQuestionsPattern()–实现将编号与问题模版的结构化语句存放到Map当中.运行上述代码之后,结果如下图: 六.注意事项:6.1 字符串转整型在loadQuestionsPattern()方法中:123456while ((line = br.readLine()) != null) &#123; String[] tokens = line.split(\":\"); Integer index = StringToInt(tokens[0]); String pattern = tokens[1]; questionsPattern.put(index, pattern); &#125; 在这句:Integer index = Integer.ParseInt(tokens[0]);出现了:单单仅在进行第一轮循环的时候: For input String:”0”的问题,弄了好久,到现在还没有搞清楚,在进行代码调试的时候,Integer index = Integer.ParseInt(&quot;0&quot;)却可以实现解析,为此我专门自定义一个方法进行补救,方法如下:12345678910111213public static int StringToInt(String s) &#123; try &#123; Optional&lt;Integer&gt; STI = Optional.of(Integer.parseInt(s)); /** * integer转换为int类型 */ int sti = STI.get().intValue(); return sti; &#125; catch (NumberFormatException e) &#123; //出现 异常就返回0, return 0; &#125; &#125; 虽然没有得到较好的解决,但也基本实现了问句的分类功能,为之后的问句扩展奠定了基础,这个问题,以后多留心,争取早日解决.tokens[0]同时要保证只含有数字的字符串,可以通过调试进行实现. 6.2 测试类@Test两个要求: (1)方法没有参数; (2)方法没有返回值,最后结果都打印在屏幕上,通过System,out.println(结果)实现.进行代码运行的时候出现java.lang.Exception: No runnable methods－－加上＠Test java.lang.Exception: Method 方法名() should be void--@Test标注的方法,没有返回值和内参–因为不需要交互(不需要内参即用户不必输入参数)并且直接运行输出(不需要返回值). (3)不能在测试注解的下面添加类的属性,即使上面添加了也没有用,最好是在方法里面添加 6.3 int,string,integer转换转换方法见下图: Integer是int的包装类,此包装类主要是为了实现各种数据类型之间的转换.实现对各种数据类型的对象的操作(方便类型转换,操作对象–有各种方法如Integer.ParseInt()). 开头大写字母的是包装类,小写的则是基本类型 6.4 字符串比较 ==:地址比较–&gt;两个变量是否引用同一个地址的对象; s1.equals(s2):则是对内容的比较 6.5 系统找不到指定的文件 java.io.FileNotFoundException: D:\\java\\HANLP\\1.6.8\\data\\question\\【1】年龄.txt(系统找不到指定的文件。) 以后遇到这种问题,把相应地址复制到资源管理器的地址栏上,看下是否可以访问 6.6 Map数据类型 泛型（比如：Map&lt;K, V&gt;）中继承过来的数据，K可能是String(95%以上)、Integer等等。只要唯一即可,自定义类型即可,如果需要map.get(key)，得先确保key的类型跟map的K匹配. Map集合存储数据的主要目的是为了查找 而List集合是为了输出.","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"语义解析","slug":"NLP/语义解析","permalink":"http://yoursite.com/categories/NLP/语义解析/"}],"tags":[{"name":"语义解析","slug":"语义解析","permalink":"http://yoursite.com/tags/语义解析/"},{"name":"问句分类","slug":"问句分类","permalink":"http://yoursite.com/tags/问句分类/"}]},{"title":"问句向量化(二)","slug":"问句向量化(二)","date":"2018-10-12T16:12:29.000Z","updated":"2018-11-13T03:11:06.000Z","comments":true,"path":"2018/10/13/问句向量化(二)/","link":"","permalink":"http://yoursite.com/2018/10/13/问句向量化(二)/","excerpt":"问句向量化,主要是为了方便比较问句之间的相似度(数字化之后便于计算),挑选最大相似度的问句,从而得到该问句所属的问句模版.因此词汇表的单词制定要全部根据问题模版内容的单词来定制,因为最后把是把问句映射到词汇表的空间中–问句中的单词对应词汇表空间中单词对应的位置,存在为1,不存在为0.最后在词汇表空间中就会形成很多问句向量.(好好屡屡)","text":"问句向量化,主要是为了方便比较问句之间的相似度(数字化之后便于计算),挑选最大相似度的问句,从而得到该问句所属的问句模版.因此词汇表的单词制定要全部根据问题模版内容的单词来定制,因为最后把是把问句映射到词汇表的空间中–问句中的单词对应词汇表空间中单词对应的位置,存在为1,不存在为0.最后在词汇表空间中就会形成很多问句向量.(好好屡屡) 一.词汇表 1.1 设置词汇表 1.2 设置加载词汇的方法 二.设置问句向量化方法 三.注意事项: 3.1 断点调试 3.2 字符串转换成整数 一.词汇表1.1 设置词汇表 词汇表是问句向量化的重要工具,问句在进行分词之后,通过使用词汇表与分词结果进行一一对应,如果匹配,则将该单词的值变为1,否则变为0.词汇表的索引编号,最好从0开头,同时与之后数组向量的索引编号一一对应,便于开发.如下图: 数组向量的索引编号如下:1234567/** * 模板对照词汇表的大小进行初始化，全部为0.0 * 其实i的值最好与词汇表当中的索引是相一致的 */ for (int i = 0; i &lt; vocabulary.size(); i++) &#123; vector[i] = 0; &#125; 1.2 设置加载词汇的方法 既然要借助特征词汇表实现问句向量化,那么就需要先设置读取方法来读取特征词汇表.具体实现情况如下:1234567891011121314151617181920212223242526272829303132333435363738394041/** * 加载特征词汇 * @return */ public Map&lt;String, Integer&gt; loadVocabulary() &#123; Map&lt;String, Integer&gt; vocabulary = new HashMap&lt;String, Integer&gt;(); File file = new File(rootDirPath+\"vocabulary.txt\"); BufferedReader br = null; try &#123; br = new BufferedReader(new FileReader(file)); /** * 其实抛异常就是针对当前步骤可能会出现的情况 */ &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; String line; try &#123; while ((line = br.readLine()) != null) &#123; String[] tokens = line.split(\":\"); String word = tokens[1]; /** * 以后对于字符串转整型就用自定义方法，会尽量少出现异常。 */ int index=StringToInt(tokens[0]); vocabulary.put(word, index); &#125; &#125; /** * 下面这个几个catch就是获取抛出的异常. */ catch (NumberFormatException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return vocabulary; &#125; 二.设置问句向量化方法 既然已经读取了特征词汇表,那么需要设置如何实现将问句映射在词汇表的空间中,也就是问句向量化的过程.具体实现情况如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 问句向量化测试 */public class sentenceToArraysTest &#123; /** * 主要是为了后期，修改成本，和减少设置词汇表地址的错误 */ String rootDirPath=\"D:/java/HANLP/1.6.8/data/question/\"; @Test /** * 问句向量化测试 */ public void sentenceToArrays() throws Exception &#123; Map&lt;String, Integer&gt; vocabulary=loadVocabulary(); String sentence=\"今天天气很不错 ,我正好需要散散心\"; double[] vector = new double[vocabulary.size()]; /** * 模板对照词汇表的大小进行初始化，全部为0.0 * 其实i的值最好与词汇表当中的索引是相一致的 */ for (int i = 0; i &lt; vocabulary.size(); i++) &#123; vector[i] = 0; &#125; /** * HanLP分词，拿分词的结果和词汇表里面的关键特征进行匹配 */ Segment segment = HanLP.newSegment(); List&lt;Term&gt; terms = segment.seg(sentence); for (Term term : terms) &#123; String word = term.word; /** * 如果命中，0.0 改为 1.0 */ if (vocabulary.containsKey(word)) &#123; /** * 这个才是重点 */ int index = vocabulary.get(word); vector[index] = 1; &#125; &#125; System.out.println(Arrays.toString(vector)); &#125; 三.注意事项:3.1 断点调试 (1)断点无效解决:可能调试的程序太多了和清除缓存,rebuild project (2)调试：绿色是变量，灰色是上一步的结果，想看那行结果就调试哪行.如图: (3)调试快捷键: F8:函数内部的下一步;(更多的是展现每行的结果) F7:实际运行流程的下一步(调用了哪些方法–往往是自定义方法,不是系统内置的方法,何时调用的,这个比较详细) shift+F7:进入本行所涉及的系统方法 shift+F8:跳出系统内置方法–&gt;常在使用shift+F7之后,跳出内置方法 F9:运行下一个断点 ctrl+F8:设置/取消断点 ctrl+shift+F8:查看断点,点击more设置界面(或者再次ctrl+shift+F8),具体如下图: 3.2 字符串转换成整数 由于integer.parseint(String s,[进制])–默认是10进制,容易导致异常–NumberFormatException--For input String:&quot;numbers&quot;,同时导致这种异常又有三种情况: (1)字符串留有空格–字符串加上trim()–消除空格与换行符号,代码如下:integer.parseint(s.trim()) (2)传过来的参数是空值; (3)数据格式转换错误–最难排除,为了降低成本,统一使用如下方法:123456789101112131415161718/** * 使用该方法，字符串变整型，不易发生异常 * @param s * @return */ public static int StringToInt(String s)&#123; try &#123; Optional&lt;Integer&gt; STI= Optional.of(Integer.parseInt(s)); /** * integer转换为int类型，使用intValue(); */ int sti=STI.get().intValue(); return sti; &#125; catch (NumberFormatException e) &#123; return 9999; &#125; &#125; 其中Optional类是一个可以为null的容器对象，能够解决NullPointException，同时也可以装载很多数据类型，是一个很强大的类型．详情参考jdk8 Optional 的正确姿势．","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"词性标注","slug":"NLP/词性标注","permalink":"http://yoursite.com/categories/NLP/词性标注/"}],"tags":[{"name":"问句向量","slug":"问句向量","permalink":"http://yoursite.com/tags/问句向量/"},{"name":"词汇表","slug":"词汇表","permalink":"http://yoursite.com/tags/词汇表/"}]},{"title":"自定义词典及其词性设置(一)","slug":"自定义词典及其词性设置(一)","date":"2018-10-11T15:37:31.000Z","updated":"2018-11-13T03:16:46.000Z","comments":true,"path":"2018/10/11/自定义词典及其词性设置(一)/","link":"","permalink":"http://yoursite.com/2018/10/11/自定义词典及其词性设置(一)/","excerpt":"本文以HANLP分词器为例,通过自定义词典,实现词性的自定义和问句的抽象,有利于方便后面的问题模版的内容设置.其实问答系统的流程就是:小明的联系方式–&gt;nm的联系方式–&gt;nm 手机号码–&gt;小明 手机号码–&gt;查询.问答系统的核心在于问句解析,问句解析的核心:问题的抽象化.","text":"本文以HANLP分词器为例,通过自定义词典,实现词性的自定义和问句的抽象,有利于方便后面的问题模版的内容设置.其实问答系统的流程就是:小明的联系方式–&gt;nm的联系方式–&gt;nm 手机号码–&gt;小明 手机号码–&gt;查询.问答系统的核心在于问句解析,问句解析的核心:问题的抽象化. 1.自定义词典添加目录 最好新建一个自定义词典在data\\dictionary\\custom目录下,最好是英文名,格式为txt,最好是相同词性放在同一个词典之中. 2.词性设置: (1)可以直接在词典里面设置,格式为:单词 词性(可以自定义) 词频 (2)可以在hanlp.properties中的自定义词典配置项的中实现动态添加.例如 1CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; 现代汉语补充词库.txt; 全国地名大全.txt ns; 人名词典.txt; 机构名词典.txt; 上海地名.txt ns;data/dictionary/person/nrf.txt nrf; 因为在hanlp.properties中已经设置好了配置文件根目录:root=D:/java/HANLP/1.6.8,没有/,记住. 因此,在CustomDictionaryPath项中,设置路径为相对路径即可,如果后面添加的文件是与前一个词典是同以目录下,则后面需要;,也就是空一格,否则重新编写路径,如”data/dictionary/person/nrf.txt”.其中全国地名大全.txt ns表示全国地名大全这个词典里面的单词的词性都是ns,实现了动态词性的设置,方便.因此可以不用事先静态设置词性. 3.注意事项: (1)最好不要在自定义词典:CustomDictionary.txt中添加单词及其词性、词频,修改词性或者 词频倒是可以. (2)如果修改了CustomDictionary.txt,最好需要把CustomDictionary.txt.bin删除(也就是缓存删除) (3)直接使用customDictionary.add(“现在”,”n 1”),这是通过编程动态添加,自定义词典会加载到内存中,虽然也会把上述单词设置成自定义词典的一部分,但是最终不会保存在硬盘中. 4.运行结果:运行代码为:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091/** * 分词测试+自定义词性标注 */public class segmentTest &#123; @Test public void TestA()&#123; String lineStr = \"手机号码,年龄,性别,zxp和zzxp明天19岁了.\"; try&#123; /** * 开启分词器 */ Segment segment = HanLP.newSegment(); /** * 同时添加引入自定义词典进行分词 */ segment.enableCustomDictionary(true); /** * 分词结果为Term类型的对象,但有多个,因此由List进行接收. */ List&lt;Term&gt; seg = segment.seg(lineStr); /** * 为问句初次抽象做准备.根据词性来 */ String abstractQuery = \"\"; /** * 为后期已经结构化的抽象问句还原做准备 */ HashMap abstractMap = new HashMap&lt;String, String&gt;(); int nrCount = 0; //nr 人名词性这个 词语出现的频率 for (Term term : seg) &#123; /** * 只取出取出单词,为后面加入K-v结构.从而实现还原做准备 */ String word= term.word; String termStr=term.toString(); /** *为了输出美观 */ System.out.print(\" \"); System.out.print(termStr);// System.out.println(word); /** * 重点介绍:对分词之后的每个单词进行词性查询, * 其中词性ph则是人工自定义的,通过在自定义文件夹下自定义词典phone.txt, * 因为hanlp.properties已经配置好了自定义词典的目录.所以会自动导入. * 解释:如果某个单词存在ph这个词性,则在抽象问句中国将该单词变为phone, * 这样做的,其实更深层次的目的就是为了后面问题模版的构造,因为该问题模版的内容为: * \"phone是哪里的\"---对应结构化语句--\"phone 地址\",便于后期的模型训练 */ if (termStr.contains(\"ph\")) &#123; /** * 自定义的词典 */ abstractQuery += \"phone \"; abstractMap.put(\"phone\", word); /** * nrCount主要为了区别不同的人名,之后也是便于编写问题模版, * 从而有利于问题分类模型的训练 */ &#125; else if (termStr.contains(\"nm\") &amp;&amp; nrCount == 0) &#123; //nr 人名 abstractQuery += \"name1 \"; abstractMap.put(\"name1\", word); nrCount++; &#125;else if (termStr.contains(\"nm\") &amp;&amp; nrCount == 1) &#123; abstractQuery += \"name2 \"; abstractMap.put(\"name2\", word); nrCount++; &#125;else if (termStr.contains(\"se\")) &#123; abstractQuery += \"sex \"; abstractMap.put(\"sex\", word); &#125; else &#123; /** * 不存在上述词性,则单词添加空格输出. */ abstractQuery += word + \" \"; &#125; &#125; System.out.println('\\n'); System.out.println(abstractQuery); &#125; /** * 捕捉到一个异常 */ catch(Exception ex)&#123; System.out.println(ex.getClass()+\",\"+ex.getMessage()); &#125; &#125;&#125; 运行结果:","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"词性标注","slug":"NLP/词性标注","permalink":"http://yoursite.com/categories/NLP/词性标注/"}],"tags":[{"name":"自定义词典","slug":"自定义词典","permalink":"http://yoursite.com/tags/自定义词典/"},{"name":"HANLP","slug":"HANLP","permalink":"http://yoursite.com/tags/HANLP/"}]},{"title":"三层架构与mvc","slug":"三层架构与mvc","date":"2018-10-11T05:30:27.000Z","updated":"2018-11-13T03:07:22.000Z","comments":true,"path":"2018/10/11/三层架构与mvc/","link":"","permalink":"http://yoursite.com/2018/10/11/三层架构与mvc/","excerpt":"一般项目开发会遵循三层架构:数据层,业务层,表示层,其中:表示层是与前台进行连接;业务层是实业务的操作逻辑;数据层/DAO层则是对数据的操作-CRUD.各层要尽量做到低耦合,减少层之间的数据传递.最后还重点讲述了MVC与三层结构的区别.","text":"一般项目开发会遵循三层架构:数据层,业务层,表示层,其中:表示层是与前台进行连接;业务层是实业务的操作逻辑;数据层/DAO层则是对数据的操作-CRUD.各层要尽量做到低耦合,减少层之间的数据传递.最后还重点讲述了MVC与三层结构的区别. 一.数据层 1.1 model层 1.2 Dao层 二.service/业务层 三.action/界面层 3.1 业务的管理、调度 3.2 页面跳转 3.3 页面传值 四.三层架构与MVC 4.1 不同点 4.2 相同点及其联系: 4.3 MVC模式 五.参考文献 java三层架构: 界面层(UI层/action层),业务层(service层),数据层(model层+数据访问层),说明：model层是描述的是对应数据表的实体类。具体来看下图:注: ❋ PO 持久对象，数据； ❋ BO 业务对象，封装对象、复杂对象 ，里面可能包含多个类； ❋ DTO 传输对象，前端调用时传输 ； ❋ VO 表现对象，前端界面展示。 上述四者都是POJO对象. ❋ DAO:数据访问对象,进行数据库增删改查的。 一.数据层1.1 model层 modle层就是对应的数据库表的实体类(如User类)。 1.2 Dao层 Dao层，一般可以再分为Dao接口和DaoImpl实现类，如userDao接口和userDaoImpl实现类,接口负责定义数据库curd的操作方法，实现类负责具体的实现，即实现Dao接口定义的方法。 二.service/业务层 作用：业务的具体实现。 调用dao从数据库取需要的数据，然后加工处理成需要的格式(BO对象)，在这里可以编写自己需要的代码（比如简单的判断），也可以再细分为Service接口和ServiceImpl实现类。 三.action/界面层 三大功能：业务管理、调度；页面和程序之间传输数据；页面跳转. 3.1 业务的管理、调度 根据前后台传递过来的值,判断调用什么业务逻辑方法，然后把业务逻辑方法的返回值再传递给用户，你就可以把他当作一个快递分检站 . 3.2 页面跳转 有时候可以通过返回值的形式,跳转到指定的页面， 3.3 页面传值 当然也能接受页面传递的请求数据，也可以做些计算处理、前端输入合法性检验(前端可修改网页绕过前端合法性检验，需在后台加一层)。 引申到项目结构则可以表示为:123456789xxx:代表公司名称yyy：代表项目名称com.xxx.yyy.dao dao层接口com.xxx.yyy.dao.impl dao层实现com.xxx.yyy.service service层接口com.xxx.yyy.service.impl service层实现--业务具体实现 com.xxx.yyy.action web层--业务调度,页面传值,页面跳转com.xxx.yyy.util 工具包com.xxx.yyy.domain javabean 四.三层架构与MVC4.1 不同点 三层架构 MVC 性质 设计思想(适用广) 设计模式,给出了较具体的方法 划分标准 基于业务逻辑划分 基于页面 组成部分 界面层、业务层、数据层 模型M、视图V、控制器C Model的作用 对应数据表的实体类 业务逻辑与访问数据 适用范围 B/S居多 C/S居多 4.2 相同点及其联系: 都是侧重于高内聚,低耦合. 可以理解MVC模式为三层架构思想的细化与升华 4.3 MVC模式​ View提交请求数据给Controller，Controller返回数据用于渲染View，两者之间以Model(VO- ViewModel)的形式进行通信.示意图如下: 五.参考文献 ❋ javaWeb之三层架构 ❋ Web项目的三层架构和MVC架构异同 ❋ 三层架构，MVC，与ssm的系统架构关系","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/categories/设计模式/"}],"tags":[{"name":"三层架构","slug":"三层架构","permalink":"http://yoursite.com/tags/三层架构/"},{"name":"MVC","slug":"MVC","permalink":"http://yoursite.com/tags/MVC/"}]},{"title":"UML时序图初识","slug":"UML时序图","date":"2018-10-11T02:07:23.000Z","updated":"2018-11-13T02:53:30.000Z","comments":true,"path":"2018/10/11/UML时序图/","link":"","permalink":"http://yoursite.com/2018/10/11/UML时序图/","excerpt":"时序图作为UML建模的重要表现形式,主要是以时间轴的形式来进行,涉及到的基本元素较多,为此总结时序图的一些基本知识,便于日后快速上手.","text":"时序图作为UML建模的重要表现形式,主要是以时间轴的形式来进行,涉及到的基本元素较多,为此总结时序图的一些基本知识,便于日后快速上手. 1.概念 2.基本元素 3.参考文献 1.概念 序列图(时序图)主要用来更直观的表现各个对象交互的时间顺序，将体现的重点放在 以时间为参照，各个对象发送、接收消息，处理消息，返回消息的 时间流程顺序，也称为时序图。 2.基本元素 角色：人，系统|子系统 对象：接收|发送的主体 激活：竖立的长方形 生命线：虚线（与激活常在一起） 消息： 同步消息：发送人需要等待消息的响应。实心箭头表示。 异步消息：不需要等待消息的响应，线性箭头：——&gt; 返回消息：返回的消息，虚线线性箭头：–&gt; 自内联消息：类似 3.参考文献 UML序列图——时序图基本使用","categories":[{"name":"UML","slug":"UML","permalink":"http://yoursite.com/categories/UML/"},{"name":"时序图","slug":"UML/时序图","permalink":"http://yoursite.com/categories/UML/时序图/"}],"tags":[{"name":"UML","slug":"UML","permalink":"http://yoursite.com/tags/UML/"},{"name":"时序图","slug":"时序图","permalink":"http://yoursite.com/tags/时序图/"}]},{"title":"eclipse+maven环境配置","slug":"eclipse+maven环境配置","date":"2018-10-11T01:54:29.000Z","updated":"2018-11-13T02:19:52.000Z","comments":true,"path":"2018/10/11/eclipse+maven环境配置/","link":"","permalink":"http://yoursite.com/2018/10/11/eclipse+maven环境配置/","excerpt":"本文主要讲的是maven在IDE中的配置,本文义eclipse为例,进行说明,主要有四个步骤,为便于回忆,特此总结.","text":"本文主要讲的是maven在IDE中的配置,本文义eclipse为例,进行说明,主要有四个步骤,为便于回忆,特此总结.主要有四个步骤: (1)单独安装maven; (2)maven在eclipse的集成:windows–&gt;&gt;preferences中找到maven选项 (3)maven配置文件的本地仓库修改:就是jar包存放的路径 (4)之后在eclipse中使用自定义的maven配置文件参考文献: maven之一：maven安装和eclipse集成","categories":[{"name":"maven","slug":"maven","permalink":"http://yoursite.com/categories/maven/"},{"name":"配置","slug":"maven/配置","permalink":"http://yoursite.com/categories/maven/配置/"}],"tags":[{"name":"maven","slug":"maven","permalink":"http://yoursite.com/tags/maven/"},{"name":"配置","slug":"配置","permalink":"http://yoursite.com/tags/配置/"}]},{"title":"navicat导出表结构","slug":"navicat导出表结构","date":"2018-10-11T01:46:51.000Z","updated":"2018-11-13T02:38:26.000Z","comments":true,"path":"2018/10/11/navicat导出表结构/","link":"","permalink":"http://yoursite.com/2018/10/11/navicat导出表结构/","excerpt":"本文主要讲述的是使用Navicat导出数据表的方法,之后选择相应的格式进行导出,期间最好先自定义好导出的文件名,节约时间.","text":"本文主要讲述的是使用Navicat导出数据表的方法,之后选择相应的格式进行导出,期间最好先自定义好导出的文件名,节约时间. 新建查询：之后粘贴以下内容：1234567891011121314151617SELECT COLUMN_NAME 列名, COLUMN_TYPE 数据类型, DATA_TYPE 字段类型, CHARACTER_MAXIMUM_LENGTH 长度, IS_NULLABLE 是否为空, COLUMN_DEFAULT 默认值, COLUMN_COMMENT 备注 FROM INFORMATION_SCHEMA.COLUMNSwhere-- developerclub为数据库名称，到时候只需要修改成你要导出表结构的数据库即可table_schema ='lefun'AND-- article为表名，到时候换成你要导出的表的名称-- 如果不写的话，默认会查询出所有表中的数据，这样可能就分不清到底哪些字段是哪张表中的了，所以还是建议写上要导出的名名称table_name = 't_order' 之后运行，得到表结构，之后导出表结果，详情如下：","categories":[{"name":"navicat","slug":"navicat","permalink":"http://yoursite.com/categories/navicat/"},{"name":"导出","slug":"navicat/导出","permalink":"http://yoursite.com/categories/navicat/导出/"}],"tags":[{"name":"navicat","slug":"navicat","permalink":"http://yoursite.com/tags/navicat/"},{"name":"导出","slug":"导出","permalink":"http://yoursite.com/tags/导出/"}]},{"title":"环境变量粗识","slug":"环境变量粗识","date":"2018-10-11T01:37:06.000Z","updated":"2018-11-13T02:59:24.000Z","comments":true,"path":"2018/10/11/环境变量粗识/","link":"","permalink":"http://yoursite.com/2018/10/11/环境变量粗识/","excerpt":"本文主要是介绍环境配置中环境变量的一些基本知识,总的来说,环境变量主要是为了方便开发和减少迁移成本.其中涉及到系统变量、用户变量、环境变量之间的关系,ClassPath、path、JAVA_HOME之间的区别.","text":"本文主要是介绍环境配置中环境变量的一些基本知识,总的来说,环境变量主要是为了方便开发和减少迁移成本.其中涉及到系统变量、用户变量、环境变量之间的关系,ClassPath、path、JAVA_HOME之间的区别. 1.环境变量 2.ClassPath，path，JAVA_HOME 1.环境变量系统变量、用户变量、环境变量环境变量=系统变量+用户变量系统变量：针对所有用户；–PATH用户变量则是针对当前用户；–Path 2.ClassPath，path，JAVA_HOME❋ 三者都是环境变量 path:系统变量，就是将软件所在的安装目录，放到该变量下，在cmd当中就可通过输入该目录下的程序名进行运行。 ClassPath：因为经过javac编译之后，系统不会自动在当前目录下进行扫描class文件，需要自己在ClassPath设定好，之后 javac 相应class文件名 方可启动。 JAVA_HOME：设置好JAVA的安装目录，有时候第三方插件会调用java下的程序，即使以后要改，成本也小很多，相当于是一个自定义变量。","categories":[{"name":"windows","slug":"windows","permalink":"http://yoursite.com/categories/windows/"},{"name":"环境变量","slug":"windows/环境变量","permalink":"http://yoursite.com/categories/windows/环境变量/"}],"tags":[{"name":"windows","slug":"windows","permalink":"http://yoursite.com/tags/windows/"},{"name":"环境变量","slug":"环境变量","permalink":"http://yoursite.com/tags/环境变量/"}]},{"title":"maven与pom初探(一)","slug":"maven与pom初探","date":"2018-10-10T14:33:54.000Z","updated":"2018-11-13T02:35:34.000Z","comments":true,"path":"2018/10/10/maven与pom初探/","link":"","permalink":"http://yoursite.com/2018/10/10/maven与pom初探/","excerpt":"本文主要是对maven与pom进行了初探,maven是项目管理工具,具体实现方式是以pom文件的形式实现的,本文首先讲述了maven的生命周期和命令规范.之后讲述了pom的组成部分.重点讲述了pom的依赖管理、parent与models标签。","text":"本文主要是对maven与pom进行了初探,maven是项目管理工具,具体实现方式是以pom文件的形式实现的,本文首先讲述了maven的生命周期和命令规范.之后讲述了pom的组成部分.重点讲述了pom的依赖管理、parent与models标签。 一.Maven 1.1 MAVEN含义 1.2 maven的项目结构 1.3 MAVEN命令 1.3.1 Maven命令规范 1.3.2 创建Maven项目 (1)cmd交互模式创建 (2)批处理模式创建 (3) IDE工具创建 1.4 MAVEN生命周期lifecycle 1.5 插件 二.POM.xml 2.1 POM的含义 2.2 Pom.xml组成部分 2.2.1 项目的基本信息 2.2.2 repositories 2.2.3 parent与modules (1)relativePath/ 2.2.4 dependencies 2.2.5 dependencyManagement 2.2.6 properties 2.2.7 build 2.2.8 plugin 2.2.9 pluginManagement 一.Maven1.1 MAVEN含义 Maven，发音是meivn,”专家”的意思。它是一个很好的项目管理工具。它负责管理项目开发过程中的几乎所有的东西. Maven的仓库有本地与远程之分: 实际上可将本地仓库理解“缓存”，因为项目首先会从本地仓库中获取 jar 包，当无法获取指定 jar 包的时候，本地仓库会从 远程仓库（或 中央仓库） 中下载 jar 包，并放入本地仓库中以备将来使用。这个远程仓库是 Maven 官方提供的，可通过 http://search.maven.org/ 来访问。 MAVEN的意义: ​ ❋ 统一开发规范与工具(大家都通过配置进行开发,版本规范) ​ ❋ 统一管理 jar 包 1.2 maven的项目结构maven默认的文件存放结构如下：1234567891011121314/项目目录 pom.xml 用于maven项目的配置文件 /src 源代码目录 /src/main 工程源代码目录 /src/main/java 工程java源代码目录 /src/main/resource 工程的资源目录 /webapp 目录下存放 Web 应用相关代码。 ／WEB-INF WEB-INF是Java的WEB应用的安全目录, 访问该里面的网页,必须通过映射才可以访问,不能直接地址栏访问. /src/test 单元测试目录 /src/test/java 测试代码 /src/test/resource 测试资源文件 /target 输出目录，所有的输出物都存放在这个目录下 /target/classes 编译之后的class文件 项目的生命周期的编译阶段:就是将/src/main/java下的java文件,编译成classes文件夹下,输出在target这个输出目录下. 1.3 MAVEN命令1.3.1 Maven命令规范 例如:mvn archetype:generate，mvn tomcat7:run-war 。其实，可使用两种不同的方式来执行 Maven 命令： 方式一：mvn &lt;插件&gt;:&lt;目标&gt; [参数] 方式二：mvn &lt;阶段&gt; 现在我们接触到的都是第一种方式，而第二种方式才是我们日常中使用最频繁的，例如： mvn clean：清空输出目录（即 target 目录） mvn compile：编译源代码 mvn package：生成构件包（一般为 jar 包或 war 包） mvn install：将构件包安装到本地仓库 mvn deploy：将构件包部署到远程仓库 执行 Maven 命令需要注意的是：必须在 Maven 项目的根目录处执行，也就是当前目录下一定存在一个名为 pom.xml 的文件 1.3.2 创建Maven项目(1)cmd交互模式创建 我们不妨创建一个 Java Web 项目，只需在 cmd 中输入： 1mvn archetype:generate 随后 Maven 将下载 Archetype 插件及其所有的依赖插件，这些插件其实都是 jar 包，它们存放在您的 Maven 本地仓库中。 在 cmd 中，您会看到几百个 Archetype（原型），可将它理解为项目模板，您得从中选择一个。 我们的目标是创建 Java Web 项目，所以您可以选择 maven-archetype-webapp（可以在 cmd 中进行模糊搜索），随后 Maven 会与您进行一些对话，Maven 想知道以下信息： 项目 Archetype Version（原型版本号）是什么？—— 可选择 1.0 版本 项目 groupId（组织名） 是什么？—— 可输入 com.smart 项目 artifactId（构件名）是什么？—— 可输入 smart-demo 项目 version（版本号）是什么？—— 可输入 1.0 项目 package（包名）是什么？—— 可输入 com.smart.demo 以上这种方式称为 Interactive Mode**（交互模式）**。 (2)批处理模式创建 如果您是一位高效人士，或许觉得这样的交互过于繁琐，那么您也可以尝试仅使用一条命名，来完成同样的事情： mvn archetype:generate -DinteractiveMode=false -DarchetypeArtifactId=maven-archetype-webapp -DgroupId=com.smart -DartifactId=smart-demo -Dversion=1.0 以上这种方式成为 Batch Mode**（批处理模式）**。 (3) IDE工具创建 当然，还有第三种选择，使用 IDE 来创建 Maven 项目，您可以使用 Eclipse、NetBeans、IDEA 来创建 Maven 项目，操作过程应该是非常简单的 1.4 MAVEN生命周期lifecycle maven把项目的构建划分为不同的生命周期(lifecycle)：编译、运行测试、打包、集成测试(test)、验证、部署。具体来分的有如下９个阶段（phase）． 阶段 插件名称 作用 clean clean 清理自动生成的文件，也就是 target 目录 validate 由 Maven 核心负责 验证 Maven 描述文件(Pom.xml)是否有效 compile compiler、resources 编译 Java 源码 test compiler、surefire、resources 运行测试代码 package war 项目打包，就是生成构件包，也就是打 war 包 verify 由 Maven 核心负责 验证构件包是否有效 install install 将构件包安装到本地仓库 site site 生成项目站点，就是一堆静态网页文件，包括 JavaDoc deploy deploy 将构件包部署/发布到远程仓库 1.5 插件 上节表格中所出现的插件名称实际上是插件的别名（或称为前缀），比如：compiler 实际上是 org.apache.maven.plugins:maven-compiler-plugin:2.3.2，这个才是 Maven 插件的完全名称。 每个插件又包括了一些列的 Goal（目标），以 compiler 插件为例，它包括以下目标： compiler:help：用于显示 compiler 插件的使用帮助。 compiler:compile：用于编译 main 目录下的 Java 代码。 compiler:testCompile：用于编译 test 目录下的 Java 代码。 插件目标才是具体干活的人，一个插件包括了一个多个目标，一个阶段可由零个或多个插件来提供支持。 我们可以在 pom.xml 中定义一些列的项目依赖（构件包），每个构件包都会有一个 Scope（作用域），它表示该构件包在什么时候起作用，包括以下五种： compile：默认作用域，在编译、测试、运行时有效 test：对于测试时有效 runtime：对于测试、运行时有效 provided：对于编译、测试时有效，但在运行时无效 system：与 provided 类似，但依赖于系统资源–表示使用本地的依赖 二.POM.xml2.1 POM的含义 POM（Project Object Model–项目对象模型）是 Maven 工程的工作基础，以 pom.xml 的形式存在于项目中，在这里配置构建工程的详细信息。 pom.xml用于描述整个 Maven 项目，所以也称为 Maven 描述文件 2.2 Pom.xml组成部分2.2.1 项目的基本信息 一个依赖至少包括 groupId、artifactId、version 三个元素，如果该依赖不是 jar 类型，则需要指定 。 1.modelVersion：这个是 POM 的版本号，现在都是 4.0.0 的，必须得有，但不需要修改。下面三个是组织名、构件名、版本号，它们三个合起来就是 Maven 坐标: 2.groupId - 项目组 id–可以代表一个公司,例如–org.apacheartifactId - 项目 id,正在开发中的项目可以用一个特殊的标识，这种标识给版本加上一个”SNAPSHOT”的标记. 3.version - 项目版本,maven在版本管理时候可以使用几个特殊的字符串 SNAPSHOT (开发测试版),LATEST(最新版) ,RELEASE (最新发布版)。 4.scope 作用域限制,LifeCycle(项目构建的生命周期) 5.packaging 项目的打包类型，默认是jar，描述了项目打包后的输出。类型为jar的项目产生一个JAR文件，类型为war的项目产生一个web应用。 6.type:type ，默认是 jar。如果是 war ，则需指定 war ，一般在pom引用依赖时候出现，其他时候不用. 7.optional:是否可选依赖 8.name,url,description 12345&lt;name&gt;banseon-maven&lt;/name&gt;&lt;!-- 哪个网站可以找到这个项目,提示如果 Maven 资源列表没有，可以直接上该网站寻找, Maven 产生的文档用 --&gt;&lt;url&gt;http://www.baidu.com/banseon&lt;/url&gt;&lt;!-- 项目的描述, Maven 产生的文档用 --&gt;&lt;description&gt;A maven project to study maven.&lt;/description&gt; 2.2.2 repositories 在很多情况下，默认的中央仓库无法满足项目的需求，可能项目需要的jar包存在另一个远程仓库中，这时就可以在pom.xml文件中配置仓库，代码如下： 12345678910111213141516&lt;repositories&gt; &lt;repository&gt; &lt;!-- Maven 自带的中央仓库使用的Id为central 如果其他的仓库声明也是用该Id 就会覆盖中央仓库的配置 --&gt; &lt;id&gt;mvnrepository&lt;/id&gt; &lt;name&gt;mvnrepository&lt;/name&gt; &lt;url&gt;http://www.mvnrepository.com/&lt;/url&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt; 2.2.3 parent与modules 由于maven的多模块聚合结构,父模块的&lt;modules&gt;记录着所有的子模块,在每一个子模块中也都标明了自己的父模块的配置信息.例如 qmxbb-platform为父模块 webapps为子模块 父模块的&lt;modules&gt;中记录了子模块的artifactId 子模块中&lt;parent&gt;也记录着父模块的配置信息 子模块中中的中记录的是父项目的pom文件的相对路径(相对于子项目的).这样的逻辑是,在构建子项目的时候首先根据相对路径找到父项目. 示例 ​ 父模块 ​ 子模块 ​ (1)relativePath/1.作用: 指定父工程 pom 文件(相对于子模块/工程的文件路径)的路径 2.默认值: 为../pom.xml3.查找顺序： relativePath元素中的地址–本地仓库–远程仓库 设定一个空值将始终从仓库中获取. 如果项目结构是下面这样的 12345. |-- my-module | `-- pom.xml `-- parent `-- pom.xml12345 则 my-module 工程的 pom 配置如下 12345678910&lt;project&gt; &lt;parent&gt; &lt;groupId&gt;com.mycompany.app&lt;/groupId&gt; &lt;artifactId&gt;my-app&lt;/artifactId&gt; &lt;version&gt;1&lt;/version&gt; &lt;relativePath&gt;../parent/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;my-module&lt;/artifactId&gt;&lt;/project&gt;12345678910 通过 &lt;relativePath&gt; 元素指定父工程 pom 文件的路径 2.2.4 dependencies 内部包含若干 { groupId, artifactId,version 三个基本属性 ​ type :默认jar类型 ​ scope： 当前包依赖类型 ​ optional： false时， 继承这个项目所有的子项目。 true时， 该项目的子项目需要显式引入 ​ exclusion ：排除某项 } 2.2.5 dependencyManagement作用: 方便继承父pom的部分依赖,如果需要继承父pom的全部配置(插件,依赖),则可以不需要. 注意: dependencyManagement只共享dependencyManagement元素里面的依赖. A，B 需要使用 JUnit，C 不需要使用，但我们还是需要在父工程进行统一管理，那我们就需要配置 &lt;dependencyManagement&gt; 进行管理. 先创建一个公有的父pom文件: 注意: 如果一个工程是parent父或者aggregation（muti-module,集成）， 它的packaging 赋值必须为 pom 1234567891011121314151617181920212223242526&lt;project&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;maven&lt;/groupId&gt; &lt;artifactId&gt;parent&lt;/artifactId&gt; &lt;!-- 如果一个工程是parent父或者aggregation（muti-module,集成）， 它的packaging 赋值必须为 pom--&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.test&lt;/groupId&gt; &lt;artifactId&gt;project1&lt;/artifactId&gt; &lt;version&gt;1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt;123456789101112131415161718192021222324252627282930 在&lt;dependencyManagement&gt; 中的依赖，只是进行管理，但并不引入(子模块A，B负责引入)。比如我们工程 A，B 需要使用 JUnit，则配置 123456789101112131415161718192021222324&lt;project&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;maven&lt;/groupId&gt; &lt;artifactId&gt;A(or B)&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;!--这个是重点--&gt; &lt;parent&gt; &lt;groupId&gt;maven&lt;/groupId&gt; &lt;artifactId&gt;parent&lt;/artifactId&gt; &lt;version&gt;1&lt;/version&gt; &lt;!--父pom文件的相对路径--&gt; &lt;relativePath/&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt;123456789101112131415161718 这里不需要为依赖制定版本号，因为已经在父工程指定。 而 C 工程只要不配置 JUnit 依赖，就不会引入 JUnit. 2.2.6 properties声明一些常量。如： ​ &lt;file.encoding&gt;UTF-8&lt;file.encoding&gt; 引用时 ： ${file.encoding} ​ 也可以通过project.xx引用项目的定义的属性，例如： ${project.groupId} 引用当前pom定义的groupId 2.2.7 build build：包括项目的源码目录配置、输出目录配置、插件配置、插件管理配置等 2.2.8 plugin 声明项目中所使用的插件 2.2.9 pluginManagement pluginManagement作用类似于dependencyManagement， 定义子项目中的插件。 这样在子项目中使用插件时，可以不用指定其版本，由父项目统一进行管理.详情插件配置： 参考文献 ❋ Maven POM文件中依赖与插件的配置 ❋ maven POM.xml内的标签大全详解 ❋ maven 工程pom文件详解 ❋ Maven学习– ❋ Maven 那点事儿","categories":[{"name":"maven","slug":"maven","permalink":"http://yoursite.com/categories/maven/"},{"name":"pom","slug":"maven/pom","permalink":"http://yoursite.com/categories/maven/pom/"}],"tags":[{"name":"maven","slug":"maven","permalink":"http://yoursite.com/tags/maven/"},{"name":"pom","slug":"pom","permalink":"http://yoursite.com/tags/pom/"}]},{"title":"pom文件的插件管理","slug":"pom插件管理","date":"2018-10-10T13:32:48.000Z","updated":"2018-11-13T02:47:22.000Z","comments":true,"path":"2018/10/10/pom插件管理/","link":"","permalink":"http://yoursite.com/2018/10/10/pom插件管理/","excerpt":"本文主要讲的是pom文件当中的插件管理,分别从三个方面进行阐述:插件的配置,插件目标的配置,插件的继承配置.初次深入,参考了该文,调理较为清晰,特此转发,谢谢网友的奉献.","text":"本文主要讲的是pom文件当中的插件管理,分别从三个方面进行阐述:插件的配置,插件目标的配置,插件的继承配置.初次深入,参考了该文,调理较为清晰,特此转发,谢谢网友的奉献. 一.插件配置 二.插件目标的配置 三.忽略继承 一.插件配置转自：&lt;https://blog.csdn.net/zsensei/article/details/77624596#t2&gt; mvn的命令:mvn &lt;插件&gt;:&lt;目标&gt; [参数] Maven 所有的工作都是由插件完成的，插件分为两类 Build plugins 在构建项目的时候执行，应该被配置在 &lt;build&gt; 元素中 Reporting plugins 在生成站点的时候执行，应该被配置在 &lt;reporting&gt; 元素中 所有的插件配置要求有三个信息：groupId-有时不写,默认当前项目的插件 ， artifactId ，version ，这跟依赖的配置相似。类似 &lt;dependencyManagement&gt; ， 插件配置也有 &lt;pluginManagement&gt; ，用法也一样，参考依赖管理。 一个普通的配置看起来如下： 123456789101112131415161718192021&lt;project&gt; ... &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-myquery-plugin&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;configuration&gt; &lt;url&gt;http://www.foobar.com/query&lt;/url&gt; &lt;timeout&gt;10&lt;/timeout&gt; &lt;options&gt; &lt;option&gt;one&lt;/option&gt; &lt;option&gt;two&lt;/option&gt; &lt;option&gt;three&lt;/option&gt; &lt;/options&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; ...&lt;/project&gt; &lt;configuration&gt; 里面的元素对应插件目标的参数，如果想知道某个插件目标的可用参数，通常可以通过下面的命令查询 mvn &lt;pluginName&gt;:help -Ddetail -Dgoal=&lt;goalName&gt; 比如想知道 install 插件的 install 目标的参数，则可执行命令 mvn install:help -Ddetail -Dgoal=install 会看到如下的输出 二.插件目标的配置 通常我们需要配置插件目标执行时的参数，看下面的例子 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;project&gt; ... &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-myquery-plugin&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;execution1&lt;/id&gt; &lt;phase&gt;test&lt;/phase&gt; &lt;configuration&gt; &lt;url&gt;http://www.foo.com/query&lt;/url&gt; &lt;timeout&gt;10&lt;/timeout&gt; &lt;options&gt; &lt;option&gt;one&lt;/option&gt; &lt;option&gt;two&lt;/option&gt; &lt;option&gt;three&lt;/option&gt; &lt;/options&gt; &lt;/configuration&gt; &lt;goals&gt; &lt;goal&gt;query&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;execution2&lt;/id&gt; &lt;configuration&gt; &lt;url&gt;http://www.bar.com/query&lt;/url&gt; &lt;timeout&gt;15&lt;/timeout&gt; &lt;options&gt; &lt;option&gt;four&lt;/option&gt; &lt;option&gt;five&lt;/option&gt; &lt;option&gt;six&lt;/option&gt; &lt;/options&gt; &lt;/configuration&gt; &lt;goals&gt; &lt;goal&gt;query&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; ...&lt;/project&gt; 即将 &lt;configuration&gt; 置于 &lt;execution&gt; 标签中，&lt;execution&gt; 标签中通过配置 &lt;phase&gt; 、&lt;goal&gt; 分别指定了配置应用的阶段和目标，如例子中的 id 为 execution1 的配置会应用在 test 阶段中的 query 目标中。我们可以看到 id 为 execution2 的 &lt;execution&gt; 中没有 phase 标签，那么它会在什么时候应用呢？ 如果该目标默认绑定了一个阶段，则在这个阶段应用。 如果该目标没有默认的绑定，则不会应用。 这里的 &lt;id&gt;execution1&lt;/id&gt; 有什么用呢？其实当我们执行一条命令时，像 mvn maven-myquery-plugin:query 这时它会应用什么配置呢？如果在 &lt;executions&gt; 外有配置，则会应用，如果没有，则上面配置的 &lt;execution&gt; 并不会应用上，那么如果我们希望执行上面配置好参数的目标，那么可以加上 id 执行，如 mvn maven-myquery-plugin:query@execution1 执行时就会应用上 execution1 的配置。 三.忽略继承 默认情况下，子工程会继承父工程的插件配置，如果不希望继承，则可配置 &lt;inherited&gt; 标签 123456789101112131415&lt;project&gt; ... &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;inherited&gt;false&lt;/inherited&gt; ... &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; ...&lt;/project&gt;","categories":[{"name":"Maven","slug":"Maven","permalink":"http://yoursite.com/categories/Maven/"},{"name":"Pom","slug":"Maven/Pom","permalink":"http://yoursite.com/categories/Maven/Pom/"}],"tags":[{"name":"Maven","slug":"Maven","permalink":"http://yoursite.com/tags/Maven/"},{"name":"Pom","slug":"Pom","permalink":"http://yoursite.com/tags/Pom/"}]},{"title":"手动添加jar到本地仓库","slug":"手动添加jar包到本地仓库","date":"2018-10-10T04:33:23.000Z","updated":"2018-11-13T03:09:02.000Z","comments":true,"path":"2018/10/10/手动添加jar包到本地仓库/","link":"","permalink":"http://yoursite.com/2018/10/10/手动添加jar包到本地仓库/","excerpt":"进行Pom依赖注入的时候,当出现 Dependency &quot;XXX&quot; not found需要解决一些jar的导入问题,为此总结了手动导入jar的一些方法和技巧","text":"进行Pom依赖注入的时候,当出现 Dependency &quot;XXX&quot; not found需要解决一些jar的导入问题,为此总结了手动导入jar的一些方法和技巧 1.下载jar包 2.Pom注入 3.安装jar文件 4.强制更新 5.额外技巧 1.下载jar包官网进行查找对应的jar包并下载； 2.Pom注入粘贴jar包添加语句到pom文件当中；12345&lt;dependency&gt; &lt;groupId&gt;com.hankcs&lt;/groupId&gt; &lt;artifactId&gt;hanlp&lt;/artifactId&gt; &lt;version&gt;portable-1.6.8&lt;/version&gt; &lt;/dependency&gt; 3.安装jar文件Maven 安装 JAR 包的命令是：mvn install:install-file -Dfile=jar包的位置（任意地方都可以） -DgroupId=上面的groupId -DartifactId=上面的artifactId -Dversion=上面的version -Dpackaging=jar注意: setting.xml文件不要乱添加镜像。具体实例为：1mvn install:install-file -Dfile=D:\\HanLp\\hanlp-portable-1.6.8.jar -DgroupId=com.hankcs -DartifactId=hanlp -Dversion=portable-1.6.8 -Dpackaging=jar 安装结果： 4.强制更新 MAVEN-&gt;reimport 5.额外技巧依赖包找不到（Dependency ‘’ not found）还可能是以下情况： ❋包找不到-手动导入（参考上面）或者更换镜像； ❋没有及时更新：存在缓存，需要消除缓存并重启或者开启强制更新","categories":[{"name":"IDEA","slug":"IDEA","permalink":"http://yoursite.com/categories/IDEA/"},{"name":"配置","slug":"IDEA/配置","permalink":"http://yoursite.com/categories/IDEA/配置/"}],"tags":[{"name":"配置","slug":"配置","permalink":"http://yoursite.com/tags/配置/"},{"name":"jar包","slug":"jar包","permalink":"http://yoursite.com/tags/jar包/"}]},{"title":"可视化查询Neo4j数据库","slug":"可视化查询Neo4j数据库","date":"2018-09-16T04:31:31.000Z","updated":"2018-11-13T03:04:18.000Z","comments":true,"path":"2018/09/16/可视化查询Neo4j数据库/","link":"","permalink":"http://yoursite.com/2018/09/16/可视化查询Neo4j数据库/","excerpt":"本文是基于Spring-boot框架构建的可视化查询Neo4j的项目,基本的查询功能已经解实现,可以实现查询属性与关系,在学习过程中,遇到了许多的困难,但最终都比较顺利的解决了,为此写下这篇报告,记录项目的基本流程,以便日后复习.","text":"本文是基于Spring-boot框架构建的可视化查询Neo4j的项目,基本的查询功能已经解实现,可以实现查询属性与关系,在学习过程中,遇到了许多的困难,但最终都比较顺利的解决了,为此写下这篇报告,记录项目的基本流程,以便日后复习. 一.准备工作 1.项目构建 2.jar包的引入 二.后台部分 1.属性的配置 2.实现基本的查询 3.构建Controller 4.不同查询功能的实现 4.1 属性的查询 4.2 关系的查询 三.前台部分 1.thymeleaf模版引擎的使用 2.取值 3.提交方式 3.1表单方式 3.2.ajax方式提交表单 四.结果 五.注意事项: 一.准备工作1.项目构建 本文基于Springboot框架进行构建,通过添加相应的Jar包,快速实现java web项目. 2.jar包的引入 jar包:包含Neo4j,ThymeLeaf模版引擎,Web.1234567891011121314151617181920212223242526272829303132333435&lt;dependencies&gt; &lt;!--springboot连接Neo4j,实现CRUD需要的工具包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-neo4j&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--springboot-web项目需要的web包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--springboot项目的测试与部署所需要的包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Neo4j连接数据库的方式--http连接 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.neo4j&lt;/groupId&gt; &lt;artifactId&gt;neo4j-ogm-http-driver&lt;/artifactId&gt; &lt;version&gt;$&#123;neo4j-ogm.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- thymeleaf模版框架 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--热部署,不用每次重启--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 二.后台部分1.属性的配置 主要涉及到Neo4j数据的配置,项目端口的配置,ThymeLeaf模版引擎的配置,具体是在application.properties文件中进行配置:12345678server.tomcat.uri-encoding=utf8# Neo4上的配置(当输入浏览器上的地址localhost:8080时,后台就跳转到http://neo4j:zxpp@localhost:7474)spring.data.neo4j.uri=http://neo4j:zxpp@localhost:7474spring.data.neo4j.username=neo4jspring.activemq.password=zxpp#模版配置#浏览器上的对应端口server.port=8080 其中thymeleaf的配置可以选用,其中的配置如下:12345678910111213#thymelea模板配置#thymeleaf模版文件的存放目录--prefixspring.thymeleaf.prefix=classpath:/templates/#模版文件的后缀是html文件spring.thymeleaf.suffix=.html#模版解析的模式spring.thymeleaf.mode=HTML5#模版文件的编码spring.thymeleaf.encoding=UTF-8#模版文件的内容spring.thymeleaf.content-type=text/html#关闭thymeleaf模版引擎的缓存spring.thymeleaf.cache=false 补充: 说明下:先附上张项目图: thymeleaf是Spring推荐的模版引擎,同时默认将Resources/templates这个文件夹作为项目根目录.其中Resources/static则是存放静态文件:js,css,image之类的),其中js的引入,有时可以使用在线src,引入到Resources/templates文件夹下的html文件当中,不用担心路径问题. 2.实现基本的查询两种方法: ❋ 通过注解,注入Neo4j的cypher语句(就是相当于自定义方法了),实现基于springboot项目对Neo4j的查询1234567891011121314151617181920212223242526272829303132333435363738@Repositorypublic interface PersonRepository extends CrudRepository&lt;Person,Long&gt; &#123; /** * 说明:findByPersonName这个是自定义方法(包含实现),具体实现是通过@query来实现了,不用再写抽象方法的具体实现了. * @param name * @return */ @Query(\"MATCH (t:Person) WHERE t.name =~ ('.*'+&#123;name&#125;+'.*') RETURN t\") Collection&lt;Person&gt; findByPersonName(@Param(\"name\") String name);// @Query(\"Match (t:Person) where t.id=~ ('(?i).*'+&#123;id&#125;+'.*') RETURN t\")// Collection&lt;Person&gt; findByPersonId(@Param(\"id\") Long id); /** * 根据id属性进行查询 * @param id * @return */ @Query(\"Match(t) WHERE id(t)=&#123;id&#125; RETURN t\") Person findOne(@Param(\"id\") Long id); /** * 通过phone属性查询对应的信息 * @param phone * @return */ @Query(\"match (t:Person) where t.phone=&#123;phone&#125; return t\") Person findPhone(@Param(\"phone\") String phone); /** * 记住既然已经是字符串,因此没有必要在&#123;name1&#125;加上引号 * @param name1 * @param name2 * @return */ @Query(\"match (a)&lt;-[r]-(b) where a.name=&#123;name1&#125; and b.name=&#123;name2&#125; return type(r)\") String searchRelationship(@Param(\"name1\") String name1, @Param(\"name2\") String name2);&#125; ❋ 根据通过对CrudRepository接口定义好的抽象方法的覆写,实现对Neo4j的查询.具体是在service包下的实现类PersonServiceImpl.1234567891011121314151617@Servicepublic class PersonServiceImpl implements PersonService &#123; @Autowired PersonRepository personRepository; @Override public Person findByName(String name)&#123; /** * 说明:findByName这个是CrudRepository接口配置好的方法:findBy+实体的属性就可以实现查询. * 来源:Person findByName(@Param(\"name\") String name); * 提示:在使用方法的过程之中,尽量使用现成的的.提高效率, * @param name * @return */ Person person = personRepository.findByName(name); return person;&#125; 3.构建Controller 基于Person类节点实现的构建.为此定义一个PersonConntroller,实现和前台的映射.特别需要注意对应地址的映射路径.好好理解 123456789101112131415161718192021222324252627282930313233/** * 类描述: PersonController---与前台的对接,映射 * * @author * @version V1.0 * @date 2018-9-1 09:41:53 **/@Controller@RequestMapping(\"/person\")public class PersonController &#123; @Autowired PersonServiceImpl personServiceImpl; @Autowired PersonRepository personRepository; @RequestMapping(\"/index\") public String index(HttpServletRequest request, HttpResponse response) &#123; /** * 跳转到输入界面(好好理解,要先到这个界面,面才会有输入的数据) * 在浏览器上输入这个映射地址,跳转到search.html,之后才有数据的输入 */ return \"/person/search\"; &#125; /** * 根据前台页面-search.html中输入框的提交的数据,使用HttpServletRequest,监听得到参数,从而确定处理参数的方法. */ @GetMapping(\"/search\") public ModelAndView find(HttpServletRequest request, HttpResponse response) &#123; String param1 = request.getParameter(\"name1\"); String param2 = request.getParameter(\"name2\"); return ((param1!= null)&amp;&amp;(param2 == \"\"))?findName(param1):findRelationship(param1, param2); &#125; 4.不同查询功能的实现 上文出现了两个方法:findName()与findRelationship(),分别可以实现属性与关系的查询. 4.1 属性的查询12345678910111213/** * 当参数值只有一个的时候,使用姓名查询单个节点 * @param name1 * @return */ public ModelAndView findName(@RequestParam(\"name1\") String name1) &#123; Person person = personServiceImpl.findByName(name1); ModelAndView mv = new ModelAndView(\"/person/search\"); mv.addObject(\"name\", person.getName()); mv.addObject(\"id\", person.getId()); mv.addObject(\"phone\", person.getPhone()); return mv; &#125; 4.2 关系的查询123456789101112/** * 当参数值有两个的时候,查询对应的节点关系 * @param name1 * @param name2 * @return */ public ModelAndView findRelationship(@RequestParam(\"name1\") String name1, @RequestParam(\"name2\") String name2) &#123; String str1 = personRepository.searchRelationship(name1, name2); ModelAndView mv = new ModelAndView(\"/person/search\").addObject(\"relationship\", str1); return mv; &#125; 三.前台部分1.thymeleaf模版引擎的使用 本文使用的是thymeleaf模版引擎,除了需要引入jar包,thymeleaf的属性配置,还需要在html模版文件当中进行空间的命名:1&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:th=\"http://www.thymeleaf.org\"&gt; 从而实现对不同dom节点(标签)的属性设置,例如: 1id:&lt;span th:text=\"$&#123;id&#125;\"&gt;&lt;/span&gt;&lt;br&gt; thymeleaf模版的使用可以参考: 初步认识Thymeleaf：简单表达式和标签 2.取值主要使用的是,thymeleaf模版12345id:&lt;span th:text=\"$&#123;id&#125;\"&gt;&lt;/span&gt;&lt;br&gt; name:&lt;span th:text=\"$&#123;name&#125;\"&gt;&lt;/span&gt;&lt;br&gt; name:&lt;span th:text=\"$&#123;sex&#125;\"&gt;&lt;/span&gt;&lt;br&gt; phone: &lt;span th:text=\"$&#123;phone&#125;\"&gt;&lt;/span&gt;&lt;br&gt; relationship:&lt;span th:text=\"$&#123;relationship&#125;\"&gt;&lt;/span&gt;&lt;br&gt; 3.提交方式 提交方式有两种:form表但提交,ajax表单提交,对与本项目 ,因为本人做的是问答系统,所以输入和输出始终在一个界面当中,因此两者差别不大,前期为加快项目的成型,先采用form表单提交,但同时也测试了ajax方式提交,但是有时会带来反应速度的慢的情况(原因正在排查).为此,在这优先使用form表单提交. 3.1表单方式123456789101112&lt;!--设置数据提交的界面--&gt;&lt;form action=\"/person/search\"&gt; &lt;input id=\"name1\" name=\"name1\" placeholder=\"请输入姓名1\"/&gt; &lt;input id=\"name2\" name=\"name2\" placeholder=\"请输入姓名2\"/&gt; &lt;button type=\"submit\" value=\"查询\"&gt;查询&lt;/button&gt; &lt;br&gt; id:&lt;span th:text=\"$&#123;id&#125;\"&gt;&lt;/span&gt;&lt;br&gt; name:&lt;span th:text=\"$&#123;name&#125;\"&gt;&lt;/span&gt;&lt;br&gt; name:&lt;span th:text=\"$&#123;sex&#125;\"&gt;&lt;/span&gt;&lt;br&gt; phone: &lt;span th:text=\"$&#123;phone&#125;\"&gt;&lt;/span&gt;&lt;br&gt; relationship:&lt;span th:text=\"$&#123;relationship&#125;\"&gt;&lt;/span&gt;&lt;br&gt;&lt;/form&gt; 3.2.ajax方式提交表单form表单提交改为ajax方式提交表单,分以下几步: ① 去掉form当中的action属性–改为增加到ajax的url中; ② 增加form的id,便于后期的序列化$(&quot;#form的id&quot;).serialize(),减轻大量input的数据提交 ③ 将button或者input的type值从submit改为text就可以.增加对应的id ④ 增加脚本:$(&quot;#button或者input的id&quot;).click(function (){},可以放在本文件,或者放入statics/js文件夹下,但要记得引入.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:th=\"http://www.thymeleaf.org\"&gt;&lt;head&gt; //在线src的引用,减少路径的错误 &lt;script src=\"https://code.jquery.com/jquery-3.1.1.min.js\"&gt;&lt;/script&gt; &lt;script&gt; $(document).ready(function()&#123; $(\"#chaxun\").click(function () &#123; // alert($(\"#searchName\").serialize());//$(\"\")取的是id值为name的标签/dom节点,的属性--val()--&gt;value=\"$&#123;name&#125;,好好理解 var name1 = $(\"#name\").val();// 表示根据id获取元素,.根据css样式获取元素,val是jquery中的代表value,用来获取属性值 $.ajax( &#123; async: false, //这个其实和form表单提交数据的页面一样 url:'/person/search',//请求的参数:,当需要提交的参数比较多的时候,可以使用这个方法.ajax data:$(\"#searchName\").serialize(),//请求的类型: type:\"post\",//返回的数据类型,为了实现简单 dataType:\"json\",//就是参数发送成功之后,会执行的操作,所以可以将数据进行输出 success:function(data) &#123; alert(data.id+\" \"+data.name+\" \"+data.phone); //获取后台传递过来的json数据 $(\"#id\").html(data.id); $(\"#name\").html(data.name); $(\"#phone\").html(data.phone); &#125;, error: function(request) &#123; alert(\"查询失败,error\"); &#125; &#125;); &#125;) &#125;) &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;form id=\"searchName\"&gt; id:&lt;p th:text=\"$&#123;id&#125;\"&gt;&lt;/p&gt;- name:&lt;p th:text=\"$&#123;name&#125;\"&gt;&lt;/p&gt;- phone: &lt;p th:text=\"$&#123;phone&#125;\"&gt;&lt;/p&gt;- &lt;div id=\"id\"&gt;&lt;/div&gt; &lt;div id=\"name\"&gt;&lt;/div&gt; &lt;div id=\"phone\"&gt;&lt;/div&gt; &lt;input id=\"name\" name=\"name\"/&gt; &lt;button type=\"button\" value=\"查询\" id=\"chaxun\"&gt;查询&lt;/button&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 四.结果 五.注意事项: 由于IDEA没有将资源文件实现一起发布的功能,需要先将后台文件进行打包,之后把resource文件夹下的文件复制到E:\\Project\\Pratice\\target\\classes\\templates\\person之中,即:target\\classes\\templates\\person然后进行运行.只要资源文件进行了修改,就需要在重新发布(build Project)之后添加进去.其余文件不需要.","categories":[{"name":"Neo4j","slug":"Neo4j","permalink":"http://yoursite.com/categories/Neo4j/"},{"name":"Spring-boot","slug":"Neo4j/Spring-boot","permalink":"http://yoursite.com/categories/Neo4j/Spring-boot/"}],"tags":[{"name":"Spring-boot","slug":"Spring-boot","permalink":"http://yoursite.com/tags/Spring-boot/"},{"name":"thymeleaf","slug":"thymeleaf","permalink":"http://yoursite.com/tags/thymeleaf/"}]},{"title":"RequestMapping,responseBody,requestBody","slug":"@RequestMapping，@responseBody，@requestBody","date":"2018-09-05T11:25:40.000Z","updated":"2018-11-13T02:18:22.000Z","comments":true,"path":"2018/09/05/@RequestMapping，@responseBody，@requestBody/","link":"","permalink":"http://yoursite.com/2018/09/05/@RequestMapping，@responseBody，@requestBody/","excerpt":"@RequestMapping，@responseBody，@requestBody三个注解在实际中运用广泛，@RequestMapping是地址映射注解；@requestBody则是将将前台数据转换成javabean对象；@responseBody则是将javabean对象解析成json/xml格式数据.","text":"@RequestMapping，@responseBody，@requestBody三个注解在实际中运用广泛，@RequestMapping是地址映射注解；@requestBody则是将将前台数据转换成javabean对象；@responseBody则是将javabean对象解析成json/xml格式数据. 1.@RequestMapping(“url”) url 作用 2.@requestBody 作用 说明 3.@responseBody 作用 说明： 1.@RequestMapping(“url”)urlurl是请求路径的一部分,实际使用,需加上主机地址. 作用地址映射，可以映射到类（可以写），方法上（必写）。123456789@RequestMapping(value = \"/test\")//类级别映射，可以没有，一般用于减少书写量public class myController &#123; #方法级别映射，必须有，那么这个方法的访问地址就是/test/aaa， #请求到的页面就是test.jsp【当然，这里的.jsp需要在配置文件中配置】 @RequestMapping(value = \"/aaa\") public String getMyName() &#123; return \"test\"; &#125;&#125; 2.@requestBody作用 作用在形参上;将前台数据转换成javabean对象。 说明 将前台发过来的数据（一般数据格式固定，json或xml），统一转换/封装成javaBean对象（便于处理）。会自动使用合适的HtppMessageConvert插入到指定的javabean对象中。12345678@RequestMapping(\"/login.do\") @ResponseBody #将前台传过来的数据转换成javabean对象--User类型的对象--loginUuser. public Object login(@RequestBody User loginUuser, HttpSession session) &#123; user = userService.checkLogin(loginUser); session.setAttribute(\"user\", user); return new JsonResult(user); &#125; 3.@responseBody作用作用在controller方法的返回值/对象；将返回值解析成json或xml格式，并存入到存到response的http的body（正文）区。 说明： 当使用@RequestMapping之后，返回值一般通常将javabean对象解析成页面的跳转地址。但是有@responseBody，那么返回值不再解析为跳转的地址，而是解析为json数据或者XMl，并存入到response的http的body（正文）区之中。1234567@RequestMapping(\"/login\") @ResponseBody public User login(User user)&#123; #因为有@ResponseBody,所以user转换成json格式数据存入到response的http的body（正文）区之中 #之后前台（浏览器、客户端）解析response的http，得到：&#123;\"userName\":\"xxx\",\"pwd\":\"xxx\"&#125; return user; &#125; User字段：userName pwd 那么在前台接收到的数据为：’{“userName”:”xxx”,”pwd”:”xxx”}’ 效果等同于如下代码：12345 @RequestMapping(\"/login\") public void login(User user, HttpServletResponse response)&#123; #有@ResponseBody，数据格式会自动转换。 response.getWriter.write(JSONObject.fromObject(user).toString()); &#125;","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"注解","slug":"java/注解","permalink":"http://yoursite.com/categories/java/注解/"}],"tags":[{"name":"注解","slug":"注解","permalink":"http://yoursite.com/tags/注解/"}]},{"title":"ORM,OGM,SDN,POJO,JPA","slug":"ORM,OGM,SDN,POJO,JPA","date":"2018-09-05T11:09:19.000Z","updated":"2018-11-13T02:45:42.000Z","comments":true,"path":"2018/09/05/ORM,OGM,SDN,POJO,JPA/","link":"","permalink":"http://yoursite.com/2018/09/05/ORM,OGM,SDN,POJO,JPA/","excerpt":"在学习Neo4j的SDN建模时候,涉及到一些基本概念,为此在此进行总结.以便日后复习.ORM与JPA是针对关系POJO对象与关系数据库的,映射ORM的实现形式可以使用注解的形式实现,JPA就是针对数据库的接口.OGM与SDN则是针对POJO对象与Neo4j图数据库的,映射OGM的实现也可以用注解实现,SDN也是数据库的接口。","text":"在学习Neo4j的SDN建模时候,涉及到一些基本概念,为此在此进行总结.以便日后复习.ORM与JPA是针对关系POJO对象与关系数据库的,映射ORM的实现形式可以使用注解的形式实现,JPA就是针对数据库的接口.OGM与SDN则是针对POJO对象与Neo4j图数据库的,映射OGM的实现也可以用注解实现,SDN也是数据库的接口。 1. 概念 2. 关系 1. 概念 (1)ORM:Object/Relation Mapping–对象/关系数据库映射 (2)OGM:Object/Graph Mapping–对象/图数据库映射 (3)POJO:plain Ordinary Java Object–简单java对象 (4)SDN:Spring Data Neo4j–为访图数据库提供接口(是在建立映射之后) (5)JPA:Java Persistence API–Java持久层API–主要是为访问关系数据库提供接口(是在建立映射之后) 2. 关系 创建一个POJO对象,对象建模,然后不同的存储库/数据库访问接口(SDN/JPA)使用对应的映射(ORM,OGM)实现对象与数据库当中元素的转换(映射),最终实现CRUD的功能.以SDN使用OGM映射机制为例,只需要添加简单的注解就可以实现对象与图数据库的映射","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"知识点","slug":"java/知识点","permalink":"http://yoursite.com/categories/java/知识点/"}],"tags":[{"name":"ORM","slug":"ORM","permalink":"http://yoursite.com/tags/ORM/"},{"name":"OGM","slug":"OGM","permalink":"http://yoursite.com/tags/OGM/"},{"name":"SDN","slug":"SDN","permalink":"http://yoursite.com/tags/SDN/"},{"name":"POJO","slug":"POJO","permalink":"http://yoursite.com/tags/POJO/"},{"name":"JPA","slug":"JPA","permalink":"http://yoursite.com/tags/JPA/"}]},{"title":"model,ModelMap,ModelAndView区别与联系","slug":"model,ModelMap,ModelAndView区别与联系","date":"2018-09-05T10:07:49.000Z","updated":"2018-11-13T02:36:40.000Z","comments":true,"path":"2018/09/05/model,ModelMap,ModelAndView区别与联系/","link":"","permalink":"http://yoursite.com/2018/09/05/model,ModelMap,ModelAndView区别与联系/","excerpt":"本文主主要是讲述了，如何从控制层中，将输入的数据传输到前台当中(jsp页面),主要是运用Model,ModelMap,ModelAndView这三个数据模型,在控制层中实现数据装载,之后设定页面跳转的地址,最后jsp页面使用EL表达式解析数据模型的实例装载的数据.根据${key}实现解析.","text":"本文主主要是讲述了，如何从控制层中，将输入的数据传输到前台当中(jsp页面),主要是运用Model,ModelMap,ModelAndView这三个数据模型,在控制层中实现数据装载,之后设定页面跳转的地址,最后jsp页面使用EL表达式解析数据模型的实例装载的数据.根据${key}实现解析. 1.作用： 2.三者关系 3.数据装载 4.数据传输 1.作用： 装载数据,传递数据.{从控制类（以controller为例）向页面（jsp页面）传值}。 2.三者关系 Model是接口,ExtendModelMap是Model接口的实现类，ExtendModelMap的父类是modelMap类，ModelMap类实现Map接口。 ModelAndView则是包含模型和视图的类,Model和ModelMap分别是只包含模型的接口与类. 3.数据装载(1)model与ModelMap123456789@RequestMapping(\"hello\")#ModelMap model作为控制器方法参数传入#public String testVelocity(Model model,String name)public String testVelocity(ModelMap model,String name)&#123;#数据装载:将name值传入到数据模型model当中 model.addAttribute(\"test\",name);#页面跳转:返回到hello.jsp页面,以地址以字符串形式作为返回值返回。 return \"hello\";&#125; 说明： Model与ModelMap的实例直接定义就可以由spirng mvc框架来自动创建并作为控制器方法参数传入，用户无需自己创建。页面跳转原则上不行,但可以使用控制器的返回值进行页面跳转,一般可以作为控制器的参数.(2)ModelAndView123456789101112131415@RequestMapping(\"hello\")public ModelAndView helloModel(String name) &#123; #1.页面跳转:手动构建ModelAndView实例，并设置跳转地址 #两种方式都是手动创建实例,new方法 #第一种寻址方式(页面跳转设置),跳转到hello.jsp页面 ModelAndView mv = new ModelAndView(); mv.setViewName(\"hello\") #第二种 ModelAndView view = new ModelAndView(\"hello\"); #2.数据装载:用addObject()装到ModelAndView对象view中,第二个参数可以是任何java类型 # \"test\"是key,输入的name是value,之后在hello.jsp页面使用El表达式,接收,解析. view.addObject(\"test\",name); #返回ModelAndView的实例view即可. return view;&#125; 说明： 该类需要自己手动创建,可以进行业务寻址的，通过setViewName来设置跳转地址，因此最后返回自己就可以，因为ModelAndView实例的地址跳转属性已经设置好了。 4.数据传输在控制层的java代码中写入：12345678910//数据传输@RequestMapping(\"hello\")public String hello(Model model) &#123; // 接收查询的信息 List&lt;Hello&gt; cs2= helloService.list(); // 封装了查询的数据 model.addAttribute(\"test\", cs2); //重要！！需要给出返回model跳转的路径 return \"hello\";&#125; 然后在hello.jsp(记住要和之前返回的页面一致,否则接收不到)页面中写入：12345678910&lt;!-- 获取值的时候，使用EL表达式“$&#123;key&#125;”进行一一对应，--&gt;&lt;!--因此可以接收Model类型实例--model的key对应的value值。--&gt;&lt;!--同时对接收的数据进行了重命名，取了个别名为c，--&gt;&lt;!--之后还是使用EL表达式接收传递过来的数据--&gt; &lt;c:forEach items=\"$&#123;test&#125;\" var=\"c\" varStatus=\"st\"&gt; &lt;tr&gt; &lt;td&gt;$&#123;c.id&#125;&lt;/td&gt; &lt;td&gt;$&#123;c.name&#125;&lt;/td&gt; &lt;/tr&gt; &lt;/c:forEach&gt; 最后在浏览器输入：主机地址/hello,就可以得到如下结果说明下： ❋ hello–跳转的页面hello。jsp ❋ test–数据模型model、ModelMap，ModelAndView实例的key； ❋ name：就是用户输入的数据。感谢： ❋ Model、ModelMap和ModelAndView使用区别理解❋ SPRING MVC中的MODELMAP作用及用法❋ Spring中Model,ModelMap以及ModelAndView之间的区别❋ Model、ModelMap和ModelAndView的使用详解❋ ModelMap的用法","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"知识点","slug":"java/知识点","permalink":"http://yoursite.com/categories/java/知识点/"}],"tags":[{"name":"model","slug":"model","permalink":"http://yoursite.com/tags/model/"},{"name":"ModelMap","slug":"ModelMap","permalink":"http://yoursite.com/tags/ModelMap/"},{"name":"ModelAndView","slug":"ModelAndView","permalink":"http://yoursite.com/tags/ModelAndView/"}]},{"title":"Page与Pageable","slug":"Page与Pageable","date":"2018-09-05T07:22:27.000Z","updated":"2018-11-13T02:46:28.000Z","comments":true,"path":"2018/09/05/Page与Pageable/","link":"","permalink":"http://yoursite.com/2018/09/05/Page与Pageable/","excerpt":"本文主要讲述的是page与Pageable的两者的不同,目的都是为了实现分页查询,只不过Pageable提供分页的配置信息,page则是对查询到的数据依据Pageable提供的配置信息进行分页.特此总结下,以便日后回顾.","text":"本文主要讲述的是page与Pageable的两者的不同,目的都是为了实现分页查询,只不过Pageable提供分页的配置信息,page则是对查询到的数据依据Pageable提供的配置信息进行分页.特此总结下,以便日后回顾.目录如下： 1.pageable的实现类: 2.Page的实现类: 3.联系 1.pageable的实现类:1new PageRequest(page,sort,size) 说明: ①page:第几页，从0开始，默认为第0页,开始的页数(可以设定)–分页信息 ②size:每页的大小–分页信息 ③sort:排序的方式–例如sort = new Sort(Direction.DESC, “id”),其中id是数据的属性 2.Page的实现类:1new pageImpl(list,total,pageable)--返回值是Page类型 说明: ①list:查询到的数据–new Arraylist:其中East是一个自定义类型的数据(有很多属性) ②total:查询到的数据的数量 ③pageable:总页数,每页的大小,以便于page对象的分页(因为已经有total–得到数据数量,再根据pageable的分页设置信息,排序设置信息,完成分页、排序设置). 3.联系 Pageable接口其实就是提供分页,排序的设置信息,试下方式就是通过new PageRequest(page,size,sort),或者直接Pageable pageable,Spring会根据直接将设置好的配置信息()注入到,具体实现方法如下:1234@RequestMapping(value = \"\", method=RequestMethod.GET)public Page&lt;Blog&gt; getEntryByPageable(@PageableDefault(value = 15, sort = &#123; \"id\" &#125;, direction = Sort.Direction.DESC) Pageable pageable) &#123; return blogRepository.findAll(pageable); 之后Page接口,利用设置好的信息–pageable、查询到的数据–List，查询到的数据的数量–total作为PageImpl方法的参数，生成Page类型的对象,最终完成分页.详细情况请参考,如下博文: ❋整合Spring Data JPA与Spring MVC: 分页和排序❋JPA中的Page与Pageable","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"知识点","slug":"java/知识点","permalink":"http://yoursite.com/categories/java/知识点/"}],"tags":[{"name":"java","slug":"java","permalink":"http://yoursite.com/tags/java/"},{"name":"知识点","slug":"知识点","permalink":"http://yoursite.com/tags/知识点/"}]},{"title":"springboot连接Neo4j","slug":"springboot连接Neo4j","date":"2018-08-27T14:34:28.000Z","updated":"2018-11-13T02:51:18.000Z","comments":true,"path":"2018/08/27/springboot连接Neo4j/","link":"","permalink":"http://yoursite.com/2018/08/27/springboot连接Neo4j/","excerpt":"本文主要讲述的是Springboot与Neo4j的集成,涉及到的知识点有cypher语句,Springboot框架,Neo4j的注解,http测试等等,遇到的坎比较多,但好在一一解决,述写此文仅作后续复习。","text":"本文主要讲述的是Springboot与Neo4j的集成,涉及到的知识点有cypher语句,Springboot框架,Neo4j的注解,http测试等等,遇到的坎比较多,但好在一一解决,述写此文仅作后续复习。 一.Neo4j的写入操作 二.Spring项目的快速建立 三.项目代码介绍 四.http测试 五.查询结果 六.注意事项 一.Neo4j的写入操作 首先使用cypher语句进行节点(Person类型的节点)及其属性的插入,为后来在springboot中建立节点实体类(Person类)提供依据,具体的cypher语句形式为:1Create (n:Person&#123;name:\"zxp\",phone:\"10086\",sex:\"男\"&#125;) 结果形式为: 二.Spring项目的快速建立 本项目使用Springboot框架实现Neo4j的集成,先使用IDEA进行spring项目的构建,其中构建步骤可以参考使用IDEA创建SpringBoot项目,其中添加的依赖为:web与Neo4j. 三.项目代码介绍 四.http测试主要采用Restful接口进行测试,在IDEA中的Tool选项,具体情况如图所示:注意查询的地址就是:1http://localhost:8080/p/zxp 不是1http://localhost:7474/p/zxp 这个要记住,对应上图的Restful接口测试。 五.查询结果 六.注意事项 在资源配置文件:application.properties中,配置为如下形式:1234#uri地址一定要写成这样,以管理者权限进行访问spring.data.neo4j.uri=http://neo4j:zxpp@localhost:7474spring.data.neo4j.username=neo4jspring.activemq.password=zxpp 其中spring.data.neo4j.uri=http://localhost:7474会造成权限不够访问失败,出现如下信息:1234&#123;\"timestamp\":\"2018-08-27T12:27:56.677+0000\",\"status\":500,\"error\":\"Internal Server Error\",\"message\":\"http://localhost:7474/db/data/transaction/commit; nested exception is org.neo4j.ogm.exception.ConnectionException: http://localhost:7474/db/data/transaction/commit\",\"path\":\"/p/zxp\"&#125; 截图:","categories":[{"name":"Neo4j","slug":"Neo4j","permalink":"http://yoursite.com/categories/Neo4j/"},{"name":"Spring-boot","slug":"Neo4j/Spring-boot","permalink":"http://yoursite.com/categories/Neo4j/Spring-boot/"}],"tags":[{"name":"Neo4j","slug":"Neo4j","permalink":"http://yoursite.com/tags/Neo4j/"},{"name":"Spring-boot","slug":"Spring-boot","permalink":"http://yoursite.com/tags/Spring-boot/"}]},{"title":"知识图谱的发展史,关键技术与应用","slug":"知识图谱的发展史,关键技术与应用","date":"2018-08-17T01:21:04.000Z","updated":"2018-11-13T03:14:46.000Z","comments":true,"path":"2018/08/17/知识图谱的发展史,关键技术与应用/","link":"","permalink":"http://yoursite.com/2018/08/17/知识图谱的发展史,关键技术与应用/","excerpt":"本文是基于达观数据的桂洪冠老师的&lt;&lt;知识图谱关键技术与应用&gt;&gt;和文因互联鲍捷老师的&lt;&lt;知识图谱发展史&gt;&gt;进行整理的,用来加深对知识图谱的认识。重点是把握图谱构建的各个环节所用到的技术和知识图谱的发展历程。","text":"本文是基于达观数据的桂洪冠老师的&lt;&lt;知识图谱关键技术与应用&gt;&gt;和文因互联鲍捷老师的&lt;&lt;知识图谱发展史&gt;&gt;进行整理的,用来加深对知识图谱的认识。重点是把握图谱构建的各个环节所用到的技术和知识图谱的发展历程。 一.概述 1.1 两个问题 1.1.1 用三元组来描述KG的原因 1.1.2 AI为什么需要知识图谱 1.2 出现原因 二.应用 2.1 风控反欺诈 2.2 信贷审核（对个人） 2.3 投研分析（对公司） 2.4 精准营销(个性化推荐) 2.5 改善搜索 2.6 改善问答 三.图谱构建周期 四.知识建模 4.1 本体层 五.知识抽取: 5.1 实体抽取 5.2 关系抽取 5.2.1 方法 5.2.2 工具 (1).deepdvie (1.1) 工作流程 (2).联合学习标注 (2.1) 基于深度学习端到端的联合标注 (2.1.1) 序列标注： 六.实体融合 6.1 实体对齐 6.1.1 计算相似度 6.1.2 融合人工对齐的标注数据 6.2 实体合并 七.知识存储 八.知识推理 8.1 基于符号的推理 8.2 基于图计算的推理 8.2.1 图计算引擎 8.2.2 中心度 8.2.3 信息流 8.2.4 关系 8.3 基于分布式推理 8.3.1 transe系列模型 8.3.2 基于深度学习 九.经验 十.知识图谱发展史 一.概述1.1 两个问题1.1.1 用三元组来描述KG的原因 三元组是一种简单的易于人类解读的结构三元组方便编写计算机程序来进行抽取和加工处理 1.1.2 AI为什么需要知识图谱 (1)帮助人工智能从感知阶段迈向认知阶段。认知的建立需要思考的能力，而思考是建立在知识基础上的知识图谱富含实体、概念、属性、事件、关系等信息，基于一定的知识推理为可解释性AI提供了全新的视角和机遇 (2)消除自然语言和深度学习黑盒之间的语义鸿沟。因为深度学习的可解释性差,知识图谱的可解释强 1.2 出现原因 补充: (1)答案在知识库的问答系统就是KBQA (2)通用知识图谱为行业知识图谱提供框架,常识性知识,反过来行业知识图谱扩展了通用知识图谱的深度; 二.应用2.1 风控反欺诈 不一致性验证: 就是两个不同的人填写的号码是同一个公司的; 组团欺诈: 三个不相关的人同一天向银行发起贷款,并且号码一样. 静态异常检测: 以前几个人不常联系,最近常联系,会引起注意; 动态异常检测: 节点关系发生脱离; 失联客户管理 联系不到本人的时候,可以联系与它与相关的人,不只只是亲戚2.2 信贷审核（对个人） 这种知识图谱一般是融合多个数据库,通过搜索某个人，就可以快速知道该人的社会关系和自身情况等多方面情况,可以确定是否需要进行放贷。2.3 投研分析（对公司） 这种知识图谱一般也是融合多个数据库，搜索该公司就可以得到该公司的自身情况与与其他公司的关系，从而确定公司的评级，从而辅助投资报告的生成。2.4 精准营销(个性化推荐)基本思想: 根据人物的自身属性，社会属性，行为属性推荐符合该用户的情况的产品 场景化推荐 （沙滩鞋-&gt;游泳衣、防晒霜、海岛度假产品） 任务型推荐 （牛肉卷、羊肉卷-&gt;火锅底料、电磁炉？ ；螺丝、螺钉-&gt;多功能螺丝刀） 冷启动环境（最初使用的时候）下推荐，可以根据浏览时长最长的内容的标签来推荐。（语义标签：摄影VS旅游；相同导演或相同主演的电影；） 跨领域推荐 （微博如何推荐淘宝商品？用户经常晒九寨沟、黄山、泰山的照片-&gt;淘宝登山装备 ) 知识型推荐—基于知识库的推荐 （清华大学、北京大学-&gt;复旦大学（985名校);阿里、百度-&gt;腾讯（互联网BAT等）趋势：从基于行为的推荐发展到行为与语义融合的智能推荐2.5 改善搜索 有利于让用户探索知识发现的快乐，（利用知识图谱，在提高结果准确率的同时，同时推荐与该结果同主题的知识，例如：搜索长城，推荐故宫，因为两者有一个共同的属性：所在地都是北京），而不是单单搜索答案。2.6 改善问答 之前KBQA讲过很多,这里不在叙述。三.图谱构建周期 讲完概念，应用之后，接下来就涉及到图谱的构建。具体情况见如下图： 有五个过程：知识建模,知识抽取,知识融合,知识存储,知识推理（循环迭代）四.知识建模 在进行知识建模之前。需要对本体要有一定的认识。4.1 本体层 本体：描述事物的本质，在维基百科中的定义：特定领域的实体及其属性的相互关系的定义 总共有五层：事件层,实体层,概念层,主题分类层 本体编辑器protege屏蔽了具体的本体描述语言，用户只需在概念层次上进行领域本体模型的构建五.知识抽取: 信息抽取(information extraction, IE)；信息抽取是一项从文本中发现实体(entity)以及实体之间的关系(relation)的技术。 共指解析(coreference resolution)可以帮助IE系统对文中出现的指称(mention)进行归类，避免提取冗余的信息。（就是确定多个指称指向同一个实体）5.1 实体抽取 crf只能学习相邻比较近的上下文特征。（距离较短）无法获取整个句子，甚至段落更长的特征。业界主流： 深度循环神经网络（bi-LSTM）+CRF+Embeding ，具体情况如下图: 顶层:用crf可以对结果有更好的控制。（之前用深度循环神经网络（bi-LSTM）来细化，之后用crf粗化(约束)，防止过拟合） 中层:深度循环神经网络（bi-LSTM）从前往后与从后往前双向学习上下文的特征，得到序列信号(的记忆与传递。 (1)从前往后：华为发布了新一代的麒麟处理X (2)从后往前：X鲜和美国签订了新一轮的谅解备忘录 底层:词向量技术,先将句子/文章转换为高维的稀疏词向量(one-hot),之后使用embeding技术转换成低维的稠密的词向量5.2 关系抽取 信息抽取(information extraction, IE)；信息抽取是一项从文本中发现实体(entity)以及实体之间的关系(relation)的技术。 共指解析（coreference resolution）可以帮助IE系统对文中出现的指称(mention)进行归类，避免提取冗余的信息5.2.1 方法(1)有监督的学习方法: 分类的方法,根据训练的数据,设计有效的特征,来学习各个分类模型—需要大量的人工标注/训练的语料(2)半监督的学习方法 常用Bootstrapping进行关系抽取:对于要抽取的关系:先基于手工设定若干种子实例,之后迭代的从数据当中抽取关系对应的模版和更多的实例,使用迭代的方式来抽取越来越多的实例。(2)无监督的学习方法:聚类 先假设拥有相同语义关系的实体(实体链接相同)拥有相同的上下文,因此可以利用实体的上下文代表实体的语义关系,然后对实体的语义关系进行聚类 三种方法当中: 有监督的方法能够抽取有效的特征,其余两种效果不是很好.因此如何高效获取训练样本,就成为了一个问题.使用远程监督（Distant supervision）技术，代表工具:deepdive:信息抽取框架。5.2.2 工具(1).deepdvie 使用弱监督方法（Distant supervision技术）的deppdive,lDeepdive是由斯坦福大学InfoLab实验室开发的一个开源知识抽取系统。它通过弱监督学习，从非结构化的文本中抽取结构化的关系数据 ，可以判断两个实体间是否存在指定关系。具有较强的灵活性，可以自己训练模型。 DeepDive要求开发者思考特征而不是算法，可以通过使用已有的领域知识指导推理，接受用户反馈，提高预测的质量，使用Distant supervision技术，只需少量/甚至不需要训练数据，有监督学习法因为能够抽取并有效利用特征，在获得高准确率和高召回率方面更有优势，是目前业界应用最广泛的一类方法。先讲一下一些相关的概念： (1)实体(entity)：知识库中完整定义的，唯一存在的条目，在coreference resolution这个任务中，每一个实体都可以看作是指代它的名词短语或代词构成的集合(巴拉克－奥巴马={美国总统, 奥巴马, 第44任美国总统, 他})。 (2)指称(mention)：实体在自然语言文本中的别名或另一种指代形式，美国总统, 奥巴马, 第44任美国总统, 他 等都是mention。mention的类型一般有3类：专有名词（proper noun，巴拉克－奥巴马），名词性词（nominal, 第44任美国总统），代词（pronominal, 他） (3)共指(corefer)：如果文本或句子中的两个mention指代的是同一个entity，那么它们之间存在着一种共指(corefer)关系。美国总统与第44任美国总统 即是共指的两个mention(1.1) 工作流程具体工作流程如下：1.输入句子（可以长句）： 如”Barack and Michelle are married”这个句子—mention级数据2.实体识别： 识别实体的指称(mention)，实体在自然语言文本中的别名或另一种指代形式，美国总统, 奥巴马, 第44任美国总统, 他 等都是mention。mention的类型一般有3类：专有名词（proper noun，巴拉克－奥巴马），名词性词（nominal, 第44任美国总统），代词（pronominal, 他）—mention级实体3.确定mention级关系： 确定-mention级实体之间的关系，因为mention级实体两两匹配可能会有多个mention级关系。—-mention级关系3.实体链接: 就是将实体的指称映射到知识库当中,当然这也是以知识库当中的知识都是正确的。—Mention的实体与知识库级实体的映射4.确定实体级/知识库级关系： 在知识库当中查找确定最终关系.—实体级/知识库级关系总体就是流水线式抽取，但是有两个缺点： （1）错误传播，如果实体识别错了，后面一定会错； （2）关系冗余：因为mention级实体两两匹配可能会有多个mention级关系，导致在知识库级实体的关系也是有多个，尽管有些关系不存在，但是还是需要进行验证。(2).联合学习标注通常是基于神经网络的联合标注两个问题: 模型参数共享的问题与标注的策略。 其中模型参数共享的问题:在训练的时候能够进行实体识别与关系分类。通过反向传播来更新参数，实现模型参数的依赖来完成上述两个子任务（也是相互依赖，实体识别与关系分类），本质还是两个任务，只不过将两个子任务的依赖体现在参数的不断更新，使得模型合适实体识别与关系分类，因此仍然会有关系冗余（训练之前还需要进行NER，然后对句子中出现的各个实体进行相两两匹配，进行关系的分类）。(2.1) 基于深度学习端到端的联合标注将抽取的问题转换成标注任务，两个步骤： (1)训练模型：先构建一个标注框架,之后训练一个端到端的标注模型。 (2)答案提取：输入句子之后，先使用标注框架进行标注（实质是进行实体识别,之后两两匹配,得到多个关系,也就是得到的标注情况有多种），之后使用训练好的模型得到最合适的序列标注(确定实体关系)，然后将同样关系类型的实体合并成一个三元组，作为最终的提取结果。具体过程见下图：因此需要对序列标注进行着重讲解:(2.1.1) 序列标注：具体情况见下图: (1)实体中词的位置：（实体中词的位置，B，I，E分别表示实体的开始，内部，结尾） (2)关系类型的信息：cp:总统,cf:创始人 (3)实体的位置:是实体1还是实体2 (4)除了实体及其关系的其他词语都用o来表示六.实体融合 主要任务是实体对齐，实体合并。最主要实体对齐,其中实体合并的过程包括实体对齐，不同实体的合并问题等等。6.1 实体对齐 旨在发现具有不同标识实体但却代表真实世界中同一对象的那些实体，并将这些实体归并为一个具有全局唯一标识的实体对象添加到知识图谱中（例如china，中国，中华人民共和国指的是相同的实体）.和共指解析有异曲同工之妙。通常采用聚类的方法,关键定义相似度的阈值。主要有两种方法实现实体对齐：计算相似度，融合人工对齐的标注数据。6.1.1 计算相似度实体对齐主要通过计算实例之间相似度 (1)字符相似度维度：实体间的描述的相似度； (2)属性相似度：实体间的属性相似度，相同属性值 (3)结构相似度：具有相同邻居的实体可能指向同一个对象6.1.2 融合人工对齐的标注数据 如LOD (linked open data)中已有的人工对齐标注数据（使用owl:sameAs关联两个实体）可以作为训练数据学习发现更多相同的实体对。6.2 实体合并例如：不同实体的合并问题，隐含父子关系 (1)考虑不同实体间对应数据源的可靠性； (2)考虑不同实体在各个数据源当中的出现频率等确定选用哪个实体。 不管使用哪种方法，因为无论何种自动化方法都无法保证100%的准确率，最后都需要使用人工校验七.知识存储 数据库层面的选择有：图数据库、NoSQL数据库、关系数据库 (1)若KG结构复杂，且关系复杂，连接多，建议使用图数据库，如Neo4J等 (2)若KG侧重节点知识，关系简单，连接少，可以使用传统关系数据库 (3)若考虑KG的性能、扩展性和分布式等，可以使用NoSQL数据库（HBASE，HIVE） (4)根据实际情况，也可以多种数据库融合使用（ES和Neo4J )八.知识推理8.1 基于符号的推理1.一阶谓词逻辑 (1)一套灵活且紧凑的对象知识表示方式 (2)完善的推理能力和表达能力支持2.Semantic Net (1)一个通过语义关系连接的概念网络 (2)表达能力受限，直观但是缺乏语义支持3.Frame–框架逻辑 (1)直接表示领域知识模型,支持默认推理 (2)表达能力受限，缺乏语义支持4.Script 表达事件知识，非常受限， 对符合条件的场景非常适合5.语义网知识表示语言体系 语义网提供了一套为描述数据而设计的表示语言和工具，用于形式化的描述一个知识领域内的概念、术语和关系。一系列W3C的标准和工具: (1)RDF,RDFS: 缺少准确的语义描述; 不含推理模型。 (2)OWL,XML等缺点: 学习成本较高,有的时候杀鸡用牛刀.8.2 基于图计算的推理 主要是PRA算法(path ranking algorithm),将连接两个实体的路径作为特征来预测其之间可能存在的关系。主要涉及中心度(缩短路径距离)与信息流(优化路径)的选择。8.2.1 图计算引擎 基于图计算的推理，一般都采用图计算引擎来实现,这里结合项目，以graphX和neo4j（本质上也是一个图引擎）为例，两者区别见下图。 Neo4j graphx(算法框架,图计算引擎) 侧重点 知识存储 知识计算 使用场景 OLTP(请求应答,及时性较强,但是数据量不大) OLAP(常用来分析知识.抽取出一些特征) 结合 图数据太大，无法放在单个机器上,需要采用分布式/集群部署,需要使用graphX计算引擎加快数据处理，单机的时候使用neo4j足矣 特点 单机上离线存储 离线计算、批量处理，基于同步的BSP模型,速度有点劣势,但胜在处理数据量 8.2.2 中心度1.紧密中心度 紧密中心度（closeness centrality）是依据网络中各个节点之间的最短路分布而定义的中心度。 顶点到其余顶点的最短距离总和越小，紧密中心度就越大。 形象理解为紧密中心度越大的顶点越处于网络的“中央”。 2.中介(Betweenness)中心度 如果一个顶点位于其余顶点间的多条最短路径上，则认为该顶点重要节点。3.特征向量中心度 认为“顶点g的每个邻居为g贡献一个点的重要性” 8.2.3 信息流传播种类:(1)信息传播模式 replication：并行复制、串行复制 transfer (2)路径类型 path–就是两点之间的路径,点不能重复;—sdfgd trail–就是两点间的路径,线路不能重复;—sdfdgd,成立 sdfgdf就不是 walk–就是包含trail,path和其他类型的路径:点–线–点 (3)路径选择 最优选择 随机选择 8.2.4 关系 中心度衡量节点的重要性； 重要性则由网络上信息传播过程中节点的关键性体现； 不同的信息传递方式对节点重要性的度量有着不同的影响。8.3 基于分布式推理8.3.1 transe系列模型 将实体和关系映射到一个低维的embedding空间中，基于知识的语义表达进行推理建模.8.3.2 基于深度学习 词向量是基于依存数的,也就是词在文中/句中的位置,将位置写入词向量中,之后使用卷积神经网络训练,效果较之前单纯的词向量有较大的提升九.经验1.界定场景与急需解决的问题2.设计好模型： 定义出与场景和问题相关的领域概念层次结构、概念之间的关系类型定义3.搜集数据： 根据问题和场景梳理领域相关数据，结构化、半结构化、无结构化行业语料、百科相关数据、行业词典、专家规则（规则也是数据，类似词典）。4.不要重复造轮子： 合理利用百科的数据和开放知识图谱的数据5.验证与反馈： 必须要有验证与反馈机制，确保知识的精确性6.持续迭代： 本身是一个持续迭代的系统工程，人机交互，持续运营，不断丰富完善。补充: (1)结构化数据:sql数据库 (2)半结构划数据:—excel,csv,json,xml (3)无结构化数据:纯文本,html (4)知识图谱的更新,在三元组基础上加上时间维度,或者知识图谱实时连接到实时更新的关系数据库上。十.知识图谱发展史","categories":[{"name":"知识图谱","slug":"知识图谱","permalink":"http://yoursite.com/categories/知识图谱/"},{"name":"综述","slug":"知识图谱/综述","permalink":"http://yoursite.com/categories/知识图谱/综述/"}],"tags":[{"name":"知识图谱","slug":"知识图谱","permalink":"http://yoursite.com/tags/知识图谱/"},{"name":"综述","slug":"综述","permalink":"http://yoursite.com/tags/综述/"}]},{"title":"控制反转与面向切面理解","slug":"控制反转与面向切面理解","date":"2018-07-23T08:59:21.000Z","updated":"2018-11-13T03:04:44.000Z","comments":true,"path":"2018/07/23/控制反转与面向切面理解/","link":"","permalink":"http://yoursite.com/2018/07/23/控制反转与面向切面理解/","excerpt":"IOC与AOP是spring库的两大思想,能够极大的降低程序耦合程度,提高了程序之间的逻辑性,便于日后维护及其扩展,本文主要从概念,原因,内容三个方面进行阐述,不正之处,忘大家指出,共同进步.","text":"IOC与AOP是spring库的两大思想,能够极大的降低程序耦合程度,提高了程序之间的逻辑性,便于日后维护及其扩展,本文主要从概念,原因,内容三个方面进行阐述,不正之处,忘大家指出,共同进步. 一.概述 二.控制反转 2.1 概念 2.2 原因 2.3 内容 2.3.1 控制反转 2.3.2 依赖注入 三.面向切面 3.1 概念 3.2 原因 3.3 内容 一.概述 spring 是一个库,提供了一个软件框架,让各个程序之间的逻辑更清晰,配置更加灵活,耦合度大大降低,因此大家习惯性叫spring框架.实现上述效果主要运用到的思想就是控制反转(Inversion Of Control简称IOC)/依赖注入(Dependence-Injection简称DI) 和 AOP(Aspect-Oriented Programing).其中IOC主要负责数据的注入,AOP主要是负责程序/逻辑的注入.两种思想的最终目的降低程序的耦合程度,提高程序的后期维护与扩展.都是让开发者只专注于核心点. 二.控制反转2.1 概念 程序只需要专注业务逻辑,而不需要控制非核心的功能,例如依赖对象的创建及其对象关系的维护等,把这些功能的控制权交给容器管理,这也就是控制反转的来源.当需要对象/数据的时候,告诉IOC容器,IOC则会把对应的数据(依赖对象,对象)传到给程序当中,程序只需要设置一个接收接口进行数据接收.这就是依赖注入的过程. 注:(1)控制反转:指的是获取依赖对象的方式发生了反转,原先是程序主动创建,现在是等待IOC容器传递过来． (2)正转:程序当中的对象中主动控制去直接获取依赖对象 2.2 原因 有利于对象/类之间的耦合程度下降,因为从上面我们可以看到,程序是被动接收数据.IOC帮对象找依赖对象. 2.3 内容2.3.1 控制反转 (1)谁控制谁:IOC容器控制了依赖对象 (2)控制什么:IOC容器控制了依赖对象(包括对象、资源/文件、常量数据)的获取和生命周期。 (3)为何是反转:从前都是程序当中的对象主动控制依赖对象的获取,现在是需要IOC控制依赖对象的产生/传递. (4)哪些方面反转了：程序当中的对象获取依赖对象的方式反转了。 2.3.2 依赖注入 (1)谁依赖于谁：程序依赖IOC容器； (2)为什么需要依赖：程序为了更k快实现某个功能,需要(调用外部对象的方法,这就产生了程序依赖外部对象； (3)谁注入谁：IOC查找到依赖对象注入到程序当中； (4)注入了什么：就是注入程序中某个对象所需要的依赖对象（包括对象、资源/文件、常量数据） 三.面向切面3.1 概念 AOP思想目的是让程序专注于核心功能,其余非核心功能通过AOP进行代码注入.例如在进行业务处理,AOP能够自动帮程序进行事务的开启,关闭.不需要人为显式定义事务的开启,关闭. 3.2 原因 (1)OOP(面向对象编程)是专注于上下关系(子父类),不适合定义左右关系(兄弟类) (2)提高程序模块化比重,降低耦合程度. 3.3 内容 AOP其实就是OOP(面向对象编程)的细化,补充.AOP主要是为分散对象引入公共行为(例如为每个类型打印日志),因为分散的对象分布在对象的各个层次.分布在各个层次的公共的非核心代码就叫做横切代码,可以对这些横切代码进行单独封装,—&gt;体现了面向切面。以便日后重用.之后按需注入.这就是AOP的过程.","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"知识点","slug":"java/知识点","permalink":"http://yoursite.com/categories/java/知识点/"}],"tags":[{"name":"IOC","slug":"IOC","permalink":"http://yoursite.com/tags/IOC/"},{"name":"AOP","slug":"AOP","permalink":"http://yoursite.com/tags/AOP/"}]},{"title":"java容器与Docker理解","slug":"java容器与Docker理解","date":"2018-07-23T03:41:38.000Z","updated":"2018-11-13T02:31:00.000Z","comments":true,"path":"2018/07/23/java容器与Docker理解/","link":"","permalink":"http://yoursite.com/2018/07/23/java容器与Docker理解/","excerpt":"java容器类主要为了解决不同类的对象的存储、操作问题.Docker则是为了减少应用配置复杂度,保持开发,测试,生产环境的一致性.上述两个问题主要是从概念,原因,组成部分三个部分来进行阐述.文章涉及的概念问题比较多,需要好好屡屡.","text":"java容器类主要为了解决不同类的对象的存储、操作问题.Docker则是为了减少应用配置复杂度,保持开发,测试,生产环境的一致性.上述两个问题主要是从概念,原因,组成部分三个部分来进行阐述.文章涉及的概念问题比较多,需要好好屡屡. 一.java容器 1.1 概念 1.2 原因 1.3 作用 1.4 组成部分 1.4.1 collection 接口 1.set 2.List 3.Queue 1.4.2 Map接口 1.5 并发容器 1.5.1 CopyOnWrite容器 1.5.2 ConcurrentLinkedQueue 1.5.3 ConcurrentHashMap与锁分段技术 1.6 迭代器 二.Docker 2.1 概念 2.2 原因 2.2.1 减少配置的复杂度。 2.2.2 保证开发、测试和生产环境一致。 2.3 容器和 VM 容器 2.4 组成部分 2.4.1 Docker 2.4.2 镜像 2.4.3 容器 2.4.4 Docker 仓库 参考文献 一.java容器1.1 概念 容器也是一个类,只不过是用来存放其他类的对象的类.其中对象之间的性质/类型可以不一样.因此容器也可以叫做集合.也就是用来存储数据的数据结构.容器可以分成两类:collection接口范围的容器,Map接口类型的容器. 1.2 原因为了弥补数组的两大缺陷: (1)长度难以扩充; (2)数组中元素类型必须相同. 1.3 作用 容器可以管理对象的生命周期、对象与对象之间的依赖关系。通过一个配置文件或者一些注解来配置实现，通过定义对象的名称、产生方式，哪个对象产生后必须设定成为那个对象的属性等，容器启动之后所有对象都可以直接取用，不用编写任何一行程序来产生对象或者建立对象与对象之间的依赖关系. 1.4 组成部分1.4.1 collection 接口 1.set Set不保存重复的元素，通常用于快速查找元素。值得一提的是，Set具有与Collection完全一样的接口，没有任何额外的功能。 存入的元素必须定义equals()方法 Set类型 使用场景 底层实现 HashSet 快速查找，元素必须定义hashCode() 链表 TreeSet 保持次序，元素必须实现Comparable接口 红-黑树结构 LinkedHashSet 维护次序的HashSet, 元素必须定义hashCode() 链表 2.List 可以保存重复元素，两种类型的List，ArrayList和LinkedList List类型 优点 缺点 底层实现 ArrayList 随机访问元素较快 中间元素的插入和删除较慢 数组 LinkedList 中间元素的插入和删除，顺序访问的优化 随机访问元素较慢 双向链表 3.Queue 除了并发应用，Queue仅有的两个实现是LinkedList和PriorityQueue, 其中LinkedList同时实现了List, Deque接口。它们的差异在于排序行为而不是性能。 1.4.2 Map接口 Map类型 使用场景 底层实现 HashMap 快速查询 散列表 LinkedHashMap 迭代遍历具有顺序(插入顺序 or 最近最少使用) 链表 TreeMap 具有排序，唯一可以返回子树的Map(subMap()) 红-黑树结构 WeakHashMap 弱键映射，映射之外无引用的键，可以被垃圾回收 散列表 ConcurrentHashMap 线程安全的Map 链表 IdentityHashMap 使用==代替equals()对键进行排序，专位解决特殊问题 链表 PS:不要使用过时的容器 如Vector Enumeration Hashtable Stack(没错，这就是java最初的糟糕设计，实际中使用栈的话推荐LinkedList) 1.5 并发容器1.5.1 CopyOnWrite容器 写时复制的容器。通俗的理解是当我们往一个容器添加元素的时候，不直接往当前容器添加，而是先将当前容器进行Copy，复制出一个新的容器，然后新的容器里添加元素，添加完元素之后，再将原容器的引用指向新的容器。这样做的好处是我们可以对CopyOnWrite容器进行并发的读，而不需要加锁，因为当前容器不会添加任何元素。所以CopyOnWrite容器也是一种读写分离的思想，读和写不同的容器。 1.5.2 ConcurrentLinkedQueue 在并发编程中，有时候需要使用线程安全的队列或列表。通常实现线程安全有两种方式，一种是使用阻塞算法，一种是使用非阻塞算法。非阻塞算法实现基础为循环CAS(Compare and Swipe 比较和交换)。 ConcurrentLinkedQueue技术上的实现与CopyOnWriteArrayList与Copy类似，但是容器只有部分内容而不是整个容器可以被复制和修改。ConcurrentLinkedQueue有head节点和tail节点组成，每个节点由节点元素(item)和指向下一个结点(next)的引用组成。节点之间通过next关联起来，形成一张链表结构的队列。 1.5.3 ConcurrentHashMap与锁分段技术1.ConcurrentHashMap 线程安全且高效的HashMap。多线程环境下，使用非线程安全的HashMap会导致死循环，而如文章中建议的那样，HashTable这种过时容器效率低下(使用synchronized来保证线程安全)。ConcurrentHashMap使用锁分段技术，大大提高了并发使用的效率。2.锁分段技术 假设容器有多把锁，每一把锁用于锁容器其中一部分数据，当多线程访问容器不同数据段数据时，线程间就不存在锁竞争，从而提高并发访问效率。 1.6 迭代器 迭代器统一了对容器的访问方式，同时创建它的代价很小。值得注意的是，Iterator只能单向移动。要想双向移动,需要ListIterator是Iterator的扩展之内，用于各种List类访问，支持双向移动。 一般步骤: (1)通过容器的iterator()方法拿到容器的迭代器 (2)迭代器的next()获取下一个元素 (3)hasNext()判断是否还有元素 (4)remove()删除指定元素. 二.Docker2.1 概念 Docker就是一个基于软件层面的轻量虚拟机,与基于硬件的VM不同,它直接复用了 Host 主机的 OS,在 Docker Engine 层面实现了调度和隔离重量一下子就降低了好几个档次。 Docker 的容器利用了LXC，管理利用了 namespaces 来做权限的控制和隔离， cgroups 来进行资源的配置，并且还通过aufs 来进一步提高文件系统的资源利用率。 其中的 aufs 是个很有意思的东西，是 UnionFS 的一种。他的思想和 git 有些类似，可以把对文件系统的改动当成一次 commit 一层层的叠加。这样的话多个容器之间就可以共享他们的文件系统层次，每个容器下面都是共享的文件系统层次，上面再是各自对文件系统改动的层次，这样的话极大的节省了对存储的需求，并且也能加速容器的启动 2.2 原因2.2.1 减少配置的复杂度。 比如我现在想用MySQL那我就找个装好MySQL的容器，运行起来，那么我就可以使用 MySQL了。那么我直接装个 MySQL不就好了，何必还需要这个容器这么诡异的概念？话是这么说，可是你要真装MySQL的话可能要再装一堆依赖库，根据你的操作系统平台和版本进行设置，有时候还要从源代码编译报出一堆莫名其妙的错误，可不是这么好装。而且万一你机器挂了，所有的东西都要重新来，可能还要把配置在重新弄一遍。但是有了容器，你就相当于有了一个可以运行起来的虚拟机，只要你能运行容器，MySQL的配置就全省了。而且一旦你想换台机器，直接把这个容器端起来，再放到另一个机器就好了。硬件，操作系统，运行环境什么的都不需要考虑了。 2.2.2 保证开发、测试和生产环境一致。 若果利用容器的话，那么开发直接在容器里开发，提测的时候把整个容器给测试，测好了把改动改在容器里再上线就好了。通过容器，整个开发、测试和生产环境可以保持高度的一致。此外容器也和VM一样具有着一定的隔离性，各个容器之间的数据和内存空间相互隔离，可以保证一定的安全性。 2.3 容器和 VM容器 可见容器是在操作系统层面上实现虚拟化，直接复用本地主机的操作系统，而传统方式（VM）则是在硬件层面实现，VM 的 Hypervisor 需要实现对硬件的虚拟化，并且还要搭载自己的操作系统，自然在启动速度和资源利用率以及性能上有比较大的开销。 2.4 组成部分2.4.1 Docker 之前概念部分说过Docker就是一个基于软件层面的轻量虚拟机，可以多平台运行,用来打包软件运行环境和基于运行环境开发的软件。 2.4.2 镜像 我们把打包好的运行环境（即系统）和基于运行环境开发的软件生成的东东叫镜像（image），类似我们装windows系统使用过的GHO、ISO等系统镜像,可以在多平台运行. 要生成自定义镜像，有个东东来了，那就是Dockerfile，简单来说，Dockerfile是一个脚本，用来编写要自定义的镜像该如何去生成的步骤，比如说，要生成一个自定义的image镜像，是基于ubuntu的，那么在Dockerfile的最前头应该有这么一句话 FROM ubuntu等等，具体的Dockerfile的指令操作，在此不细说，大家只要明白Dockerfile是个用来描述自定义镜像的生成步骤的脚本，啰嗦一句，每个Dockerfile中操作的指令都会让docker在执行Dockerfile的时候生成一个临时的layer，多个layer构成一个image镜像，如果命令有改动，会生成新的layer，没有改动，就只保留一份layer. docker打包系统生成image镜像，并不是在虚拟机中，而是对我们正在运行的系统进行打包操作，这点要注意. 镜像文件是只读文件.静态的 2.4.3 容器 镜像运行的地方就叫做Container–容器，因此Docker不单单是只有容器，其中Container是被docker管理的,当要在container中运行image镜像的时候，docker会拷贝一个image到container中，原本的image还是被docker管理着，运行在container中的image是一个副本，准确来说，不是副本，这里不深入说，有兴趣的可以去搜搜cgroup，docker利用了linux系统的这个弄的。不深入来看，可以理解为一个image副本.因为镜像自身是只读的。 容器从镜像启动的时候，Docker会在镜像的最上层创建一个可写层，镜像本身将保持不变。要保存修改,那么就将当前镜像进行打包成新的镜像。 每个容器都是相互隔离的、保证安全的平台 2.4.4 Docker 仓库 集中存放镜像文件的场所。有时候会把仓库和仓库注册服务器（Registry）混为一谈，并不严格区分。实际上，仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签（tag）。 仓库分为公开仓库（Public）和私有仓库（Private）两种形式。 最大的公开仓库是 Docker Hub，存放了数量庞大的镜像供用户下载。国内的公开仓库包括Docker Pool 等，可以提供大陆用户更稳定快速的访问。当然，用户也可以在本地网络内创建一个私有仓库。 当用户创建了自己的镜像之后就可以使用 push 命令将它上传到公有或者私有仓库，这样下次在另外一台机器上使用这个镜像时候，只需要从仓库上pull 下来就可以了。 参考文献(1)https://blog.csdn.net/albertfly/article/details/52403393(2)https://blog.csdn.net/S_gy_Zetrov/article/details/78161154","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"知识点","slug":"java/知识点","permalink":"http://yoursite.com/categories/java/知识点/"}],"tags":[{"name":"容器","slug":"容器","permalink":"http://yoursite.com/tags/容器/"},{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"}]},{"title":"泛型系列之泛型初探(一)","slug":"泛型粗识","date":"2018-07-21T01:06:00.000Z","updated":"2018-11-13T02:58:16.000Z","comments":true,"path":"2018/07/21/泛型粗识/","link":"","permalink":"http://yoursite.com/2018/07/21/泛型粗识/","excerpt":"泛型是java的重要知识内容之一,泛型的目的主要是为了避免类型的强制转换,提高代码复用率,提升性能.本文主要从泛型的概念,含义,通配符,泛型类,泛型方法,类型擦除,类型体系,实践技巧八个方面进行阐述,初识泛型难免有不正确之处,希望大家可以指出.共同进步.","text":"泛型是java的重要知识内容之一,泛型的目的主要是为了避免类型的强制转换,提高代码复用率,提升性能.本文主要从泛型的概念,含义,通配符,泛型类,泛型方法,类型擦除,类型体系,实践技巧八个方面进行阐述,初识泛型难免有不正确之处,希望大家可以指出.共同进步. 一.含义 二.原因 三.类型擦除 3.1 过程 3.2 注意事项 3.3 原因 四.泛型类 五.通配符(类型参数) 六.泛型方法 6.1 类型通配符（?）与泛型方法 6.1.1 总体 6.1.2 形式 6.1.3 场景 七.类型体系(重要) 7.1 两个维度 7.2 注意事项 八.java不能创建一个确切的泛型类型的数组 九.实践技巧 泛型知识体系如下图: 一.含义 在类建立的时候,不确定类中属性的具体类型,在声明和实例化对象时候指定属性的类型.泛型可以是具有占位符(类型参数)的接口,类,结构,参数,方法.看如下代码:12345678910# 建立类public class Box&lt;T&gt; &#123; // T stands for \"Type\" private T t; public void set(T t) &#123; this.t = t; &#125; public T get() &#123; return t; &#125;&#125;#外部声明#新建Box类实例化的对象integerBox,同时指定属性类型是Integer.Box&lt;Integer&gt; integerBox = new Box&lt;Integer&gt;(); 二.原因(1).实现代码的复用 例如处理int和double数据,处理逻辑一样,只是类型不一样,因为java是强类型语言,因此在泛型出来之前,需要为每个数据类型单独构建处理方法.有了泛型之后,可以设置参数的类型是T,在实例化对象的时候才会指定,这样实现了代码复用,代码如下:12public class deal(T)&#123;略&#125;#其中T就是可以在实例化对象中指定相应类型. (2).在编译时就进行类型检查,提升程序安全性. 对于确定是违反相关原则的地方，会给出编译错误。当编译器无法判断类型的使用是否正确的时候，会给出警告信息.(3).减少类型的强制转换,提高性能. 最明显就是创建的集合类,如list&lt;T&gt;之类,但是泛型对数组的支持不是很好.后面会讲到 三.类型擦除3.1 过程 泛型在编译的过程之中,编译器将会把泛型当中指定的数据类型进行擦除,也就是会把类型参数去掉. 例如:List&lt;object&gt;与List&lt;string&gt;,编译器会把他们都转换成List,这就导致JVM(运行期间)看到的只有List,也就是JVM对泛型的类型参数不敏感. 这就说明java其实是伪泛型,因为在编译过程当中就采取了类型擦除. 3.2 注意事项 类型擦除不是把对象的数据类型擦擦除了,而是使用Obeject类型去引用之前定义的泛型数据,这样就实现了类型擦除.因为Object是所有数据类型的父类.这样才实现了类型统一 3.3 原因 为什么通过类型擦除来实现泛型,目的就是为了实现向后兼容(也就是希望1.5版本的文件可以在8.0版本中运行)–当然指的是二进制兼容，而非源码兼容.并且成本也低,其实现方式就是将非泛型容器扩展为泛型容器,记住是扩展. 四.泛型类泛型类与普通java类只是多了一个&lt;类型参数&gt;.其中12345678910111213#其中X,Y,Z是类型参数,可以定义为变量的类型,返回值,方法参数#但不能是静态变量的.也不能创建对象,如new X()之类的.class ClassTest&lt;X extends Number, Y, Z&gt; &#123; private X x; private static Y y; //编译错误，不能用在静态变量中 public X getFirst() &#123; //正确用法 return x; &#125; public void wrong() &#123; Z z = new Z(); //编译错误，不能创建对象 &#125;&#125; 泛型类并没有自己独有的Class类对象。比如并不存在List&lt;String&gt;.class或是List&lt;Integer&gt;.class，而只有List.class 五.通配符(类型参数)(1)含有通配符的泛型如下:12345678public void wildcard(List&lt;?&gt; list) &#123;//因为通配符?所表示的类型未知,不能对List进行操作. list.add(1);//因为通配符?所表示的类型未知,不能创建对象. ArrayList&lt;?&gt; arr= new ArrayList&lt;?&gt;()//这样才是对的,指定了参数的类型 List&lt;String&gt; list = new ArrayList&lt;String&gt;();&#125; 因为只含有通配符?的泛型,其中List&lt;?&gt;的参数类型的上限默认是Object,因此List&lt;?&gt;中的元素可以调用Object类的方法,其中add方法不在object类当中,所以会有上面的错误.同时也就引出了有上下界的泛型.(2)含有通配符和上下界泛型如下:123456#表明?可以是Number类及其某一子类class ClassTest(List&lt;? extends Number&gt; list)&#123;//因为?的父类是Number,有initValue()方法//根据继承法则,这个是成立.list.initValue(1);&#125; 六.泛型方法泛型方法是在类当中进行定义的,可以是泛型类,代码如下:1234public class A&lt;T&gt; &#123; //泛型类定义 public void func(T t) &#123; //这个就是泛型方法 &#125;&#125; 6.1 类型通配符（?）与泛型方法6.1.1 总体 能用类型通配符（?）解决的问题都能用泛型方法解决，并且泛型方法有时可以解决的更好. 6.1.2 形式(1)类型通配符: public void func(List&lt;? extends A&gt; list);—前面没有&lt;&gt;之类的就是通配符方法(2)泛型方法: 12//修饰符 &lt;类型参数列表&gt; 返回类型 方法名(形参列表) &#123; 方法体 &#125;public &lt;T extend A&gt; void func(List&lt;T&gt; list)&#123;略&#125; PS:通配符?是只读的,但是T是可读可写 6.1.3 场景 (1)只读情景-使用通配符(安全性更好);要修改–使用泛型方法,如下代码123public &lt;T&gt; void func(List&lt;T&gt; list, T t) &#123; list.add(t);&#125; (2)方法中参数之间或者参数与返回值之间存在依赖,使用泛型方法更好,代码如下:12#其中第一个参数依赖第二个参数public &lt;T&gt; void func(List&lt;? extends T&gt; list, T t)&#123;&#125; 七.类型体系(重要)7.1 两个维度 引入泛型之后的类型系统增加了两个维度,以一个代码来进行展开描述: 1public class ClassTest(List&lt;?&gt; list) (1)根据类型参数–?,来进行展开可以得到一个维度:类型参数继承于Object—使用继承机制; (2)根据泛型类和接口自身的继承体系结构:List接口继承于collection接口. 7.2 注意事项 当泛型类的类型声明如果有通配符,子类型的分析可以从上面两个维度进行阐述 12//例如:因此List&lt;?&gt; arr的子类型:ArrayList&lt;?&gt;,ArrayList&lt;object&gt; 八.java不能创建一个确切的泛型类型的数组看如下代码:1List&lt;String&gt;[] ls = new ArrayList&lt;String&gt;[10] 上述代码是错误的,而使用通配符(不确切的类型)创建泛型数组是可以的，如下面代码：1List&lt;?&gt;[] ls = new ArrayList&lt;?&gt;[10] 同时,下面这样也是可以的：12//可以创建这个属性,但不是数组类型的List&lt;String&gt;[] ls = new ArrayList[10] 为什么java不能创建一个确切的泛型类型的数组,解释这个问题,我们可以先假设这个确切类型的泛型数组成立.先看一个代码例子:123456List&lt;String&gt;[] stringLists=new List&lt;String&gt;[1];//Arrays.asList()将数组转化为listList&lt;Integer&gt; intList = Arrays.asList(40);Object[] objects = stringLists;objects[0]=intList;String s=stringLists[0].get(0); 程序在编译时,由于泛型的类型擦除，List&lt;Integer&gt;，List&lt;String&gt;与List在编译之后并没有区别，所以List&lt;String&gt;[]放入List&lt;Integer&gt;并不会产生ArrayStoreException(数组存储)异常。但是在运行的过程中,使用get()方法将数据读取的时候,就会发现,放在List&lt;Integer&gt;的List&lt;String&gt;,由于数据类型不一致,需要进行类型转换(一个string类型数组,一个是integer类型的List列表,两者转换会出现异常),将会抛出ClassCastException异常。 如果泛型没有限制类型。比如List s=new ArrayList[5];或者List&lt;?&gt;[] s=new ArrayList[5];还是可以使用的，应为没有了编译时的类型检查，需要开发者自己保证类型转换安全。 总之,最好避免使用泛型数组. 九.实践技巧 (1)在代码中避免泛型类和原始类型的混用。比如List和List不应该共同使用。这样会产生一些编译器警告和潜在的运行时异常。 (2)最好明确通配符的上下界。这样才方便判断哪些操作合法.(根据上下界,也就是父类),否则就会由于具体的类型是未知的，导致很多操作是不允许的。 (3)最好避免使用泛型数组。当需要类似数组的功能时候，使用集合类即可。 (4)重视编译器给出的警告信息。 ​","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"泛型","slug":"java/泛型","permalink":"http://yoursite.com/categories/java/泛型/"}],"tags":[{"name":"泛型","slug":"泛型","permalink":"http://yoursite.com/tags/泛型/"}]},{"title":"依赖注入系列之注解初探(一)","slug":"依赖注入系列之注解初探(一)","date":"2018-07-20T08:08:09.000Z","updated":"2018-11-13T03:11:54.000Z","comments":true,"path":"2018/07/20/依赖注入系列之注解初探(一)/","link":"","permalink":"http://yoursite.com/2018/07/20/依赖注入系列之注解初探(一)/","excerpt":"本文是依赖注入系列的第一篇,先讲述在java中,常用的注解有哪些及其使用场景.众所周知,注解,在java中是很重要的一个部分,主要是为了解决配置繁琐的问题,主要功能有两个:数据注入,代码检查.本文对java当中的注解知识,作了比较初步的认识,主要从原因,作用,内容,实现,应用,这五方面进行阐述,初学难免会有不正确之处,希望大家可以指出,共同进步.","text":"本文是依赖注入系列的第一篇,先讲述在java中,常用的注解有哪些及其使用场景.众所周知,注解,在java中是很重要的一个部分,主要是为了解决配置繁琐的问题,主要功能有两个:数据注入,代码检查.本文对java当中的注解知识,作了比较初步的认识,主要从原因,作用,内容,实现,应用,这五方面进行阐述,初学难免会有不正确之处,希望大家可以指出,共同进步. 一.原因 二.用途 三.内容 3.1 形式 3.2 元注解 3.2.1 @Retention 3.2.2 @Documented 3.2.3 @Target 3.2.4 @Inherited 3.2.5 @Repeatable 3.3 自定义注解—数据注入 3.3.1 注解属性 3.4 java注解–检查代码 3.4.1 @Deprecated 3.4.2 @Override 3.4.3 @SuppressWarnings 3.4.4 @SafeVarargs 3.4.5 @FunctionalInterface 四.实现 4.1 获得注解 4.2 注解注入 4.2.1 注入到成员变量上 4.3 反射步骤 五.应用 首先将注解的知识体系贴出: 一.原因 减少配置的复杂度,可以尽量避免使用xml文件进行配置 二.用途 1.结合编译器,实现代码格式的检查(例如@override就可以检查所注解的地方是否实现了重写)–java注解 2.利于配置信息(数据)注入(主要是利用反射机制)—到方法,变量(参数),类当中—自定义注解 注意事项: 注解不是代码的一部分,失去这个注解代码也能运行,不会报错,只是结果不是我们需要的,其实注解就是一个标签. 三.内容3.1 形式123#注解的基本形式.public @interface TestAnnotation &#123;&#125; 3.2 元注解 对其他注解进行配置(或者解释说明)的注解,总共有如下几类: @Retention、@Documented、@Target、@Inherited、@Repeatable 5 种。 3.2.1 @Retention1.作用 主要是对注解的存在时间进行说明.参数见图中的例子:123@Retention(RetentionPolicy.RUNTIME)public @interface TestAnnotation &#123;&#125; 2.@Retention参数: (1)RetentionPolicy.SOURCE 注解只在源码阶段保留，在编译器进行编译时它将被丢弃忽视。只存在java文件当中–编译之前会丢弃. (2)RetentionPolicy.CLASS 注解只被保留到编译进行的时候，它并不会被加载到 JVM 中。会保存在class文件当中—可以被编译,编译之后,运行之前会被丢弃. (3)RetentionPolicy.RUNTIME 注解可以保留到程序运行的时候，它会被加载进入到 JVM 中，所以在程序运行时可以获取到它们.会保留到exe文件当中,运行之后会被丢弃. 3.2.2 @Documented1.作用 主要是将@Documented注解的对象打包在javadoc当中. PS:一般情况下,注解都不会被收录在javadoc当中,除非是@Documented才会正式收录.2.形式文件打包在打包之后,进入doc文件夹,打开index.html文件可以看到: 3.2.3 @Target1.作用: 描述注解适用的对象2.@Target参数:12345678ElementType.ANNOTATION_TYPE 可以给一个注解进行注解ElementType.CONSTRUCTOR 可以给构造方法进行注解ElementType.FIELD 可以给属性进行注解ElementType.LOCAL_VARIABLE 可以给局部变量进行注解ElementType.METHOD 可以给方法进行注解ElementType.PACKAGE 可以给一个包进行注解ElementType.PARAMETER 可以给一个方法内的参数进行注解ElementType.TYPE 可以给一个类型进行注解，比如类、接口、枚举 3.2.4 @Inherited1.特点: 一个注解可以对应多个对象2.作用: 让注解可以被继承,减少注解的重复数量.3.形式1234567891011@Inherited@Retention(RetentionPolicy.RUNTIME)#上面两个是元注解#下面是自定义注解的定义@interface Test &#123;&#125;#下面就是说明@Test这个注解服务于类A,#但是由于类B是类A的子类,并且@Test这个注解是可继承的(@Inherited),#因此@Test也服务于类B@Testpublic class A &#123;&#125;public class B extends A &#123;&#125; 3.2.5 @Repeatable1.特点: 多个不同取值的注解可以重复应用在一个类中(只有@Repeatable,没有@Inherited)2.作用: 为了满足拥有多个不同取值的注解的使用.3.形式12345678910111213141516@interface Persons &#123; Person[] value();&#125;#@Repeatable参数是容器注解(存放很多注解的地方),名称是Persons,#但是@Repeatable又注解了Person,#同时Persons的属性是Person[]---一个Person类型注解数组,#因此Person注解可以实现多个取值,只不过是存放在Persons这个容器注解当中.@Repeatable(Persons.class)@interface Person&#123; String role default \"\";&#125;@Person(role=\"artist\")@Person(role=\"coder\")@Person(role=\"PM\")public class SuperMan&#123;&#125; 3.3 自定义注解—数据注入3.3.1 注解属性1.含义: (1)注解主要是由成员变量构成,其中成员变量只有属性没有方法. (2)属性类型的构成:8种基本数据类型,类,接口,注解及其它们的的数组形式(例如在@repeated中的容器注解Persons,它的属性就是注解数组.) (3)注解的属性值:可以设置默认值,需要添加相应的关键字–default. (4)注解的使用情况:最好是显示使用,不要取巧.2.形式: 首先观察自定义注解:12345678910@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)public @interface TestAnnotation &#123; public int id() default -1; public String msg() default \"Hi\"; Person[] value();&#125;public @interface Person&#123; String role default \"88\";&#125; 之后使用TestAnnotation注解:1234#最好显式设置注解成员变量的新值@TestAnnotation(id=1,msg=\"dd\",@person(role=\"66\"))public class SuperMan&#123;&#125; 3.4 java注解–检查代码3.4.1 @Deprecated作用: 在进行版本迭代的时候,有些方法/类设计的不好,已经有新的方法或者api代替,但是为了兼容之前的版本,因此使用@depcreated,标记改程序过时,以后,@Deprecated所标记的方法/类可能会删除,最好不要使用,方法被@Deprecated标记后的效果. 3.4.2 @Override 在某些情况下,为了实现重写父类的某个方法,同时有利于代码的检查.看是否覆写了父类对应的方法(检查名称属性). 3.4.3 @SuppressWarnings1.作用 压制警告,为了减少不必要的警告,例如调用过时的方法会产生警告,若定使用该方法,因此需要进行警告压制.例子如下:123456@SuppressWarnings(\"deprecation\")public void test1()&#123; Hero hero = new Hero(); hero.say(); hero.speak();&#125; 2.参数12345678910111213141516171819all---（抑制所有警告）boxing---（抑制装箱、拆箱操作时候的警告）cast---（抑制映射相关的警告）dep-ann---（抑制启用注释的警告--deprecated annotation)）deprecation ---（抑制过期方法警告）fallthrough---（抑制确在switch中缺失breaks的警告）finally ---（抑制finally模块没有返回的警告）hiding ---（抑制本地隐变量）incomplete---忽略没有完整的switch语句)nls----忽略非nls格式的字符)null ---忽略对null的操作rawtypes---使用generics时忽略没有指定相应的类型restriction---抑制禁止引用有关的警告serial---忽略在serializable类中没有声明serialVersionUID变量)static---抑制不正确的静态访问方式警告)synthetic---抑制子类没有按最优方法访问内部类的警告）unchecked---抑制没有进行类型检查操作的警告）unqualified--- （抑制没有权限访问的域的警告）unused---抑制没被使用过的代码的警告 3.4.4 @SafeVarargs 参数安全类型注解—safe var arguments,就是将一个精度高的数值,赋值给精度低的类型变量. 3.4.5 @FunctionalInterface 声明所注释的接口是一个函数式编程接口.为了就是更方便转换成lambda表达式.1234@FunctionalInterfacepublic interface Runnable &#123; public abstract void run();&#125; 四.实现 上面说了,注解的作用有两个,代码检查(可以用来做测试.)与数据注入,其中代码检查–使用java注解就可以实现,那么数据注入应该使用什么技术呢?答案就是使用反射机制. 4.1 获得注解 1.一般是先在对应的层面(类,变量,方法)判断是否存在注解—isAnnotationPresent(注解名.class) 2.基于上步之后,如果存在,先初始化对应的注解对象,之后使用getAnnotations()或者getAnnotation(注解名.class)获得对应的注解. 3.之后使用注解的属性值.12345678910111213141516171819202122232425262728293031323334353637383940414243444546#在类层面使用TestAnnotation注解@TestAnnotation(msg=\"hello\")public class Test &#123;#在变量层面使用@Check注解 @Check(value=\"hi\") int a;#在方法层面(testMethod()方法)使用@perform注解 @Perform public void testMethod()&#123;&#125;#在test1()方法层面使用suppressWarning注解 @SuppressWarnings(\"deprecation\") public void test1()&#123; Hero hero = new Hero(); hero.say(); hero.speak(); &#125; public static void main(String[] args) &#123; #在类的层面判断是否存在注解 boolean hasAnnotation = Test.class.isAnnotationPresent(TestAnnotation.class); #如果有注解,根据注解名获得对应注解 if ( hasAnnotation ) &#123; TestAnnotation testAnnotation = Test.class.getAnnotation(TestAnnotation.class); //获取类的注解 System.out.println(\"id:\"+testAnnotation.id()); System.out.println(\"msg:\"+testAnnotation.msg()); &#125; #在变量层面上检查是否存在注解 #但是需要先根据变量名获取对应位置的变量 Field a = Test.class.getDeclaredField(\"a\"); #由于是private类型变量需要先设置访问权限 a.setAccessible(true); //获取该变量上的注解 Check check = a.getAnnotation(Check.class); #如果存在,打出成员变量值 if ( check != null ) &#123; System.out.println(\"check value:\"+check.value()); &#125; #获取方法上的注解,需要使用的一个变量,存储对应位置的方法名. Method testMethod = Test.class.getDeclaredMethod(\"testMethod\"); if ( testMethod != null ) &#123; // 获取方法中的注解(注解有多个) Annotation[] ans = testMethod.getAnnotations(); #依次输出对应的注解名称 for( int i = 0;i &lt; ans.length;i++) &#123; System.out.println(\"method testMethod annotation:\"+ans[i].annotationType().getSimpleName()); &#125; &#125; 4.2 注解注入4.2.1 注入到成员变量上在成员变量上使用注解123456789public class PersonDao &#123; @InjectPerson(username = \"zhongfucheng\",age = 20) private Person person; public Person getPerson() &#123; return person; &#125; public void setPerson(Person person) &#123; this.person = person; &#125;&#125; 编写注入工具12345678910111213141516171819202122232425262728293031323334//1.得到想要注入的属性 Field field = PersonDao.class.getDeclaredField(\"person\"); //2.得到属性的具体对象 Person person = (Person) field.getType().newInstance(); //3.得到属性上的注解 Annotation annotation = field.getAnnotation(InjectPerson.class); //4.得到注解的属性【注解上的属性使用方法来表示的】---记住是getMethod()方法获得 Method[] methods = annotation.getClass().getMethods(); //5.将注入的属性填充到person对象上 for (Method method : methods) &#123; //5.1得到注解属性的名字 String name = method.getName(); //查看一下Person对象上有没有与之对应的写方法--便于将获得的注解的成员变量值写入到属性当中 try &#123; //如果有写方法 PropertyDescriptor descriptor = new PropertyDescriptor(name, Person.class); //得到Person对象上的写方法 Method method1 = descriptor.getWriteMethod(); //得到注解上的值(其中invoke(类/注解/接口的实例名称,其中方法的对应参数)--就是执行对应的方法) //invoke()方法就是用来执行指定对象的方法 Object o = method.invoke(annotation, null); //填充person对象,因为只有setPerson()方法有参数 method1.invoke(person, o); &#125; catch (IntrospectionException e) &#123; //如果没有想对应的属性，继续循环 continue; &#125; &#125; //循环完之后，person就已经填充好数据了 //6.把person对象设置到PersonDao中 PersonDao personDao = new PersonDao(); #因为是私有变量private,因此需要设置访问权限. field.setAccessible(true); field.set(personDao, person); System.out.println(personDao.getPerson().getUsername()); 4.3 反射步骤 (1)首先获得Class对象， (2)然后实例化对象，获得类的属性、方法或者构造函数—为了访问属性(getXXX()方法),调用方法(setXXX()方法) (3)最后访问属性、调用方法、调用构造函数创建对象。其中invoke()方法就是用来执行指定对象的方法。 五.应用使用注解完成测试123456789101112131415161718192021222324252627package ceshi;import ceshi.Jiecha;public class NoBug &#123; @Jiecha public void suanShu()&#123; System.out.println(\"1234567890\"); &#125; @Jiecha public void jiafa()&#123; System.out.println(\"1+1=\"+1+1); &#125; @Jiecha public void jiefa()&#123; System.out.println(\"1-1=\"+(1-1)); &#125; @Jiecha public void chengfa()&#123; System.out.println(\"3 x 5=\"+ 3*5); &#125; @Jiecha public void chufa()&#123; System.out.println(\"6 / 0=\"+ 6 / 0); &#125; public void ziwojieshao()&#123; System.out.println(\"我写的程序没有 bug!\"); &#125;&#125; 上面的代码，有些方法上面运用了 @Jiecha 注解。这个注解是我写的测试软件框架中定义的注解。123456package ceshi;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;@Retention(RetentionPolicy.RUNTIME)public @interface Jiecha &#123;&#125; 然后，我再编写一个测试类 TestTool 就可以测试 NoBug 相应的方法了。12345678910111213141516171819202122232425262728293031323334353637383940414243444546package ceshi;import java.lang.reflect.InvocationTargetException;import java.lang.reflect.Method;public class TestTool &#123; public static void main(String[] args) &#123; // TODO Auto-generated method stub NoBug testobj = new NoBug(); Class clazz = testobj.getClass(); Method[] method = clazz.getDeclaredMethods(); //用来记录测试产生的 log 信息 StringBuilder log = new StringBuilder(); // 记录异常的次数 int errornum = 0; for ( Method m: method ) &#123; //只有被 @Jiecha 标注过的方法才进行测试(这个才是重点)---&gt; //这也就是注解的意义.(很重要)--就是做个标记,符合该标记的才会进行执行,之后输出相应的值.从而实现使用注解达到测试的的目的. if ( m.isAnnotationPresent( Jiecha.class )) &#123; try &#123; m.setAccessible(true); #依次执行NoBug实例testobj的方法,前提方法的参数为Null m.invoke(testobj, null); &#125; catch (Exception e) &#123; // TODO Auto-generated catch block //e.printStackTrace(); errornum++; log.append(m.getName()); log.append(\" \"); log.append(\"has error:\"); log.append(\"\\n\\r caused by \"); //记录测试过程中，发生的异常的名称 log.append(e.getCause().getClass().getSimpleName()); log.append(\"\\n\\r\"); //记录测试过程中，发生的异常的具体信息 log.append(e.getCause().getMessage()); log.append(\"\\n\\r\"); &#125; &#125; &#125; log.append(clazz.getSimpleName()); log.append(\" has \"); log.append(errornum); log.append(\" error.\"); // 生成测试报告 System.out.println(log.toString()); &#125;&#125; 可以得到结果:1234567812345678901+1=111-1=03 x 5=15chufa has error: caused by ArithmeticException/ by zeroNoBug has 1 error.","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"注解","slug":"java/注解","permalink":"http://yoursite.com/categories/java/注解/"}],"tags":[{"name":"注解","slug":"注解","permalink":"http://yoursite.com/tags/注解/"},{"name":"反射","slug":"反射","permalink":"http://yoursite.com/tags/反射/"}]},{"title":"数据建模","slug":"数据建模","date":"2018-07-17T11:27:40.000Z","updated":"2018-11-13T03:09:32.000Z","comments":true,"path":"2018/07/17/数据建模/","link":"","permalink":"http://yoursite.com/2018/07/17/数据建模/","excerpt":"由于知识图谱的构建是从数据库当中读取,其间还需要进行格式的转换,以neo4j为例,需要转换成csv或者json文件(需要apoc组件包支持),因此最开始的是数据库的构建,数据库的最基础就是数据建模.本文主要讲述了数据建模的一般过程,其中还涉及到navicat的一些操作技巧.","text":"由于知识图谱的构建是从数据库当中读取,其间还需要进行格式的转换,以neo4j为例,需要转换成csv或者json文件(需要apoc组件包支持),因此最开始的是数据库的构建,数据库的最基础就是数据建模.本文主要讲述了数据建模的一般过程,其中还涉及到navicat的一些操作技巧. 一.数据建模 1.1 建表 1.1.1 数据类型 二.模型使用 2.1 导出sql文件 三.建模小技巧 3.1 不同平台数据库的模型转换 3.2 模型推送到数据库中 3.3 从已有数据库当中导入模型 3.4 逆向数据库到模型 四.导出操作 查询创建工具 一.数据建模打开软件,点击模型选项 1.1 建表右边空白区新建表进行表的重命名添加字段/主键以新建genre表为例首先字段添加:添加外键:添加外键之后的效果如下建模完成效果: 1.1.1 数据类型1.int与integer (1)Integer是int的包装类，int则是java的一种基本数据类型 (2)Integer变量必须实例化后才能使用，而int变量不需要 (4)Integer实际是对象的引用，当new一个Integer时，实际上是生成一个指针指向此对象；而int则是直接存储数据值 (5)Integer的默认值是null，int的默认值是02.char与varchar (1)char是大小固定的字符串类型,不因存储的数值的大小而改变, (2)varchar是大小可变的字符串类型,所占内存是根据实际值的大小来确定 二.模型使用2.1 导出sql文件选择导出文件导出成功后关闭模型页面．数据库运行sql文件:注意如下选项:导入成功 三.建模小技巧3.1 不同平台数据库的模型转换点击下图对应的数据库,确认即可: 3.2 模型推送到数据库中(1)推送到新数据库 需要先进行新数据库的建立,之后点击同步到数据库．同步中比对的设置细化比对步骤运行查询之后,可以选择再次比对: 再次比对界面: PS:一般最好/安全的方法就是新建一个数据库,比对一次就可以了. 3.3 从已有数据库当中导入模型 从文件选项可以看到点击之后,得到导入源的设置设置数据库导入不满意,可以使用ctrl+z撤销导入操作. 3.4 逆向数据库到模型其实就是为当前数据库修改模型具体方法如下: 四.导出操作1.导出为csv文件桌面上就有五个文件: 数据库当中的导出设置文件: 可以导出来的会乱码,之后用记事本打开,以utf-8格式保存即可2.导出为json文件 一直默认即可,最后只需要进行导出设置文件保存3.导出为sql文件 类似上面,,这步需要注意: 最后只需要进行导出设置文件保存 查询创建工具","categories":[{"name":"知识图谱","slug":"知识图谱","permalink":"http://yoursite.com/categories/知识图谱/"},{"name":"项目实战","slug":"知识图谱/项目实战","permalink":"http://yoursite.com/categories/知识图谱/项目实战/"},{"name":"数据建模","slug":"知识图谱/项目实战/数据建模","permalink":"http://yoursite.com/categories/知识图谱/项目实战/数据建模/"}],"tags":[{"name":"navicat","slug":"navicat","permalink":"http://yoursite.com/tags/navicat/"},{"name":"数据建模","slug":"数据建模","permalink":"http://yoursite.com/tags/数据建模/"}]},{"title":"艺术创作的原因与过程","slug":"艺术创作的原因与过程","date":"2018-07-08T07:59:14.000Z","updated":"2018-11-13T03:13:08.000Z","comments":true,"path":"2018/07/08/艺术创作的原因与过程/","link":"","permalink":"http://yoursite.com/2018/07/08/艺术创作的原因与过程/","excerpt":"今天看了一本王兴国的&lt;艺术与人生纵横谈&gt;,看了一些,看到了艺术创作的原因与过程.因此特地进行记录下.","text":"今天看了一本王兴国的&lt;艺术与人生纵横谈&gt;,看了一些,看到了艺术创作的原因与过程.因此特地进行记录下. 一.艺术创作的原因 二.艺术创作的过程 一.艺术创作的原因 (1)表达情感的需要,屈原的&lt;九歌&gt;,&lt;问天&gt;.鲁迅的&lt;呐喊&gt;与&lt;彷徨&gt;都表达了对国家的热爱之情 (2)实用的需要,想想现在学美术,好多专业都出现了:工业设计,陶瓷艺术等等,这些都服务于社会的经济发展. (3)政治教化的需要,以前学政治就知道,文化对人的影响是潜移默化的,因此,政治则可以借助艺术,进行思想的教化. (4)是消遣的需要,文化与经济密不可分,也正是因为现代人的压力,节奏过于快速.导致需要一些放松的方式,艺术就是很好的形式.例如在周末的时候自己进行画画等等. 二.艺术创作的过程 (1)需要苦练内功,苦练内功,古代的例子很多:磨杵成针,池水墨化等等. (2)需要开阔视野,深入生活,为自己的艺术创作提供素材.例如罗立中的&lt;父亲&gt;,它的那幅画就是在偶遇到一个挑粪农民,新生感悟.借这幅画来表达对这农民的怜悯之情. (3)选择合适的表达形式:不同的艺术就有不同的表达形式.因此杂针对具体问题的时候,具分 (4)选择合适的创作环境,激发相应的灵感.例如王羲之的&lt;兰亭集序&gt;就是在与友人相聚的时候,即兴创作的.","categories":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/categories/随笔/"},{"name":"读后感","slug":"随笔/读后感","permalink":"http://yoursite.com/categories/随笔/读后感/"}],"tags":[{"name":"艺术","slug":"艺术","permalink":"http://yoursite.com/tags/艺术/"}]},{"title":"websocket原理认识","slug":"websocket原理认识","date":"2018-07-08T03:26:59.000Z","updated":"2018-11-13T02:54:06.000Z","comments":true,"path":"2018/07/08/websocket原理认识/","link":"","permalink":"http://yoursite.com/2018/07/08/websocket原理认识/","excerpt":"websocket是为了降低服务器状态更新告诉客户端 的代价而设计出的一种通信协议.本文主要是对它的产生原因及其实现原理进行了一些简单的总结","text":"websocket是为了降低服务器状态更新告诉客户端 的代价而设计出的一种通信协议.本文主要是对它的产生原因及其实现原理进行了一些简单的总结 一.基本概念 1.1 websocketEndpoint 1.2 套接字/socket 1.3 websocket与http 二.websocket 2.1 原因 2.2 实现原理 一.基本概念1.1 websocketEndpoint websocketEndpoint:websocket终端节点–&gt;其实就是服务器 1.2 套接字/socket 套接字:一种通信机制，用套接字中的相关函数来完成通信过程. 1.3 websocket与http http是单向连接,1.0时代,一个request,和response,请求结束了.1.1时代还会保持一个keep-alive,但是一个request只能有一个response,并且服务器端还是属于被动接受的. websocket可以说是一个全新的协议,但是为了兼容之前的http协议,因此就会有交集(握手阶段).两者的关系如下： 二.websocket2.1 原因 websocket:主要是实现服务器主动向客户端发/推送送消息.因为客传统的http协议,只有客户端是主动发起请求(使用传统的套接字socket),然后服务器就会返回接结果,但是假如服务器有变化,但是由于http协议的缺陷,导致服务器不能主动告诉客户端,之后发展出来的轮询(隔段时间,客户端再次发送请求(例如刷新),但是这样非常浪费时间,效率低,不及时.之后就有了websocket. 2.2 实现原理 当使用传统http协议建立连接,服务端有两层代理:ngnix和handle(分别类似接线员与客服),client与ngnix连接速度快,但是handle处理ngnix发过来的请求比较慢,因此当服务器有消息的时候,首先客服就会将更新的消息告诉接线员,之后接线员告诉客户端.与此同时客户端与接线员,接线员与客服分别是一直连接的.这样就实现了服务器的主动.","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://yoursite.com/categories/计算机网络/"},{"name":"知识点","slug":"计算机网络/知识点","permalink":"http://yoursite.com/categories/计算机网络/知识点/"}],"tags":[{"name":"套接字","slug":"套接字","permalink":"http://yoursite.com/tags/套接字/"},{"name":"socket","slug":"socket","permalink":"http://yoursite.com/tags/socket/"}]},{"title":"三个问题的认识","slug":"三个问题的认识","date":"2018-07-07T15:11:51.000Z","updated":"2018-11-13T03:07:54.000Z","comments":true,"path":"2018/07/07/三个问题的认识/","link":"","permalink":"http://yoursite.com/2018/07/07/三个问题的认识/","excerpt":"本文主要阐述的是对三个问题的认识,可能认识稍有不足,希望各位能够指出.","text":"本文主要阐述的是对三个问题的认识,可能认识稍有不足,希望各位能够指出. 一.警惕创业陷阱 二.尖兵与死士 三.提升社会性 一.警惕创业陷阱 根据从历代王朝的农民起义那里可以看出,在起义能够得到超过十倍收益的情况下才会真正发生.这样的做法才将腐朽的王朝取而代之,因此在创业的过程当中也是,要树立取而代之的是思想,方式就是不要只盯眼前的问题进行改良,而是看看有没有比当前问题成本低十倍的方法,这样才能实现冒尖. 改良型的方案往往是大公司来做,因为这样做的风险最低,取代型的方案往往是小公司,因为他们想想寻求突破. 其中微信就是个典型的例子.用户用微信的第一原因更多是省钱,免费,因此这能够比发短信的成本低十倍,因此微信 也就慢慢的取代了移动运营商的短信. 二.尖兵与死士 尖兵就是遇到危险情况,自己不能够解决的,可以暂时撤退,死士就是不管情况怎样,都会尽最大力气解决,舍身取义也在所不惜,虽然从精神上死士这种人更让人倾佩,但是实际上,大众所知道的更多的是尖兵,因为尖兵的伤亡损失率更低,社会舆论阻力更小,也更容易让人接受,媒体也更愿意报导. 三.提升社会性 每个人都有自己的 社会价值,把自己的社会价值展现好,就是社会性.一般线下社交能力强的往往在线上也会锦上添花,社交能力弱的,在线上可能有暂时的如意,但是终究会体现出它的弱点,但我不是说这样不好,因为不同的人就有不同的活法,因此假如真想让自己社交能力提升的:可以从如下两方面思考: 1.昵称,头像的设置,尽量真实,朋友圈的要发就发有意义的; 2.和别人交谈的时候,常常需要想想自己能和他交换什么价值的东西.而不是一味的索求对方的价值.","categories":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/categories/随笔/"},{"name":"认识","slug":"随笔/认识","permalink":"http://yoursite.com/categories/随笔/认识/"}],"tags":[{"name":"认识","slug":"认识","permalink":"http://yoursite.com/tags/认识/"}]},{"title":"janusGraph之配置与数据模型(二)","slug":"janusGraph之配置与数据模型(二)","date":"2018-07-07T13:49:32.000Z","updated":"2018-11-13T02:30:32.000Z","comments":true,"path":"2018/07/07/janusGraph之配置与数据模型(二)/","link":"","permalink":"http://yoursite.com/2018/07/07/janusGraph之配置与数据模型(二)/","excerpt":"本文是janusgraph的第二篇,主要是阐述了janusgraph的配置及其数据模型,配置那方面:主要是从janusfactory和janusgraph server(就是janusgraph集成的Gremlin-server).数据模型主要是从边标签,属性键,顶点标签.分别讲了这三方面的实例生成与约束.","text":"本文是janusgraph的第二篇,主要是阐述了janusgraph的配置及其数据模型,配置那方面:主要是从janusfactory和janusgraph server(就是janusgraph集成的Gremlin-server).数据模型主要是从边标签,属性键,顶点标签.分别讲了这三方面的实例生成与约束. 一.配置 1.1 需要配置的对象 1.1.1 JanusGraphFactory 1.1.2 JanusGraph Server 1.2 配置范围 1.2.1 选项 1.2.2 例子 1.2.3 全局离线方式的配置更新 二.janusgraph模型与数据建模 2.1 Edge label(边标签) 2.1.1 形式 2.1.2 定义 2.1.3 例子 2.2 Property Keys(属性键) 2.2.1 概念 2.2.2 属性值约束类型 2.2.3 例子 2.3 关系类型 2.3.1 概念 2.3.2 例子 2.4 顶点标签 2.5 自动模式生成器 2.6 更改模型图元素 一.配置1.1 需要配置的对象1.1.1 JanusGraphFactory 在这里,配置文件是作为参数，JanusGraphFactory其实就是生成一个janusgraph图实例的类,调用open方法生成,参数为某个配置文件(或者直接配置),生成对应配置的图实例.1234#参数为某个配置文件graph = JanusGraphFactory.open(\"conf/janusgraph-cassandra.properties\")#参数为直接配置graph = JanusGraphFactory.open('cassandra:localhost') 两种方式实现该JanusGraphFactory: 1.通过Gremlin方式输入 2.将janusGraph嵌入到应用. 1.1.2 JanusGraph Server配置的对象之一:JanusGraph Server JanusGraph使用Gremlin Server engine作为服务器组件来处理和应答客户端查询，当Gremlin Server集成到JanusGraph，称为JanusGraph Server,是一个长时间运行的服务器进程 配置JanusGraphServer,配置文件位于JanusGraph安装目录下的/conf/gremlin-server服务器上1234567#就是根据配置文件生成对应实例的选项graphs: &#123; graph: conf/janusgraph-berkeleyje.properties&#125;#就是在生成janusgraph实例可以导入几个个实例,加快开发.plugins: - janusgraph.imports 1.2 配置范围1.2.1 选项 (1)local–局部:应用在单个实例,一般在初始化过程当中配置. (2)maskabel–可屏蔽:优先使用本地配置文件配置该实例,如果本地配置文件没有的,可以使用janusgraph的集群配置. (3)global–全局:优先使用全局的配置文件,但是不能覆盖已经配置好了的具体janusgraph实例. (4)global–off全局离线:只不过配置需要更新的话,需要重启才能更新配置. (5)FIXED–固定:配置好了并且已经实例化了一个集群,配置就不能在更改了. 1.2.2 例子 使用图实例graph的openManagement()方法得到一个操作配置文件的对象.更改JanusGraph集群上的默认缓存行为，请执行以下操作：1234567gmt = graph.openManagement()# 更新mgmt.set('cache.db-cache', true)// Changes option## 生效mgmt.commit()// Changes take effect 1.2.3 全局离线方式的配置更新 1.关闭集群中除一个JanusGraph实例之外的所有实例 2.连接到单个实例 3.确保所有正在运行的事务都已关闭。 4.确保不启动新事务(即群集必须脱机) 5.打开管理API 6.更改配置选项 7.提交,提交之后原先留下来的一个实例(graph)也会关闭 8.重新启动所有实例 二.janusgraph模型与数据建模 每个JanusGraph都有一个schema(模型)，该模型由edge labels, property keys和vertex labels组成。JanusGraph的schema可以显式或隐式创建，推荐用户采用显式定义的方式。JanusGraph的schema是可以在使用过程中修改的，而且不会导致服务宕机，也不会拖慢查询速度 注意：通关系型数据库不同，图数据的schema是定义一张图(可以简称为模型图)，而非定义一个vertex的。在Mysql中，我们通常将建立一张表定义为创建一个schema，而在JanusGraph中，一个Graph用于一个schema。其组成结构如下：JanusGraph Schema | |———–Vertex Lables | |———–Property Keys | |———–Edge Labels edge label, vertex label及property key在初次创建时被赋予模型元素,对应模型图中的边,顶点,属性. 2.1 Edge label(边标签)2.1.1 形式 边标签就是边的语义,形式: 1edge label:friend 其中标签名是唯一的 2.1.2 定义 使用makeEdgeLabel(String)定义,之后返回一个构造器,用来约束该标签在任意顶点间的条数.主要约束情况有 (1)针对一对顶点: MULTI–&gt;多对多 SIMPLE–&gt;最多一条边 (2)针对某个顶点 MANY2ONE–&gt;多对一(任意某个顶点有多个入度,最多只有一个出度) ONE2MANY–&gt;一对多(任意某个顶点最多只有一个入度,可以有多个出度) 2.1.3 例子 边标签属性的设置,其中makeEdgeLabel()生成一个边标签,之后用make()返回得到该边标签1234mgmt = graph.openManagement()follow = mgmt.makeEdgeLabel('follow').multiplicity(MULTI).make()mother = mgmt.makeEdgeLabel('mother').multiplicity(MANY2ONE).make()mgmt.commit() 2.2 Property Keys(属性键)2.2.1 概念 Property Keys是图模型的一部分,对应模型图的属性.其主要是为了约束属性值 2.2.2 属性值约束类型1.数据类型约束–Property Key Data Type (1)不能在已经定义系统配置好的数据类型的子类型 (2)相同key的不同值的数据类型是一样的. 2.属性值个数约束–Property Key Cardinality (1)SINGLE:只能有一个候选的属性值 (2)LIST(列表):可以有多个候选的属性值,能重复 (3)SET(集合):可以有多个候选的属性值,不能重复 2.2.3 例子123456789mgmt = graph.openManagement()//创建了一个名字为birthDate的属性，并设置值类型为LONG，且只能保存一个值birthDate = mgmt.makePropertyKey('birthDate').dataType(Long.class).cardinality(Cardinality.SINGLE).make()//创建了一个名字为name的属性，并设置值类型为String，且可以保存不能重复的多个值name = mgmt.makePropertyKey('name').dataType(String.class).cardinality(Cardinality.SET).make()//创建了一个名字为sensorReading的属性，并设置值类型为Double，且可以保存可以重复的多个值sensorReading = mgmt.makePropertyKey('sensorReading').dataType(Double.class).cardinality(Cardinality.LIST).make()//生效mgmt.commit() 2.3 关系类型2.3.1 概念 其中Edge Label(边标签,就是边得得语义)与property key(属性键,一般简称属性)都是属于关系类型,想想也是:根据SPO三元组,其中P可以是关系,属性,O可以是属性值或实体.所以关系类型,在janusGraph中其实就是起连接作用.在这里都可以叫做关系类型(边标签,属性键可以看成是SPO中P部分的关系,属性). 因为Edge Label(边标签,就是边得得语义)与property key(属性键,一般简称属性)都是同等地位,因此两者的命名不能相同. 2.3.2 例子 查询是否存在关系类型,可以通过查询属性键与边标签12345 mgmt = graph.openManagement()if (mgmt.containsRelationType('name')) name = mgmt.getPropertyKey('name')mgmt.getRelationTypes(EdgeLabel.class)mgmt.commit() 2.4 顶点标签 为了方便的叙述顶点标签,先使用一个例子:12345678mgmt = graph.openManagement()person = mgmt.makeVertexLabel('person').make()mgmt.commit()// Create a labeled vertexperson = graph.addVertex(label, 'person')// Create an unlabeled vertexv = graph.addVertex()graph.tx().commit() 根据上面的代码可以看出,创建顶点标签的方式有两种: (1)创建一个操作配置文件的对象,之后使用makeVertexLabel. (2)第二种直接使用janusGraph的addVertex()方法,这种方法最快速. 但是如果在创建顶点标签假如没有指定顶点的类型,在后面进行实例化的时候,就相当于隐式定义了顶点标签的类型. 2.5 自动模式生成器 其实就是针对隐式定义(初始化的时候没有实例化,之后单独实例化)的情况而采取的一些方案. 针对边标签:如果隐式定义,那么该边标签就会有的属性约束(多重性)就会采取默认的状态(MULTI). 针对属性键:如果隐式定义,那么该标签也是使用默认状态—数据类型是:object.class,基数(候选答案的个数)是单个. 最好的方法就是显示定义避免之后不必要的麻烦. 2.6 更改模型图元素 其实模型图就是相当于是一个视图,是对模型的类型–边标签,属性键,顶点标签的的一个映射.因此一旦模型的类型提交到模型图当中,那么边标签,属性键,顶点标签之类就不能更改,我猜,如果直接更改,将会影模型图的映射,所以一个折中的办法就是修改模型的元素(顶点,属性,边),但是修改之后也不会立即显现(在当前的事务处理与其他的janusGraph实例志宏),janusgraph会先通过存储后台告诉其他有关联的janusgraph的实例.模型元素素更名可能会引起冲突(虽然改名字在本janusgraph实例没有,但是和该元素有关联的其他janusgraph已经有相同名称的模型元素). 那么实现模型元素更名,可以先将原来元素改名为新的不存在的元素名称，然后创建新的模型元素，但不会影响已经创建的数据，需要通过批处理修改数据","categories":[{"name":"janusgraph","slug":"janusgraph","permalink":"http://yoursite.com/categories/janusgraph/"},{"name":"知识点","slug":"janusgraph/知识点","permalink":"http://yoursite.com/categories/janusgraph/知识点/"}],"tags":[{"name":"janusgraph","slug":"janusgraph","permalink":"http://yoursite.com/tags/janusgraph/"}]},{"title":"janusgraph之环境搭建(一)","slug":"janusgraph之环境搭建(一)","date":"2018-07-06T07:50:13.000Z","updated":"2018-11-13T02:29:54.000Z","comments":true,"path":"2018/07/06/janusgraph之环境搭建(一)/","link":"","permalink":"http://yoursite.com/2018/07/06/janusgraph之环境搭建(一)/","excerpt":"本文是janusgraph的第一篇,总得来说,遇到的坑是在是很多,但好在一一解决,主要是涉及:JAVA，HBASE，SOLR，HADOOP，JANUSGRAPH的安装,涉及到的知识点比较多,因此篇幅较长,当作总结,便于以后回忆.","text":"本文是janusgraph的第一篇,总得来说,遇到的坑是在是很多,但好在一一解决,主要是涉及:JAVA，HBASE，SOLR，HADOOP，JANUSGRAPH的安装,涉及到的知识点比较多,因此篇幅较长,当作总结,便于以后回忆. 一.编辑软件安装 1.1 vim编辑器 1.2 安装解压缩 二.用户添加 三.ssh配置本地免密登录 四.vim使用 4.1 命令模式 4.2 末行模式 4.3 插入模式 4.4 vim异常问题 五.shell 5.1 概念 5.2 图形界面与命令行 5.3 shell脚本(script) 六.linux补充 七.java安装 八.hadoop 8.1 安装设置 8.2 伪分布式设置 8.2.1 环境变量设置: 8.2.2 伪分布式设置 8.2.3 启动 九.hbase1.3.1 9.1 概念 9.2 spark,hadoop,hbase,HDFS 9.3 准备 9.4 解压缩 9.5 重命名 9.6 权限设置 9.7 环境配置 9.7.1 环境变量设置 9.7.2 环境设置 9.8 hbase异常问题 十.solr安装 10.1 solr关闭,启动,重启服务: 十一.搭建 janusGraph 11.1 解压 11.2 重命名 11.3 复制文件 11.4 修改文件 11.5 创建solr connection 11.5.1 上传solr配置文件 11.5.2 创建collecion 11.6 启动JanusGraph Server(服务器) 11.7 启动janusGraph client(客户端) 十二.参考文献 一.编辑软件安装1.1 vim编辑器1.安装12#参数y代表用yes回答安装过程中的所有问题yum -y install vim* 2.配置在键盘上输入:1sudo vim /etc/vimrc 来打开对应的配置文件,进入编辑模式之后在vimrc文件当中末尾增加:12345set nu #\"在左侧显示行号set tabstop=4 #\"tab 长度设置为 4set nobackup #\"覆盖文件时不备份set ruler #\"在右下角显示光标位置的状态行set autoindent #\"自动缩进 1.2 安装解压缩12#tar.gz解压,系统本身就有,tar是打包,gz是以gzip方式压缩yum -y install unzip 二.用户添加1.添加用户12345# -m就是建立用户的登录目录# -s就是指定用户所使用的命令行.这里指的是/bin/bash#下面这句话的含义就是 添加一个名为hhjs的用户,该用户所在的目录是/home/hhjs,其中所能使用的命令行就是/bin/bash.useradd -m hhjs -s /bin/bashpasswd hhjs 在shell界面上:123456789[root@VM_0_17_centos ~]# useradd -m hhjs -s /bin/bash#这行的输入就是就是需要为新用户hhjd设置一个新的密码[root@VM_0_17_centos ~]# passwd hhjsChanging password for user hhjs.New password: #因为密码是8个8,所以就会提示,正读与倒读都一样.BAD PASSWORD: The password is a palindromeRetype new password: passwd: all authentication tokens updated successfully. 在xftp5当中查看可以得知:注意事项: 上述的只是一个例子,最终的还是以hhjs这个用户为准.2.权限设置 进入超级用户的配置文件:1vim /etc/sudoers 之后在root ALL=(ALL) ALL之后添加一个:12#这行就相当于为hhjs赋予了超级权限的管理员角色.hhjs ALL=(ALL) ALL 三.ssh配置本地免密登录12345678910111213#1.生成.shh文件.或者直接在root目录下mkdir .shhlocalhost ssh#本地登录并且退出,就是为了生成.shh隐藏文件夹exit()cd ~/.ssh/#2.生成密钥,RSA(Rivest-Shamir-Adleman)密码系统ssh-keygen -t rsa t--&gt;type密钥类型 # 不断回车即可#3.添加密钥到授权文件cat id_rsa.pub &gt;&gt; authorized_keys #chmod:change model--&gt;改变类型.#一般是设置当前用户对某个文件夹/文件的权限.#4.修改授权文件的权限chmod 600 ./authorized_keys 截图如下: 四.vim使用 vim主要有三种模式(主要是总结自己在学习当中用到的 情况,可能不全但是常用.) 4.1 命令模式主要是使用esc键进行进入: (1)gg:返回首行; (2)G:返回尾行 (3)行号gg/G:跳转到指定行 (4)dd:删除当前行 (5)u:撤销当前操作 (6)ctrl+f:下翻一屏 (7)ctrl+b:上翻一屏 (8)复制/粘贴: 先使用yy,复制当前行,之后跳转到指定行,输入:P(前插入)/或者p(后插入) (9)行首:0 (10)行尾:’(shift+4),美元的符号 (11) ctrl+s锁定 (12) ctrl+q 解锁 (13)命令模式下输入?+字符串就是查找功能,小n就是下一个,N就是上一个 4.2 末行模式主要是使用:进入 (1):x/:wq —&gt;保存并退出,记住hi小写x.否则就是加密的快捷键了 (2):q—&gt;不保存退出 (3):wq!—&gt;当:wq不管用的时候,出现readonly option is set （add！to override）这种提示就使用这种方式,平常不推荐. 4.3 插入模式 主要是用i或者insert按键 4.4 vim异常问题当打开Vi编辑器时出现以下的提示时不要着急12345E325: ATTENTIONFound a swap file by the name \"/etc/.profile.swp\"owned by: root dated: Wed Jul 14 10:01:10 2010file name: /etc/profilemodified: YES 方法： (1)ls -a 列举所有的内容 (2)rm对应的.swp文件。 五.shell5.1 概念 shell就是一个命令行的解释器,支持不同种类的命令行,如sh,bash,是用户和内核的桥梁.通过shell,用户键盘输入的命令行.经过shell解释,将会传递给内核,然后内核将执行结果返回给shell,shell通过图形化界面将结果显示给用户,周而复始. 5.2 图形界面与命令行 本质上都是为了让用户控制内核,从而控制计算机,因为内核最终控制着计算机的的硬件.但是图形界面只会让简单的事情更简单,但是对于复杂的操作,需要几步,但是对于命令行往往少很多.所以命令行针对是复杂情景下的操作. 5.3 shell脚本(script) 主要是针对于一般重复性的操作,将其打包成一组命令行,增加可复用性,节省时间.因为shell是解释型语言,不需要什么专门的编译器,直接调用系统就可以.1234567891011121314#查看linux下的shell所支持的命令行种类[root@VM_0_17_centos janusgraph]# vim /etc/shellsError detected while processing /etc/vimrc:line 64:W18: Invalid character in group namePress ENTER or type command to continue 1 /bin/sh 2 /bin/bash #bash是sh的增强版 3 /sbin/nologin 4 /usr/bin/sh 5 /usr/bin/bash 6 /usr/sbin/nologin 7 /bin/tcsh 8 /bin/csh 六.linux补充1.cd - : 返回上一次目录2.jps: jps(Java Virtual Machine Process Status Tool)是JDK1.5提供的一个显示当前所有java进程pid的命令3.tar zxvf zhcon-0.2.5.tar.gz 这些都是tar的参数。.tar.gz是一个压缩包，.tar只是打包而没有压缩，注意这点。1234z：通过gzip支持压缩或解压缩。还有其他的压缩或解压缩方式，比如j表示bzip2的方式。x：解压缩。c是压缩。v：在压缩或解压缩过程中显示正在处理的文件名f：f后面必须跟上要处理的文件名。也就是说你不能写成这样 tar -zxfv zhcon-0.2.5.tar.gz 4.solr create -c my_core 其中-c全称为config5.chown： 主要是设置文件夹/文件的所有权1chown -r hhjs:hhjs1 hbase 含义:hbase这个文件夹及其子文件夹(-r的缘故)的属主是hhjs,而这个属主又是属于hhjs1这个用户组的.因此不单单可以改变访问权限(使用权),并且还拥有这个HBASE文件夹(拥有权).6.chmod: 主要是修改某个用户对某个文件/文件夹的操作权限(使用权).形式为:1chmod [who] [opt] [mode] 文件/目录名 (1)[who]参数说明:12345其中who表示对象:u：表示文件所有者 g：表示同组用户 o：表示其它用户 a：表示所有用户 (2)[opt]参数说明1234其中opt代表权限的增删操作+：添加某个权限 -：取消某个权限 =：赋予给定的权限，并取消原有的权限 (3)[mode]参数说明123r：可读 w：可写 x：可执行 例子: 12#为当前用户的同组用户a.txt增加读写权限chmod g+rw a.txt 七.java安装1.Centos中yum方式安装java123456#查看是否已经安装yum list installed |grep java#查看java的版本yum -y list java*#选择需要安装的java版本yum -y install java-1.8.0-openjdk* 安装好了之后,那么就需要进行设置环境变量.因此在需要先找到java的安装目录.2.查询java安装目录:1234#三步走which java (定位到java的真正安装目录) ls -lrt /usr/bin/javals -lrt /etc/alternatives/java 3.设置环境变量: 跳转到环境变量设置文件:1vim /etc/profile 在该配置文件的末行输入如下内容:12345export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.171-8.b10.el7_5.x86_64/jre/bin/javaexport JRE_HOME=$JAVA_HOME/jreexport CLASSPATH=$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATHexport PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH 4.在.bashrc文件中配置JAVA_HOME –＞为了之后的hadoop,在文件的首行添加:~/~1export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.171-8.b10.el7_5.x86_64 八.hadoop8.1 安装设置1.上传对应的安装包到云服务器上,本文是通过xftp软件进行上传.文件名为:hadoop-2.7.6.tar.gz,截图如下2.之后进行如下操作1234567891011121314151617181920#2.1解压安装包[root@VM_0_17_centos hhjs]# sudo tar -zxf hadoop-2.7.6.tar.gz#2.安装包重命名[root@VM_0_17_centos hhjs]# sudo mv hadoop-2.7.6 hadoop#3.设置当前用户对其的权限[root@VM_0_17_centos hhjs]# sudo chown -R hhjs:hhjs hadoop[root@VM_0_17_centos hhjs]# cd hadoop[root@VM_0_17_centos hadoop]# cd bin[root@VM_0_17_centos bin]# lscontainer-executor hadoop.cmd hdfs.cmd mapred.cmd test-container-executor yarn.cmdhadoop hdfs mapred rcc yarn[root@VM_0_17_centos bin]# cd ..#4.进入安装目录查看版本号[root@VM_0_17_centos hadoop]# bin/hadoop versionHadoop 2.7.6Subversion https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8Compiled by kshvachk on 2018-04-18T01:33ZCompiled with protoc 2.5.0From source with checksum 71e2695531cb3360ab74598755d036This command was run using /home/hhjs/hadoop/share/hadoop/common/hadoop-common-2.7.6.jar 8.2 伪分布式设置8.2.1 环境变量设置: 首先进行hadoopd环境变量的设置,在~/.bashrc文件下,需要用ls -a才可以看见12345678export HADOOP_HOME=/home/hhjs/hadoopexport HADOOP_INSTALL=$HADOOP_HOMEexport HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport YARN_HOME=$HADOOP_HOMEexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin 输入如下命令,该文件立即生效:1source ~/.bashrc 最后可以如下命令进行测试:123[root@VM_0_17_centos ~]# echo $HADOOP_HOME#说明是hadoop的安装目录/home/hhjs/hadoop 8.2.2 伪分布式设置主要配置的文件有两个: /home/hhjs/hadoop/etc/hadoop/下的core-site.xml与hdfs-site.xml,都是在文件中的configuration块,即当中添加.1.配置core-site.xml文件123456789&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;file:/home/hhjs/hadoop/tmp&lt;/value&gt;&lt;description&gt;Abase for other temporary directories.&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;&lt;/property&gt; 2.配置hdfs-site.xml123456789101112&lt;property&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;1&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;&lt;value&gt;file:/home/hhjs/hadoop/tmp/dfs/name&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;&lt;value&gt;file:/home/hhje/hadoop/tmp/dfs/data&lt;/value&gt;&lt;/property&gt; 8.2.3 启动1.NameNode的格式化 首次启动hadoop,需要执行NameNode的格式化,具体在/home/hhjs/hadoop/目录下执行:1bin/hdfs namenode -format 2.启动NaneNode 和 DataNode 守护进程,也在/home/hhjs/hadoop/目录下执行1sbin/start-dfs.sh 3.启动jps: jps全称为:java process status 在/home/hhje/hadoop/目录下执行,输入1jps 得到java进程情况 九.hbase1.3.19.1 概念 HADOOP是一个数据仓库,一般是静态的,而HDFS作为文件的存储格式(HADOOP file system),不能满足实时读写,因此在HDFS之上建立了HBASE数据库(数据库是动态的,DataBase on Hadoop),来满足实时读写 9.2 spark,hadoop,hbase,HDFS spark:处理计算数据;–(操作数据) hadoop:管理资源(计算,存储)–管理数据的存储与操作. HDFS:分布式文件系统,类似window上的资源管理器—存储数据 HBASE:分布式数据库,主要存储非结构化数据,键值形式存储.类似在window资源管理器上的Oracle数据库.–存储数据 9.3 准备 从官网上下载:hbase-1.3.1.tar.gz,上传到云服务器进行解压 ,之后将lib目录下有关hadoop-2.5.1的jar全部替换成hadoop-2.7.6的jar包,除了hadoop-client不能在hadoop的安装包中找到,其余都可以,hadoop-client-2.5.1可以删除. 9.4 解压缩1sudo tar -zxf hbase-1.3.1-bin.tar.gz 结果如下： 先在云端删除原先hadoop的jar包(14项),将对应的hadoop-2.7.6的jar包(除了hadoop-client都可以在hadoop安装包找到)上传到云端:再次打包.输入:1tar -zcvf hbase-1.3.1.tar.gz hbase-1.3.1 生成hbase-1.3.1.tar.gz,删除原先的上传的hbase-1.3.1-bin.tar结果为: 9.5 重命名12#当前目录下sudo mv hbase-1.3.1 hbase 9.6 权限设置文件权限(在hhjs目录下):123#可以参考linux基本命令那章.chown -R hhjs:hhjs hbasechmod -R 775 hbase 解释如下: 9.7 环境配置9.7.1 环境变量设置在1vim /etc/profile 行末下,添加hbase的环境变量1export HBASE_HOME=/home/hhjs/hbase 保存退出之后,输入1source /etc/profile 立即生效 9.7.2 环境设置1.配置hbase/conf/hbase-env.sh 其中hbase所指为HBASE的安装目录123export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.171-8.b10.el7_5.x86_64export HBASE_LOG_DIR=$&#123;HBASE_HOME&#125;/logsexport HBASE_MANAGES_ZK=true 2.配置hbase/conf/hbase-site.xml12345678910111213141516171819&lt;property&gt;&lt;name&gt;hbase.rootdir&lt;/name&gt;&lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt;&lt;!--注意上述路径需要与hadoop中core-site.xml中配置的fs.default.name路径相同--&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.cluster.distributed&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;&lt;!--zookeeper的法定人数也即机器数--&gt;&lt;value&gt;localhost&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;zookeeper.znode.parent&lt;/name&gt;&lt;!--指定zookeeper的相对目录--&gt;&lt;value&gt;/hbase&lt;/value&gt;&lt;/property&gt; 3.启动HBASE:12bin/start-hbase.sh #启动hbasebin/stop-hbase.sh #停止hbase 4,查看hbase数据:12#在hhjs目录下hbase/bin/hbase shell 4.测试查看java进程 9.8 hbase异常问题1.stop-habse.sh关不了 先输入：hbase-daemon.sh stop master 再输入：stop-hbase.sh就可以关闭HBase集群了截图如下: 2.hbase启动时regionserver running as process 20009. Stop it first.1234567#hbase/conf/&lt;property&gt;&lt;name&gt;hbase.rootdir&lt;/name&gt;&lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt;&lt;!--注意上述路径需要与hadoop中core-site.xml中配置的fs.default.name路径相同--&gt;&lt;!--要用HDFS为Hbase提供存储空间，定义hbase.rootdir参数时HDFS文件系统的主机名和端口号必须与Hadoop的配置文件core-site.xml中fs.default.name参数的配置一致--&gt;&lt;/property&gt; 修改之后截图: 十.solr安装1.安装包上传至云服务器,同时解压,并重命名为solr2.启动: 在solr安装目录输入:1bin/solr start 如果出现下图问题:权限不足,那么就使用:1bin/solr start -force 3.配置站点规则.或者之后新建规则:隔几分钟后:登录http://193.112.89.252:8983/solr/index.html,就可以了.结果如下:没有cloud的结果,其登录方式是:1bin/solr start 有cloud的登录结果:其启动solr方式(有cloud)1bin/solr start -cloud -z 193.112.89.252:2181 -force 注意:如果输入这样: bin/solr start -cloud -m 16g -z 193.112.89.252:2181 -force就会出现:1234#内存不足[root@VM_0_17_centos solr]# bin/solr start -cloud -m 16g -z 193.112.89.252:2181 -forceWaiting up to 180 seconds to see Solr running on port 8983 [\\] Still not seeing Solr listening on 8983 after 180 seconds!tail: cannot open ‘/home/hhjs/solr/server/logs/solr.log’ for reading: No such file or dir 10.1 solr关闭,启动,重启服务:123bin/solr stop -allbin/solr start -cloud -z 193.112.89.252:2181 -force(远程启动)bin/solr restart -cloud -z 193.112.89.252:2181 -force 参数说明:1234 -z:使用自定义ZooKeeper connection string启动Solr。该选项只能在-c选项，即SolrCloud模式下使用。如果该选项未提供，Solr将启动内置ZooKeeper实例 -c参数表明是以Cloud的方式启动服务， -p指定SolrNode端口， -s指定core的目录（相当于SOLR_HOME），会在该目录下寻找solr.xml文件。 十一.搭建 janusGraph11.1 解压1sudo unzip janusgraph-0.2.0-hadoop2.zip 11.2 重命名1mv janusgraph-0.2.0-hadoop2 janusgraph 11.3 复制文件123# 总共有两个文件需要修改cp conf/janusgraph-hbase-solr.properties conf/gremlin-server/janusgraph-hbase-solr-server.propertiescp conf/gremlin-server/gremlin-server.yaml conf/gremlin-server/janusgraph-gremlin-server.yaml 11.4 修改文件最好用记事本进行添加1.修改janusgraph-hbase-solr-server.properties12vim conf/gremlin-server/janusgraph-hbase-solr-server.properties#或者用记事本打开,使用xftp软件 将该文的全部内容先注释,末尾添加123456789101112gremlin.graph=org.janusgraph.core.JanusGraphFactorystorage.backend=hbase#hbase的zookeeper地址storage.hostname=193.112.89.252storage.hbase.table=search#hbase的表名，默认值为janusgraphstorage.hbase.table=janusgraphindex.search.backend=solrindex.search.solr.mode=cloudindex.search.solr.zookeeper-url=193.112.89.252:2181#index.x.solr.configset的值需要与solr建表指定的配置文件名保持一致 index.search.solr.configset=janusgraph 2.修改janusgraph-gremlin-server.yaml 将文中的graph{}那项改为123graphs: &#123; graph: conf/gremlin-server/janusgraph-hbase-solr-server.properties&#125; 即可,点击保存. 11.5 创建solr connection11.5.1 上传solr配置文件1/home/hhjs/solr/server/scripts/cloud-scripts/zkcli.sh -cmd upconfig -z 193.112.89.252:2181 -d /home/hhjs/janusgraph/conf/solr -n janusgraph 其中:123-z: solr的zookeeper地址-d: 本地配置文件路径-n：zookeeper上的配置文件名 上传结果如下: 11.5.2 创建collecion在浏览器中输入:1http://193.112.89.252:8983/solr/admin/collections?action=CREATE&amp;name=janusgraph&amp;numShards=1&amp;replicationFactor=1&amp;maxShardsPerNode=1&amp;property.dataDir=/data/solr/graph&amp;collection.configName=janusgraph&amp;createNodeSet=193.112.89.252:8983_solr 解释:123456numShards=1为janusgraph表使用1个分片；replicationFactor=1为graph表每个分片不设定副本；maxShardsPerNode=1为janusgraph表一个节点上的最大分片数为1；property.dataDir=/data/solr/graph为索引数据存放目录；collection.configName=schema为graph使用的配置文件目录为 janusgraph；createNodeSet=193.112.89.252:8983_solr 得到:solr界面刷新之后可以看到: 11.6 启动JanusGraph Server(服务器) 启动之前最好先启动hadoop,solr,hbase,否则会报错. 进入janusgraph对应的安装目录: 输入 1bin/gremlin-server.sh conf/gremlin-server/janusgraph-gremlin-server.yaml 可以得到结果: 关闭server的命令为1bin/janusgraph.sh stop 注意该命令会关闭所有名为GremlinServer的进程 11.7 启动janusGraph client(客户端) 这个主要是为了能够远程访问,janusGraph服务器,通过在浏览器输入在remote.yaml中设置好的ip地址与监听端口,实现远程访问.具体做法如下:1.编辑janusgrap安装目录下的conf/remote.yaml文件 为了设置地址与监听端口123hosts: [193.112.89.252]port: 1981serializer: &#123; className: org.apache.tinkerpop.gremlin.driver.ser.GryoMessageSerializerV1d0, config: &#123; serializeResultToString: true &#125;&#125; 假如有多个云服务器,通过类似1cp ./conf/remote.yaml ./conf/remote-local.yaml 得到多个远程文件,通过类似设置ip地址与端口,访问的时候:插入对应的访问名,例如:1remote connect tinkerpop.server conf/remote-local.yaml 就可以.2.进入gremlin命令行,具体做法如下:1234gremlin&gt; :remote connect tinkerpop.server conf/remote.yaml==&gt;Configured 193.112.89.252/193.112.89.252:1981gremlin&gt; :remote console==&gt;All scripts will now be sent to Gremlin Server - [193.112.89.252/193.112.89.252:1981] - type ':remote console' to return to local mode 代码中:1Configured 193.112.89.252/193.112.89.252:1981 说明已经配置好了.还有1:remote console 是用来测试的. 十二.参考文献[1]https://blog.csdn.net/waitfxfx/article/details/80674966[2]https://blog.csdn.net/oitebody/article/details/80696339[3]https://docs.janusgraph.org/latest/","categories":[{"name":"janusgraph","slug":"janusgraph","permalink":"http://yoursite.com/categories/janusgraph/"},{"name":"环境搭建","slug":"janusgraph/环境搭建","permalink":"http://yoursite.com/categories/janusgraph/环境搭建/"}],"tags":[{"name":"janusgraph,hbase,hadoop","slug":"janusgraph-hbase-hadoop","permalink":"http://yoursite.com/tags/janusgraph-hbase-hadoop/"}]},{"title":"事来则应,事去则静,英雄无言,凭色声威","slug":"事来则应,事去则静,英雄无言,凭色声威","date":"2018-07-03T12:38:12.000Z","updated":"2018-11-13T03:08:30.000Z","comments":true,"path":"2018/07/03/事来则应,事去则静,英雄无言,凭色声威/","link":"","permalink":"http://yoursite.com/2018/07/03/事来则应,事去则静,英雄无言,凭色声威/","excerpt":"上大学之后,第一次写读后感,刚开始写,有很多不足之处,希望之后可以好好改进,刚刚读完偷窥了马化腾的朋友圈，我才明白：马云还远不是对手!,里面有句话很对:”事来则应,事去则静”.很多时候,我们面对事情的时候,往往不能抓住重点,沉着冷静,更多的是慌忙不已,有时候自己也是那样,傻傻的面对,其实自己有时候也挺拖沓的,可能是没有人监督的原因.","text":"上大学之后,第一次写读后感,刚开始写,有很多不足之处,希望之后可以好好改进,刚刚读完偷窥了马化腾的朋友圈，我才明白：马云还远不是对手!,里面有句话很对:”事来则应,事去则静”.很多时候,我们面对事情的时候,往往不能抓住重点,沉着冷静,更多的是慌忙不已,有时候自己也是那样,傻傻的面对,其实自己有时候也挺拖沓的,可能是没有人监督的原因. 文中还说了:人能分三种,一种是自己能成事,一种是自己能带领别人能成事,最后一种就是能一眼看穿的人.其实想想自己,自己应该还是处在第一阶段,因此,现阶段的目标就是好好学习,而不是想其他的,男女之事,现阶段不是重点,保持就可以. 其实我打心底里挺佩服一眼能看穿的人,因为这是识人用人的最基础的技能.但往往需要自己多多思考,总结起来就是:诙谐开场,中间观察正经,其间暖场(可以诙谐,但更多的是倾听)–这个是针对一般交朋友的时候.有时可以搞笑,但是对人的观察也不能松懈,因此在以后的生活当中,用诙谐的方式,在与别人聊天的时候,也要打量这个人.然后根据最终结果,得到与这个人相处的定位,不要为了说话而说话,这样往往很容易冷场,一开始是可以使用诙谐的方式开场,然后当进入他的話场模式的时候,那么就是需要正经下来,多思考,而不是为了搞笑而搞笑.这样的谈话没有意义,但也需要有时诙谐,自己度总之需要把握好,与他人聊天,多看眼睛,嘴巴.一般这两方面,最重要. 其实刘老师教得很好,不懂的就不要装懂,当你自己的表述还没有想清楚的时候,嘴巴千万不要先动,这样真的会让人对你丧失信心,所以不懂的你可以说:我先记下来,回去我去查资料,而不是一个劲的再说,说的都是废话 第一次,不知不觉写了这么些,以后加油,让自己的文采能更上一层楼.","categories":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/categories/随笔/"},{"name":"读后感","slug":"随笔/读后感","permalink":"http://yoursite.com/categories/随笔/读后感/"}],"tags":[{"name":"读后感","slug":"读后感","permalink":"http://yoursite.com/tags/读后感/"}]},{"title":"sourceTree安装及使用","slug":"sourceTree安装及使用","date":"2018-07-01T00:30:47.000Z","updated":"2018-11-13T02:48:16.000Z","comments":true,"path":"2018/07/01/sourceTree安装及使用/","link":"","permalink":"http://yoursite.com/2018/07/01/sourceTree安装及使用/","excerpt":"sourceTree是一款用来管理git的便捷工作,用它可以完成大部分工作,本文主要对git的推送,提交,分支,回滚,下拉,获取等众多功能以图形的方式进行展现,便于以后回忆.","text":"sourceTree是一款用来管理git的便捷工作,用它可以完成大部分工作,本文主要对git的推送,提交,分支,回滚,下拉,获取等众多功能以图形的方式进行展现,便于以后回忆. 一.安装 二.使用教程 2.1 克隆 2.2 推送 2.3 获取/拉取 2.4 创建分支和合并分支 2.5 标签管理 2.5.1 绑定标签 2.5.2 删除标签 2.6 冲突解决 2.7 版本回滚 2.8 合并某次提交到另一分支 2.9 Tips 一.安装1.安装界面选择google2.账号登录成功3.插件安装4.安装完成5.忽略文件配置6.安装ssh7.beyond_compare安装注: 找到beyond Compare 4文件夹下面的BCUnrar.dll,将其删掉或者重命名,再重新打开接着使用免费30天! 30天又过了那么继续这样方法. 二.使用教程 2.1 克隆1,先登录github远程仓库 2. 2.2 推送1.在本地工作区添加一个文本: 2.返回sourceTree,就会看到页面会更新的文件.3.暂存区–&gt;分支 之后点击暂存所选(就是将文件提交本地仓库的暂存区中)–&gt;在注释提交部分,添加好自己的注释,之后进行提交.将文件提交到本地仓库的本地分支点当中(双击文本可以进行编辑.)4.点击推送注: 最好的方式:先获取,看下项目有没有变动,之后有变动的话先进行拉取,使得本地与远程一样,减少冲突,最后的才是推送. 2.3 获取/拉取1.点击图中远端,快速进入自己的github页面:2.在远程进行文件修改:3.开始获取4.获取结果5.开始拉取6.拉取结果 2.4 创建分支和合并分支1.设置分支,设置完成切换到该分支2.修改本地仓库的文件添加文字,如图3.获取–&gt;推送分支(注意)4.查看分支5.开始合并(先切换分支)6.合并方式询问:7.分支合并结果:8.合并的本地分支上传 推送之前还是需要进行,获取,拉取的操作,保险起见 2.5 标签管理2.5.1 绑定标签 2.5.2 删除标签 2.6 冲突解决1.修改远程库的文件the first.txt,添加如下内容:2.对应本地文件修改.4.之后需要点击暂存–&gt;之后提交(否则没效果)5.获取之后,点击拉取6.如果再次推送显示失败的话,直接强制提交(本地之前的代码可能有部分提交了，使用强制提交覆盖),使用shift+alt+L,打开对应的git窗口7.可以使用记事本,或者使用beyond_compare来进行对比修改.8.版本冲突的几种方式: (1)找到&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;这种和&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;这种=====这种符号删除 (2)右键–&gt;解决冲突–&gt;使用自己/他人版本解决冲突–&gt;确定–&gt;重新提交推送 (3)右键-&gt;回滚(revert)提交冲突的文件,再重新提交推送,文件已经提交到本地仓库,但未推送(push)到远程仓库,出现冲突,可以回滚提交,拉取文件时,如果出现文件冲突,可以回滚提交本地文件.回滚就是返回上一步的操作. 2.7 版本回滚 1.当已经提交了到远程仓库,可以使用回滚提交进行回退,具体方式就是: 回退是指内容的回退,不是记录的回退. 2.8 合并某次提交到另一分支 1.切换到指定的分支; 2.点击所有分支 3.确定某个提交点,之后右键点击遴选4.最后进行推送到远程仓库,如有冲突进行冲突处理,处理之后,进行推送,不能的话,进行强制推出.具体情况如图: 2.9 Tips (1)保险推送方式:代码写作完成之后-&gt;提交到本地仓库-&gt;”获取”-&gt;”拉取”-&gt;”推送” (2)重新checkout的sourcetree要记得“检出”一下，才有develop。不然在自己的文件夹下边可能只有一个什么README.md的文件，没有工程文件，没有.h，没有.m，什么也没有。 (3)提交的时候尽量提交到分支develop，不要动master。master是主枝，是最后打tag包时候用的或者对master做操作的时候才用到的 (4)fatal: Not a valid object name: ‘master’.:说明.没有提交,-&gt;没有形成master分支","categories":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/categories/linux/"},{"name":"知识点","slug":"linux/知识点","permalink":"http://yoursite.com/categories/linux/知识点/"}],"tags":[{"name":"git","slug":"git","permalink":"http://yoursite.com/tags/git/"},{"name":"sourceTree","slug":"sourceTree","permalink":"http://yoursite.com/tags/sourceTree/"}]},{"title":"git常用命令","slug":"git常用命令","date":"2018-06-29T09:02:50.000Z","updated":"2018-11-13T02:22:16.000Z","comments":true,"path":"2018/06/29/git常用命令/","link":"","permalink":"http://yoursite.com/2018/06/29/git常用命令/","excerpt":"本文主要是一个git的精简教程,包括git的产生原因,git数据库的类型,重点讲述了git的工作流程,并且以此进行展开,之后讲述了git的储藏(bash),分支功能,标签功能,最后的话,还进行了git的一些配置,可以设置不同级别的配置.","text":"本文主要是一个git的精简教程,包括git的产生原因,git数据库的类型,重点讲述了git的工作流程,并且以此进行展开,之后讲述了git的储藏(bash),分支功能,标签功能,最后的话,还进行了git的一些配置,可以设置不同级别的配置. 一.概述 1.1 原因: 1.2 git数据库 二.git工作流程 2.1 git add 2.2 git commit 2.3 git push 2.3.1 关联仓库 2.3.2 关联分支 2.4 git fetch 2.5 git reset 2.5.1 版本回退 2.5.2 撤销修改: 2.5.3 删除文件 2.6 git merge 与 git rebase 2.7 git pull与git fetch 2.7.1 git fetch 2.7.2 git pull 2.8 其他git常用 2.9 工作区,版本库,远程库 三.储藏 3.1 概念 3.2 常用命令 四.分支 4.1 概念 4.2 分支类型 4.2.1 两种分支类型(粗分): 4.2.2 四种分支类型(细分) 4.3 分支合并流程 4.4 常用命令： 五.标签 5.1 概念 5.2 目的 5.3 例子 5.4 常用标签命令 六.git配置 6.1 用户名.密码配置,编辑器,推送方式 6.2 忽略特殊文件 6.3 配置别名 七.ps # 一.概述## 1.1 原因: 为了解决版本控制问题(包含多人编辑同一文件,有时修改的部分是相同,这就会导致在多次提交的时候(每提交一次就是一个版本),出现版本冲突),所以也就是应用而生了很多版本控制工具:VSS、CVS、SVN、Git等,其中当属git应用最广泛,Git是一个分布式版本管理系统,优点是远程仓库与本地仓库是相互独立,在没有联网的情况,本地仓库可以独立工作,为了解决以上版本控制存在问题. 但是所有版本控制系统智能n只能跟踪文本文件的内容改动,对于非文本文件,只能知道文件的添加信息(大小,时间,类型),具体的内容不知道.## 1.2 git数据库 Git的数据库(Repository) 分为远程数据库和本地数据库的两种。+ 远程数据库: 配有专用的服务器，为了多人共享而建立的数据库。+ 本地数据库: 为了方便用户个人使用，在自己的机器上配置的数据库。 平时用的话,只需要使用本地数据库/仓库就可以,远程数据库更多的是为了分享与团队成员协作. 创建远程数据库方法:登录github官网,注册用户,之后进入个人主页创建一个仓库即可 创建本地数据库的方法有两种：一种是创建全新的数据库，另一种是复制远程数据库# 二.git工作流程先上图:## 2.1 git add1.形式:git add 文件名2.作用:将指定文件添加到版本库的暂存区,可以连续添加多次.## 2.2 git commit1.形式:git commit -m ‘提交信息’2.作用:将暂存区的文件一次性提交到版本库的分支。在提交的时候可能会出现,需要配置如下信息.12git config --local user.name usernamegit config --local user.email## 2.3 git push先上图: 形式:1git push origin 本地分支:远程分支 由于在发送之前需要先tracking(绑定),主要是绑定仓库,分支.一般关联仓库之后,会进行仓库的关联.节省一些操作时需要输入的参数。### 2.3.1 关联仓库形式:123# 第一种添加远程数据仓库(从字面意思理解)git remote add origin git@github.com:xiaopingzhong/xiaopingzhong.git# 先克隆远程数据库,之后切换到克隆出来的数据仓库目录.可以不用关联了,然后直接git push origin### 2.3.2 关联分支先声明: 一般远程库设为origin,那么至于origin代表的是默认的远程仓库,下图就是代表xiaopingzhong.github.io.git这个远程仓库,实际上可以在.git/config中进行修改.总共有三种方法:12345678#第一种:单纯关联,一般远程库设为origin,那么至于origin代表的是默认的远程仓库,可以在.git/config中进行修改.git branch --set-upstream [本地分支名] [远程库]/[远程分支]#或者git branch --set-upstream-to [远程库]/[远程分支] [本地分支]#第二种:在推送的时候进行关联,其中[远程分支]的[远程库] 之前已经有origin,进行声明了.git push -u origin [本地分支名] [远程分支名]#为什么需要-u,就是为了说明该分支在其他仓库也有.所以一开始关联的时候需要使用-u.#假如是分支与仓库是一对一的关系,可以不用-u.(保险期间统一使用-u)一般关联之后,以后提交分支的时候,只需要:1git push origin [远程库] [远程分支] push的推送方式:是simple,就是只会会默认推送当前分支.## 2.4 git fetch形式:1git fetch [远程仓库名/主机名] [分支名] 其中git fetch默认是获取当前仓库的当前分支的最新状态到本地.否则根据需要,获取不同仓库及其分支.## 2.5 git reset### 2.5.1 版本回退1.形式:1 git reset [--hard/soft/mixed] HEAD的不同形式/commit id(版本号)2.参数说明:用例子说明3.例子:123456#后退一个版本,头指针也会指向相应的版本,但该版本只是退回到版本库的暂存区,工作区也会保留该版本.git reset --soft HEAD~1#后退一个版本,头指针也会指向相应的版本,但暂存区不会保留该版本,但是工作区会保留该版本(mixed是默认回退方式)git reset --mixed HEAD~1#后退一个版本,头指针也会指向相应的版本,但暂存区不会保留该版本,工作区不会保留该版本总结:回退程度:hard&gt;mixedgit reset --hard HEAD~1 Git的版本回退速度非常快，因为Git在内部有个指向当前版本的HEAD指针，当你回退版本的时候，Git仅仅是把HEAD从指向append GPL：其中commit id. 查看之前的版本:git log–适用于:要回退到哪个版本. 查看之后的版本:git relog–适用于:要回到未来的哪个版本.### 2.5.2 撤销修改:有两种方式:1234#将文件从暂存区回退到工作区,说明没有提交,文件的修改还有.git reset HEAD &lt;file&gt;#将工作区的文件的修改进行撤销,实际上就是用版本库的文件替换工作区的文件git checkout --filename### 2.5.3 删除文件(1)工作区删除12#从工作区当中删除,rm filename #git checkout --filename(可以恢复)(2)版本库删除12git rm filename git commit -m '删除说明'## 2.6 git merge 与 git rebase两者区别,请看下图: merge合并：rebase合并：1.异同:(1)相同点: 当合并时没有冲突的话,都不产生新的提交点,也就是fast-forward的合并方式.如果git merge提交硬要产生一个新提交点那么就用如下方式: 1git merge --no-ff 用图片表示就是: 补充:[non] fast-forward non fast-forward:代表合并会生成新的提交点,fast-forward则是不会.例如下图: (2)不同点: 当合并产生冲突的时候:如下图: 其中==分割线上方是本地数据库的内容, 下方是远程数据库的编辑内容。 git merge 在解决冲突合并之后,再次合并的时候,会自动产生一个新的提交点,试想,如果以后合并的次数多了,那么解决冲突的点就会多起来,就会让分支结构比较复杂,不利于查看.(这是针对个人来说,针对公司的话,有利于维护). git rebase.在解决冲突之后,不会产生一个新的提交点,而是在合并过程中,进行冲突的解决与增量的修改,形成呢个一个线性序列的提交点.两者的形式如下:123456789git merge#冲突解决之后再次提交git add .git commit - m \"说明\" --------------------------------git rebase#冲突解决之后再次提交git add .git rebase --continue## 2.7 git pull与git fetch### 2.7.1 git fetch 从上图可以得知,fetch_head文件主要是用来存储,所关联的远程库及其相应的最新分支的commit id,至于显示多个个分支,具体要看git fetch的四种具体形式:12345678#将更新所有的远程库的所有分支的最新版本下载到本地,并用fetch_head这个文件进行记录git fetch#更新指定远程库名的所有分支的最新版本下载到本地,也用fetch_head这个文件进行记录git fetch 远程库名#更新特定远程库的特定分支名的最新版本下载到本地,fetch_head这个文件进行记录git fetch 远程库名 远程分支名#更新特定远程库的特定分支名的最新版本,同时创建一个本地分支来保存它,也fetch_head这个文件进行记录git fetch 远程库名 远程分支名:本地分支名总结: 其实git fetch的四种方式就是一步步细化要求.### 2.7.2 git pullgit pull= git fetch + git merge,见下图:形式:123#取回特定远程库的某个分支(也可以是该仓库的所有分支即整个仓库)的更新(git pull),之后与对应的本地分支进行合并(git merge).#如果下载的是整个仓库,那么本地仓库也就会与远程仓库进行合并,同时工作区都会合并,所以谨慎使用git pull 远程库名 远程分支名:本地分支名## 2.8 其他git常用(1)git init，初始化，表示即将对当前文件夹进行版本控制。(2)git status，查看Git当前状态，如：那些文件被修改过、那些文件还未提交到版本库等。(3)git add 文件名，将指定文件添加到版本库的暂存状态。可以连续添加多次(4)git log，查看提交记录，即：历史版本记录## 2.9 工作区,版本库,远程库1.工作区:就是.git所在目录,如下图:2.版本库/本地仓库:工作区下的一个.git文件夹,叫做git版本库.为了避免不必要的异常,最好使用全英文路径名.版本库的内容: (1)stage暂存区，还有 (2)Git为我们自动创建的第一个分支master (3)指向master的指针HEAD。文件添加两步走: git add(添加到暂存区) –&gt;git commit(添加到版本库)3.远程库: 就是github的数据库/仓库# 三.储藏## 3.1 概念 当前的工作未完成,由于需要修改其他分支上的代码/或者修改本分支上的其他紧急工作,不想马上提交到分支上去,可以将当前的工作提交到git栈当中,相当于一个暂存区.保存一个对应的工作状态.git栈可以保存多个工作状态.但是可能在修复本分支bug的代码,可能与所提交的工作状态的代码有重合部分,容易发生冲突.解决办法:找到冲突文件，手动修改冲突并提交.## 3.2 常用命令1234567891011git stash #将当前工作区所有修改过的内容存储(工作状态)到git栈中git stash list #查看git栈中的所有记录git stash clear #清空git栈的所有工作状态记录#恢复指定工作状态到工作区的同时,在git栈当中删除对应的工作状态记录.#没有指定编号num的话,形式为git stash pop,默认恢复栈顶的工作状态记录.---既取出,又删除（可能有冲突）git stash pop stash@&#123;num&#125;#恢复指定工作状态到工作区的同时,在git栈当中则保留对应的工作状态记录#没有指定编号num,形式为git bash apply,默认恢复栈顶的工作状态记录----只取出,不删除,git stash apply stash@&#123;num&#125;#删除指定的工作记录,没有指定编号num,形式为git stash drop 默认是删除栈顶的第一个工作状态记录.git stash drop stash@&#123;num&#125;# 四.分支## 4.1 概念 因为git是分布式版本管理系统,所以也就有远程与本地也自然而然是一个相对的概念.所以远程分支与本地分支也是一个相对概念. 分支就是对同一个软件(数据库)的多个发行版本(多个分支)的进行维护 HEAD指向的是当前分支的最新的一次提交点。没有切换分支的时候,通常默认指向master分支的最后一次更新。通过移动HEAD，就可以变更使用的分支。## 4.2 分支类型### 4.2.1 两种分支类型(粗分): Merge分支(合并分支):作为发行版本的主分支,可以由多个topic与单个merge分支合并 topic分支(特性分支):是为了开发新功能或修复Bug等任务而建立的分支.从merge分支分叉.### 4.2.2 四种分支类型(细分)1.主分支: master分支:主要用来发布正式版本的;(正绿色箭头)–对应上面的 develp分支:主要是用来日常开发的(黄色箭头)2.release分支: 就是当从evelp分支分叉出的(就是开发的差不多了,可以形成一个新版本了),然后提先合并到master分支,之后合并到develop分支.分支明智,命名采用release-的形式 (暗绿色箭头)12# 创建一个预发布分支：git checkout -b release-1.2 develop3.featrue分支: 也是从develop分叉出来的,用来进行功能的增加,之后合并到develop分支当中.(紫色箭头)4.hotfix分支: 其实就是bug修复分支,因为正式版本发行后肯定会出现一些bug,所以需要从master分支当中分叉出来,之后进行bug修复,修复成功之后,合并到master分支与develop分支.(粉红色箭头)## 4.3 分支合并流程 默认分支只有一个master分支,现在手工创建的dev分支上进行编码,编好之后,与master分支进行合并.具体实现方法就是:如下图:所示 上图代码:1git checkout -b dev #创建并切换分支 上图代码:1234#git push origin 本地分支名(需要先创建):远程分支名#只写git push origin dev,说明本地分支与远程分支名称相同#第一次只能使用push,之后才可以使用pullgit push origin dev:dev 上图代码:1234#先切换到master分支git checkout master#之后dev主动合并到master分支上git merge dev 上图:删除dev分支12#上图:删除dev分支git branch -d dev 上述是一个开发的一个流程 4.4 常用命令：12345678910111213141516git branch 分支名称 创建分支git checkout 分支名称 切换分支git branch -m 分支名称 创建并切换到指定分支git checkout -b 分支名称 创建并切换到指定分支git branch 查看所有分支git branch -d 分支名称 删除分支git merge 指定分支名称 将指定分支合并到当前分支git branch -r #查看远程分支git branch -a #查看所有分支，本地和远程git remote show [remote-name] #查看远程仓库信息#其中git remote show [remote-name]展示的信息包括：#会列出远程仓库的 URL 与跟踪分支的信息#列出了当你在特定的分支上执行 git push 会自动地推送到哪一个远程分支#列出了哪些远程分支不在你的本地#哪些远程分支已经从服务器上移除了#执行 git pull 时哪些分支会自动合并 五.标签5.1 概念 Git的标签虽然是版本库的快照，但其实它就是指向某个commit的指针（分支可以移动，标签不能移动/改变），所以，创建和删除标签都是瞬间完成的。 5.2 目的 由于版本号(commit ID很复杂),为了方便记忆,使用标签,与commit ID绑在一起,便于记忆. 5.3 例子 创建带有说明的标签(附注的标签,所以会有annotation.省略为a)，用-a指定标签名，-m指定说明文字：12#git tag -a v0.1 -m \"the first version\" 8f3b 用命令git show 可以看到说明文字：12345E:\\Hexo\\ivega&gt;git show v1.0commit 8f3baab0173846c0d1a031518d684461dbcd82e4 (HEAD -&gt; master, tag: v1.0)Author: xiaozhongping &lt;zhongxiaoping@tju.edu.cn&gt;Date: Fri Jun 29 11:26:24 2018 +0800 the first version 注意： 标签总是和某个commit挂钩。如果这个commit既出现在多个分支上，那么在这多个分支上都可以看到这个标签。 5.4 常用标签命令1234567git tag &lt;tagname&gt;用于新建一个标签，默认为HEAD,就是目前head指向的分支，也可以绑定对应的commit id；git tag -a &lt;tagname&gt; -m \"说明\"可以指定标签信息；git tag -l 可以查看所有标签。git push origin &lt;tagname&gt; 可以推送一个本地标签；git push origin --tags 可以推送全部未推送过的本地标签；git tag -d &lt;tagname&gt; 可以删除一个本地标签；git push origin :refs/tags/&lt;tagname&gt; 可以删除一个远程标签(删除远程之前需要先删除本地的标签) 六.git配置1.形式:1git config [option] 属性名 属性值 2.option参数说明 git配置级别(就是该配置的有效范围)有三个,分别是: –system(系统级别):对应文件:暂时找不到. –global(用户级别):文件地址”C:\\Users\\Administrator.gitconfig” –local(当前仓库/项目):文件地址:”E:\\Hexo\\ivega.deploy_git.git\\config” 范围:system&gt;global&gt;local 优先级:local&gt;global&gt;system(范围越大,优先级反倒越小) 6.1 用户名.密码配置,编辑器,推送方式123456git config --global user.name \"xxxxx\"git config --global user.email xxxxx #设置git日志的默认编辑器为vimgit config --globale core.editor \"vim\" #设置git的使用方式为simplegit condif --global push.default \"simple\" 6.2 忽略特殊文件 在工作区创建一个.gitignore文件,在提交的时候用来来忽略某些不太总要的过程文件.例如编译时的中间文件.配置模版可以在https://github.com/github/gitignore例如：下载一个关于java的忽略提交文件: 6.3 配置别名形式:1234#把status简称stgit config --global alias.st status#配置git日志的配色git config --global alias.lg \"log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit\" 七.ps1234E:\\Hexo\\ivega&gt;git commit -m'the first version'#说明是匹配的问题,因此需要使用\"\",而不是单引号.这点需要特别注意.error: pathspec 'first' did not match any file(s) known to git.error: pathspec 'version'' did not match any file(s) known to git.","categories":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/categories/linux/"},{"name":"知识点","slug":"linux/知识点","permalink":"http://yoursite.com/categories/linux/知识点/"}],"tags":[{"name":"git","slug":"git","permalink":"http://yoursite.com/tags/git/"}]},{"title":"linux目录结构解析","slug":"linux目录结构解析","date":"2018-06-27T12:52:16.000Z","updated":"2018-11-13T02:34:52.000Z","comments":true,"path":"2018/06/27/linux目录结构解析/","link":"","permalink":"http://yoursite.com/2018/06/27/linux目录结构解析/","excerpt":"本文主要讲述了linux的目录结构,主要分成四类目录:管理类目录,用户类目录,应用程序类目录,信息类目录.分别讲了根目录下各个目录的英文全称及其作用,主要是用来查漏补缺.为之后的linux的深入操作作铺垫.","text":"本文主要讲述了linux的目录结构,主要分成四类目录:管理类目录,用户类目录,应用程序类目录,信息类目录.分别讲了根目录下各个目录的英文全称及其作用,主要是用来查漏补缺.为之后的linux的深入操作作铺垫. 一.管理类目录 1.1 boot 1.2 bin 1.3 sbin 1.4 etc 1.5 sys 1.6 dev 1.7 mnt 1.8 media 二.用户类目录 2.1 home 2.2 root 三.应用程序类目录 3.1 lib/lib 64 3.2 opt 3.3 usr 3.4 selinux 四.信息类目录 4.1 srv 4.2 var 4.3 tmp 4.4 lost+found 4.5 proc 五.区别 5.1 /mnt /media和/dev 5.2 /srv、/var和/tmp 先上图: 一.管理类目录1.1 boot作用:启动配置文件,程序的引导加载文件 1.2 bin1.全称:binary:二进制可执行文件2.作用:用实现系统的基本操作,例如对文件操作的增删改查.3.目录内容: 1.3 sbin1.全称:super binary–超级用户可执行的操作2.作用:存放root用户才能执行的一些操作 1.4 etc1.全称:etcetera:等等2.作用:存放系统所有的配置文件(包括通过指令yum,让系统自动安装的程序的配置文件)3.重要目录: /etc/rc.d 放置开机和关机的脚本。 /etc/rc.d/init.d 放置启动脚本 /etc/xinetd.d 配置xinetd.conf可以配置启动其他额外服务。 1.5 sys1.全称:system2.作用:存放当前运行的硬件设备的驱动程序信息 1.6 dev1.全称:device2.作用:作为访问外部设备文件的接口,也就是所谓即设备的驱动程序,插入U盘之后在dev中显示为/dev/sd1,说明提供了一个接口,可以手动将U盘加载到指定目录(例如是/mnt/usb1)–&gt;mount /dev/sd1 /mnt/usb1 1.7 mnt1.全称:mounted–挂载2.作用:被系统管理员使用,用来手动加载其他的文件系统.(手动加载) 1.8 media作用:用来加载移动设备(可移除的)的目录(自动加载) 二.用户类目录2.1 home作用:主要是其他普通用户的家目录 2.2 root作用:是root用户的家目录 三.应用程序类目录3.1 lib/lib 64作用:主要是用来存放在系统/程序运行当中所需要的一些库. 3.2 opt作用:就是安装第三方程序,自己手动编译的. 3.3 usr1.全称:Unix Software Resource2.作用:系统自动安装的软件目录3.重要目录:1234/usr/local 主要存放那些手动安装的软件，/usr/bin 用于存放程序/usr/share 用于存放一些共享数据/usr/lib 存放一些不能不能直接运行，但却是许多程序运行所必需的一些函数库文件 3.4 selinux 主要用来加固操作系统，提高系统的安全性 四.信息类目录4.1 srv1.全称:service2.存放一些对外的服务数据,例如web,用户主动生产的数据、对外提供服务.(用户主动放入的一些文件) 4.2 var1.全称:variable2.作用:系统和程序运行当中产生的不可销毁的缓存文件(邮箱文件,数据库文件),日志记录(日志文件),只能手动清理.(产生的东西,需要手动清理) 4.3 tmp 保存了使用完毕之后,随时可以销毁的缓存文件,系统可以自动清理,可能是系统/程序自动产生,或者用户主动将文件放入该临时目录(缓存文件能自动清理) 4.4 lost+found 就是在断电之类突发情况下,为系统提供一个临时文件,用来恢复丢失的文件.这个目录平时是空的 4.5 proc1.全称:processes2.作用:操作系统运行时,整个系统的进程信息,内存资源信息,硬盘分区信息. 五.区别5.1 /mnt /media和/dev /media:自动加载外来设备目录; /mnt:手动挂载外来设备目录 /dev:为挂载外来设备提供接口 5.2 /srv、/var和/tmp/srv:用户主动提供的数据/var:系统.程序主动产生的缓存,日志数据,需手动清除/tmp:保存用完可以随时销毁的数据,可以说系统产生的,也可以是,用户临时放入的.总之系统可以随时清理.","categories":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/categories/linux/"},{"name":"知识点","slug":"linux/知识点","permalink":"http://yoursite.com/categories/linux/知识点/"}],"tags":[{"name":"目录结构","slug":"目录结构","permalink":"http://yoursite.com/tags/目录结构/"}]},{"title":"xshell连接云服务器详细步骤","slug":"xshell连接云服务器详细步骤","date":"2018-06-27T06:41:55.000Z","updated":"2018-11-13T02:54:40.000Z","comments":true,"path":"2018/06/27/xshell连接云服务器详细步骤/","link":"","permalink":"http://yoursite.com/2018/06/27/xshell连接云服务器详细步骤/","excerpt":"本文主要讲述了服务器的配置,主要使用了xshell与xftp这两款工具.配置过程出现比较多的错误,为此特地编写一个文章,便于回顾,避免再次入坑","text":"本文主要讲述了服务器的配置,主要使用了xshell与xftp这两款工具.配置过程出现比较多的错误,为此特地编写一个文章,便于回顾,避免再次入坑 一.服务器部分 1.1服务器购买 1.2服务器配置 1.2.1添加密钥 1.2.2用户密码修改[可选] 二.xhsell部分 1.1软件下载 1.2软件配置 一.服务器部分1.1服务器购买 服务器,假如是学生,直接购买学生套餐,操作系统选centos7.4 64位就可以,具体选择如下图:完成购买之后在,控制台可以看到,如图: 1.2服务器配置1.2.1添加密钥 选择添加,随机命名就可以,这个是用来之后xshell链接云服务器的.之后下载,一般放到对应的xshell的安装根目录就可以.没有点击下载,密钥就不会添加.点击云主机界面的登录:得到如下界面: 注意:由于创建了密钥,所以密钥的优先级高于密码,所以如果选择密码登录,有时会出错,提示要使用密钥登录. 注意上图,用户名是root,不是云主机上的那个ID,最后得到成功登录的界面: 1.2.2用户密码修改[可选]由于初始密码,在你购买成功之后,会发到你相应的邮箱中,如果不放心,可以手动修改.方式就是: 二.xhsell部分1.1软件下载 安装xshell6与xftp,最后安装得到如下界面,: 1.2软件配置1.xshell配置:(1)配置连接主机地址(2)配置身份验证 注意:登录方法一定是用公钥来登录的,用户名是root,这个一定注意,密码可以不用填写,直接点击连接即可,出现下述界面 否则会出现: 所选择的用户秘钥未在远程主机上注册,这个怪问题2.xftp配置: xftp主要为了方便管理云服务器上的文件(1)登录界面: 基于之前的基础,先点击xshell上的文件夹,具体位置见下图之后出现一个配置界面. 注意红框的部分,还是以公钥进行登录.选择对应的文件. 最后在弹出的页面当中点击接受性保存即可.得到链接成功的界面:","categories":[{"name":"知识图谱","slug":"知识图谱","permalink":"http://yoursite.com/categories/知识图谱/"},{"name":"软件安装","slug":"知识图谱/软件安装","permalink":"http://yoursite.com/categories/知识图谱/软件安装/"}],"tags":[{"name":"xshell","slug":"xshell","permalink":"http://yoursite.com/tags/xshell/"},{"name":"云服务器","slug":"云服务器","permalink":"http://yoursite.com/tags/云服务器/"}]},{"title":"linux 常用命令","slug":"linux 常用命令","date":"2018-06-27T06:25:33.000Z","updated":"2018-11-13T02:34:18.000Z","comments":true,"path":"2018/06/27/linux 常用命令/","link":"","permalink":"http://yoursite.com/2018/06/27/linux 常用命令/","excerpt":"本文主要讲述了在linux下(以centos为例),各种常用操作,主要分为几个部分:目录操作,文件操作,过滤(grep),管道(|),重定向(&lt;,&gt;),运维,总共六个部分,并给出了相应的代码实例,算是比较完整的总结了基本linux的常用操作命令(ping,netstat,ps)","text":"本文主要讲述了在linux下(以centos为例),各种常用操作,主要分为几个部分:目录操作,文件操作,过滤(grep),管道(|),重定向(&lt;,&gt;),运维,总共六个部分,并给出了相应的代码实例,算是比较完整的总结了基本linux的常用操作命令(ping,netstat,ps) 一.目录操作: 1.1创建 1.2切换 1.3移动 1.4删除 1.5查看目录下的文件 二.文件操作: 2.1创建 2.2复制 2.3删除 2.4查看 2.5 修改 三.过滤 四.管道((多个命令的组合)) 五.重定向 六.运维 6.1 ping 6.2 netstat 6.3 ps 一.目录操作:1.1创建mkdir -v/p/m 文件夹/及其路径，其中:1.-v:显示创建之后的具体信息:全称verbose:详细1mkdir: created directory ‘test1’ 2.-p:创建多级目录的时候才 全称:parent 123456[root@VM_0_17_centos ~]# mkdir -p file3/file4[root@VM_0_17_centos ~]# lsfile1 file2 file3[root@VM_0_17_centos ~]# cd file3[root@VM_0_17_centos file3]# lsfile4 3.-m 类似权限模式 全称model 就会发现这个对应的文件夹是有颜色的.4.创建一个目录结构,一般用实现{}1234567891011[root@VM_0_17_centos appinfo]# ls##创建一个目录结构,一般用实现[root@VM_0_17_centos appinfo]# mkdir -vp test1/&#123;bin,doc/&#123;appinfo,product&#125;,log/&#123;format,history&#125;&#125;mkdir: created directory ‘test1’mkdir: created directory ‘test1/bin’mkdir: created directory ‘test1/doc’mkdir: created directory ‘test1/doc/appinfo’mkdir: created directory ‘test1/doc/product’mkdir: created directory ‘test1/log’mkdir: created directory ‘test1/log/format’mkdir: created directory ‘test1/log/history’ 1.2切换1.cd /:切换到根目录 12345[root@VM_0_17_centos bin]# cd /#root也就是常说~,$HOME也在根目录当中[root@VM_0_17_centos /]# lsbin data etc lib lost+found mnt proc run srv tmp varboot dev home lib64 media opt root sbin sys usr 2.cd ..与cd - cd ..是返回上一层目录， cd -是返回到上一次的工作目录，1234如果当前目录是/执行cd /usr/local再执行cd ..就是到 /usr而执行cd -就是到/ 3.相对路径与绝对路径(1)相对路径:12345[root@VM_0_17_centos file5]# cd ./file3-bash: cd: ./file3: No such file or directory##其实就是先使用cd .. 返回上一层,之后进入该层的目录[root@VM_0_17_centos file5]# cd ../file3[root@VM_0_17_centos file3]# (2)绝对路径 1.3移动方式:1mv 源目录名 目标目录名 12345678[root@VM_0_17_centos ~]# mkdir -v fileMovemkdir: created directory ‘fileMove’#注意两个目录名最好是绝对路径,否则会导致[root@VM_0_17_centos ~]# mv ~/fileMove ~/file2[root@VM_0_17_centos ~]# cd file2[root@VM_0_17_centos file2]# lsfile3 file5 fileMove[root@VM_0_17_centos file2]# 1.4删除方式 :1rm [-i/r/f] 目录名 # 绝对目录(最保险) 其中: f 忽略不存在的文件，从不给出提示。 –force r 指示rm将参数中列出的全部目录和子目录均递归地删除。-recursion i 进行交互式删除。 -interaction 1.5查看目录下的文件(1)查看目录结构安装tree:1yum install tree 之后所在文件夹直接输入tree就可以; tree 以树状图显示所有文件 tree -L N 以树状图显示所有文件，子文件夹显示到第 N 层 具体情况可以查看tree help (2)ls123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# ls -s 在每个文件的后面打印出文件的大小。 size(大小)[root@VM_0_17_centos ~]# ls -stotal 124 file1 4 file2 4 file3#ls -S 以文件的大小进行排序[root@VM_0_17_centos ~]# ls -Sfile1 file2 file3#ls -L 列出文件的链接名。Link（链接）[root@VM_0_17_centos ~]# ls -Lfile1 file2 file3#ls -A 列出除了”.”和”..”以外的文件。[root@VM_0_17_centos ~]# ls -A.bash_history .bash_profile .cache .cshrc file2 .pip .ssh .viminfo.bash_logout .bashrc .config file1 file3 .pydistutils.cfg .tcshrc#ls -t 按时间进行文件的排序 Time(时间)[root@VM_0_17_centos ~]# ls -tfile2 file1 file3#ls -F 在每一个文件的末尾加上一个字符说明该文件的类型。”@”表示符号链接、”|”表示FIFOS、”/”表示目录、”=”表示套接字。[root@VM_0_17_centos ~]# ls -Ffile1/ file2/ file3/#ls -l 列出文件的详细信息，如创建者，创建时间，文件的读写权限列表等等。[root@VM_0_17_centos ~]# ls -ltotal 12drwxr-xr-x 4 root root 4096 Jun 26 20:45 file1drwxr-xr-x 5 root root 4096 Jun 26 21:43 file2drwxr-xr-x 3 root root 4096 Jun 26 20:43 file3#ls -a 列出文件下所有的文件，包括以“.“开头的隐藏文件（linux下文件隐藏文件是以.开头的，如果存在..代表存在着父目录）[root@VM_0_17_centos ~]# ls -a. .bash_logout .cache file1 .pip .tcshrc.. .bash_profile .config file2 .pydistutils.cfg .viminfo.bash_history .bashrc .cshrc file3 .ssh# ls -R 将目录下所有的子目录的文件都列出来，相当于我们编程中的“递归”实现[root@VM_0_17_centos ~]# ls -R.:file1 file2 file3./file1:file2 file4./file1/file2:./file1/file4:test testbin testdoc testlog./file1/file4/test:bin doc log./file1/file4/test/bin: 二.文件操作:2.1创建1.cat&gt;&gt;filename 可以对新建文件进行编辑并且修改123456[root@VM_0_17_centos ~]# cat&gt;&gt;new1.jav#换行输出,直接enter就可以#include &lt;&gt;^[[Dstdio.h^[[C#使用ctrl+d 保存文件并且退出[root@VM_0_17_centos ~]# cat new1.jav#include &lt;stdio.h 2.touch filename 只能创建新的空文件,不能编辑3.创建连接文件 硬连接:就是一个索引节点可以被多个文件共同指向1234567891011#设置硬连接:使得new1.jav和hardlink.txt这两个文件指向相同的索引节点--475166[root@VM_0_17_centos ~]# ln new1.jav hardlink.txt[root@VM_0_17_centos ~]# lsfile1 file2 file3 hardlink.txt new1.jav[root@VM_0_17_centos ~]# ll -itotal 20475155 drwxr-xr-x 4 root root 4096 Jun 26 20:45 file1475156 drwxr-xr-x 5 root root 4096 Jun 26 21:43 file2475157 drwxr-xr-x 3 root root 4096 Jun 26 20:43 file3475166 -rw-r--r-- 2 root root 25 Jun 27 10:11 hardlink.txt475166 -rw-r--r-- 2 root root 25 Jun 27 10:11 new1.jav 软连接:一个连接文件,保存着另一个文件的位置,类似快捷方式 -s或者–symbolic：对源文件创建软连接。 2.2复制方式:1cp [option] 源文件/目录 目标文件/目录 option参数说明:12345 -r:代表文件夹下的所有子文件夹/文件.都将递归的复制到目标文件当中. -p:除了复制该文件/夹,并且复制其权限和文件/夹的修改时间. -f:假如覆盖已有文件/夹的时候,不给出提示. -i:正好与-f相反,不管怎样都会有提示. -a:不仅把文件夹下的所有子文件夹/文件复制到目标目录/文件下,并且文件夹属性/保留链接. 2.3删除1.删除多个文件基本格式为:123456# [-rf] 代表的是可选的项,递归强制-recursion force代表#该文件夹下的所有子文件都删除rm [-rf] 文件(包括类型) 文件(包括类型) rm *.java *.txt#删除n开头的java文件,删除m开头的文件,没有指定类型rm n*.java m* 在创建多个文件中也适用(空格隔开)1234567891011121314[root@VM_0_17_centos ~]# rm testNewFile test.txtrm: remove regular empty file ‘testNewFile’? rm: remove regular file ‘test.txt’? [root@VM_0_17_centos ~]# lstestNewFile test.txt[root@VM_0_17_centos ~]# lstestNewFile test.txt##一般跳到指定文件下,这样安全,跨目录删除也可以但是不安全[root@VM_0_17_centos ~]# rm testNewFile test.txt##一定要输入yes,直接按空格无效删除rm: remove regular empty file ‘testNewFile’? yesrm: remove regular file ‘test.txt’? yes[root@VM_0_17_centos ~]# ls[root@VM_0_17_centos ~]# 2.删除特殊要求的文件123456789101112[root@VM_0_17_centos ~]# lsnew1.csv new1.java new2.py new.txt## 删除 特定类型的文件[root@VM_0_17_centos ~]# rm -rf *.txt *.java[root@VM_0_17_centos ~]# lsnew1.csv new2.py[root@VM_0_17_centos ~]# touch new2.csv[root@VM_0_17_centos ~]# lsnew1.csv new2.csv new2.py#删除[root@VM_0_17_centos ~]# rm -rf *.csv n*.py[root@VM_0_17_centos ~]# ls 2.4查看形式:1cat ~/.bash_history 其中cat就是 concatenate连接,联系含义,就是查看文件的内容. 2.5 修改1.重命名方式:1mv 源文件(包括类型) 目标文件(包括类型) 例子:123456789[root@VM_0_17_centos ~]# lsfile1 file2 file3 hardlink.txt kk.index new1.jav new2.jav#直接将 mv 源文件/文件夹(包括类型) 目标文件/文件夹(包括类型)就可以[root@VM_0_17_centos ~]# mv file1 file6[root@VM_0_17_centos ~]# lsfile2 file3 file6 hardlink.txt kk.index new1.jav new2.jav[root@VM_0_17_centos ~]# mv new1.jav new3.java[root@VM_0_17_centos ~]# lsfile2 file3 file6 hardlink.txt kk.index new2.jav new3.java 2.修改文件(1)方式: vi +5 filename:说明跳到第几行 vi filename–&gt;输入i,进入编辑模式–&gt;esc退出保存–&gt;之后输入:wq(强制保存并退出)(2)代码如下:1234567891011121314[root@VM_0_17_centos ~]# vi kk.index~....~#输入 i ,进入编辑模式-- INSERT --#写好之后,用esc退出编辑模式,-- INSERT --会消失今天天气真好~ :wq #输入之后按enter,就会返回到,如下界面:[root@VM_0_17_centos ~]# vi kk.index#查看修改的文件[root@VM_0_17_centos ~]# cat kk.index今天天气真好 (3):wq与:x的区别: :wq是强制写入并保存退出,只要进入了编辑模式,最后文件的修改时间一定会更新,在编译代码的时候,即使在之前的编辑模式下文件没有改动,但是由于时间修改了,编译器都会重新编译,增加时间. :x,只要没有改动,就不会更新时间.(4)退出方式1234:q ——直接退出 如果在文本输入模式下修改了文档内容则不能退出:wq ——保存后退出:x ——保存后退出:q! ——不保存内容 强制退出 (4):选项参数1234:set number---在命令模式下，用于在最左端显示行号；:set nonumber---在命令模式下，用于在最左端不显示行号；:行号---光标跳转到指定行的行首；:$---光标跳转到最后一行的行首； 三.过滤方式:12grep [option] pattern 文件/目录#例子:grep -r 'linux' /var/log/ 其中option参数:1234-v ---revert Match 不匹配的也显示出来-i ---ignore case 忽略字母大小写-n ---line number 显示符合样式的列号-c ---count 计算符合样式的个数 四.管道((多个命令的组合))“|”是管道命令操作符,简称管道符 管道命令只处理前一个命令正确输出，不处理错误输出；(只输出到正确的命令) 管道命令右边命令，必须能够接收标准输入流命令才行; (只接收正确的输出)123[root@VM_0_17_centos /]# ls /etc | grep -n 'ssh'169:ssh[root@VM_0_17_centos /]# 五.重定向形式: &gt;或者&lt;,箭头指向哪里,文件/输出就流向哪里 把in.ddf这个文件当作邮件的内容,主题是’testMail’发送到1213345160@qq.com的邮箱当中, -s代表发送的指令 六.运维6.1 ping(1)方式: ping [option] 网址(2)目的: 就是或者对应网址的ip地址,和连接情况(3)option参数说明: -c:向某网址发送测试信息4次,情况如图: -s:向某网址发送的测试信息的大小,情况如下图: 6.2 netstat(1)方式: netstat 参数(2)目的: netstat 命令用于显示各种网络相关信息，如网络连接, 路由表, 接口状态等等(3)常见参数1234567891011-a (all)显示所有选项，默认不显示LISTEN相关-t (tcp)仅显示tcp相关选项-u (udp)仅显示udp相关选项-n 拒绝显示别名，能显示数字的全部转化成数字。-l 仅列出有在 Listen (监听) 的服務状态(例如: netstat -lt--列出所有处于监听状态的tcp端口)-p 显示建立相关链接的程序名-r 显示路由信息，路由表-e 显示扩展信息，例如uid等-s 按各个协议进行统计-c 每隔一个固定时间，执行该netstat命令。提示：LISTEN和LISTENING的状态只有用-a或者-l才能看到 (4)netstat状态解释 从整体上看，netstat的输出结果可以分为两个部分： 一个是Active Internet connections，称为有源TCP连接，其中”Recv-Q”和”Send-Q”指%0A的是接收队列和发送队列。这些数字一般都应该是0。如果不是则表示软件包正在队列中堆积。这种情况只能在非常少的情况见到。 另一个是Active UNIX domain sockets，称为有源Unix域套接口(和网络套接字一样，但是只能用于本机通信，性能可以提高一倍)。Proto显示连接使用的协议,RefCnt表示连接到本套接口上的进程号,Types显示套接口的类型,State显示套接口当前的状态,Path表示连接到套接口的其它进程使用的路径名。(5)netstat -tulpn 查看所有的端口信息, 包括 PID 和进程名称: Local Address:该进程在本机上的地址 Foreign Address: 该进程在远程服务器上连接的地址 pid:process id :进程号 6.3 psps:process status–报告当前系统的进程状态(1)形式: 1ps [option] (2)option参数说明:12345-aux: -au:显示较详细的资讯---alluser-w:加宽显示更多信息--wide-a:列出所有进程-x:显示没有控制终端的进程 (3)例子:12345678910111213141516171819201.PID----进程编号 TTY-----与进程关联的终端2.%CPU----进程的cpu占用率 %MEM---进程的内存占用率3.VSZ----进程所使用的虚存的大小4.RSS----进程使用的驻留集大小或者是实际内存的大小5.TTY 与进程关联的终端（tty）--TeleTYpe伪终端6.STAT 检查的状态：进程状态使用字符表示的，如R（running正在运行或准备运行）、S（sleeping睡眠）、I（idle空闲）、Z (僵死)、D（不可中断的睡眠，通常是I/O）、P（等待交换页）、W（换出,表示当前页面不在内存）、N（低优先级任务）T(terminate终止)7.START （进程启动时间和日期）8.TIME （进程使用的总cpu时间）9.COMMAND （正在执行的命令行命令）#显示所有使用者的进程[root@VM_0_17_centos /]# ps -auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.1 43200 3628 ? Ss Jun26 0:03 /usr/lib/systemd/systemd --switched-root --system --deserialroot 2 0.0 0.0 0 0 ? S Jun26 0:00 [kthreadd]root 3 0.0 0.0 0 0 ? S Jun26 0:00 [ksoftirqd/0]root 475 0.0 0.0 55452 892 ? S&lt;sl Jun26 0:00 /sbin/auditdlibstor+ 510 0.0 0.0 8532 800 ? Ss Jun26 0:00 /usr/bin/lsmd -droot 512 0.0 0.3 238824 7148 ? Ssl Jun26 0:03 /usr/sbin/rsyslogd -npolkitd 517 0.0 0.5 536212 11024 ? Ssl Jun26 0:01 /usr/lib/polkit-1/polkitd --no-debugroot 518 0.0 0.0 24204 1664 ? Ss Jun26 0:01 /usr/lib/systemd/systemd-logind 最常用:ps -aux,然后再利用一个管道符号导向到grep去查找特定的进程,然后再对特定的进程进行操作,如下图所示:","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"},{"name":"知识点","slug":"Linux/知识点","permalink":"http://yoursite.com/categories/Linux/知识点/"}],"tags":[{"name":"命令行","slug":"命令行","permalink":"http://yoursite.com/tags/命令行/"}]},{"title":"第15周报告-暨YODA-QA了解","slug":"第15周报告-暨YODA-QA了解","date":"2018-06-25T12:37:47.000Z","updated":"2018-11-13T02:57:58.000Z","comments":true,"path":"2018/06/25/第15周报告-暨YODA-QA了解/","link":"","permalink":"http://yoursite.com/2018/06/25/第15周报告-暨YODA-QA了解/","excerpt":"本周主要学习了开源QA:YODA-QA,了解它的运行原理,架构流程,之后了解它的代码结构及其代码模块.","text":"本周主要学习了开源QA:YODA-QA,了解它的运行原理,架构流程,之后了解它的代码结构及其代码模块. infobox [实体链接] NOSQL 概念 图数据库 [原因][^2] [neo4j优缺点][^1] JanusGraph Cap定理 [janusGraph架构图][^3] 开源QA [种类][^7] [yoda-QA][^5] 问题分析(核心部分) 答案生成 结构化文本 非结构化文本: 答案分析 答案合成( Answer Merge) 答案评分( Answer Score) 代码结构 代码模块 OpenEphyra（Java开源） open-QA（Java开源） Watsonsim（Java开源） YODA-QA的工作流程主要有五个部分组成: 问题分析:主要是对问题进行词性,句法标注,从而实现分词,最终得到NER(实体命名识别);之后得到基于要素的QA特征:线索,焦点,LAT 答案生成:通过根据问题分析得到的QA特征对数据源的数据进行比较,也就是相似度的计算,其中非结构化数据:有两方面: 针对文档(一个文档可能有多个文章)数据:直接扫描文档全文,之后通过评分得到排名最靠前的几篇的标题作为候选答案. 或者直接进行每篇文章标题(只要是标题就可以),然后提取出相应的QA特征,之后与问题的QA特征进行比对,使用评分的方式来实现排序,得到排名靠前的几篇文章.之后进行全文搜索,将每个段落作为每个独立的部分,然后根据问题的线索在每个段落的出现频率,作为评分的标准,最后得到排名前几的段落,其余文章也是在这样.然后进行信息提取,提取相应QA特征,然后进行信息分析得到该阶段的候选答案. 之后进行答案分析:也就是为各个候选答案确定所对应的问题类型,LAT,所以这阶段主要是修改LAT. 答案合并:合并相同候选答案的文本,减少数据量,一定吃程度有利于减少过拟合,减少异常答案的出现. 候选答案的评分排序.infobox 如下图所示，是维基百科关于“清华大学”的词条内容。可以看到，在右侧有一个列表，标注了与清华有关的各类重要信息，如校训、创建时间、校庆日、学校类型、校长，等等。在维基百科中，这个列表被称为信息框（infobox），是由编辑者们共同编辑而成。信息框中的结构化信息是知识图谱的直接数据来源。 [实体链接] [手段][^9]是通过将自然语言中的文本不知识库中的条目进行链接 其实就是求从句子提取出的实体指称项与候选实体间的链接关系. 例如: 在一个句子当中,含有乔布斯及其苹果这两个实体,也叫实体指称项,假设乔布斯没有歧义问题,但苹果这个实体指称项有歧义问题,因为它有多个候选实体(可能是水果,公司,电影的含义),如何找出最符合该情景下的候选实体,也就是一个实体链接的问题.“苹果”和“乔布斯”通过命名实体消歧确定为“苹果(公司)”。实体链接可以利用上下文相似度、文本主题一致性实现，主要有两类方法： 实体链接方法——统计方法 通过知识库和大规模语料+深度学习模型实现。 实体链接方法——图方法 计算最大似然链接结果的算法(通过算出实体指称项与各个候选实体的概率,之后排序,找到最大概率的候选实体,并使其与实体指称项进行链接) 那么在知识图谱上的实体链接问题定义为：其输入包括一个源知识图谱和一个目标知识图谱。其输出为一个实体链接对集合，表示从源图谱中链接到目标图谱中的实体对(实体指称项与最大概率的候选实体)。 NOSQL概念关于NoSQL有两种解释：一种是NonSQL，表示NoSQL与传统的关系数据库有本质上的不同；另一种是Not onlySQL，表示NoSQL不仅仅能像普通（关系）数据库一样存储结构化数据，也可以存储非结构化数据。在数据规模很大的场景下，NoSQL通常是更好的选择，因为它易于横向拓展和分布式部署，相比传统关系数据库更加灵活。通常NoSQL有以下四种： KV数据库，存储key-value数据，如Redis、MemcacheDB等–数据模型比较简单的情况下,便于读写. 列式数据库（datastore），如HBase、Cassandra等(按照属性来存)–行式数据库(按照一个记录来存–代表mysql,传统数据库), 文档数据库，如MongoDB、RethinkDB等–也称为文档型数据库，旨在将半结构化数据存储为文档,也可以理解为将经常查询的数据存储在同一个文档中而不是存储在表中 。文档数据库通常以 JSON 或 XML格式存储数据如果一个应用程序需要存储不同的属性 以及大量的数据那么文档数据库将会是一个很好的选择 图数据库（graph database），如Neo4j、JanusGraph等 图数据库[原因][^2] 1.因为传统数据库的一条记录,记录众多属性及其属性值,对于查询某种关系非常不方便;特别当数据量越多的时候,劣势愈加明显. 2.图形数据库:节点/边的类别及属性,都是分开存储的,都用指针进行联系,极大有助于提高查询速度. 3.人们需求不一样传统数据库存储的是数据,追求完整性,图形数据库存储的是关系,追求精准性,所以当追求精准性的时候,图形数据库的优势就开始显现. 图数据库就是用“图”的方式来存储数据。比如官方文档中的例子：在上图中我们可以发现元素有：节点（Vertex）、有向边（Edge），属性（Property）。节点表示实体，边表示关系，属性定义了实体和关系中的相关信息。这跟ER图很像，关系数据库也可以表示为ER图，但是在关系数据库中，是无法沿着边进行访问的，只能通过链式进行遍历和计算。而在图数据库中，存储的是真实的图，可以沿着图的路径进行遍历和计算。举个例子，比如要找途中Hercules的父亲的父亲，在关系数据库中，需要先根据Herclues这一行的father列定位到他的父亲，再访问他的父亲通过同样的方法定位到父亲的父亲，需要两次索引定位；而在图数据库中可以直接沿着father边找到对应的人。（在关系数据库存在外键的时候，可以避免join操作，效率更高） [neo4j优缺点][^1](1)优点 学习资源多:作为较早的一批图形数据库之一，文档和各种技术博客较多。 安装过程较为简单 稳定性较好,市场份额大.(2)缺点 Neo4j只适合适合存储”修改较少，查询较多，没有超大节点“的图数据 图数据结构导致写入性能差，实时性读写跟不上，大数据量导入麻烦，官方提供的loadcsv模式性能也不够理想，neo4j-import倒是不错，但是只能用于数据库初始化局限太大。 不能进行分布式存储，想要提高性能和容量只能加大单个机器的硬盘 企业版闭源且费用昂贵，社区版免费开源，但是企业级项目实用性不强. JanusGraph JanusGraph是Titan的一个fork。Titan项目创建于2012年，于2016年停止维护，是一个方便拓展的图数据库，支持HBase、Cassandra等作为后端（BerkleyDB不提了），ES、Lucene等做全文索引，以TinkerPop作为图的查询和计算框架。2017年，JanusGraph项目fork了Titan，到目前已经推出了0.2.0版本。 Cap定理JanusGraph的CAPCAP定理说的是：一个分布式计算机系统无法同时满足以下三点（定义摘自Wikipedia）： 一致性（Consistency) ，所有节点访问同一份最新的数据副本 可用性（Availability)，每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据 分区容错性（Partitiontolerance），以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择。关于JanusGraph在CAP理论上的侧重，是要看底层存储的。如果底层是Cassandra，那么就是偏向于AP（Cassandra是最终一致性的）；如果底层是HBase，就是偏向于CP（强一致性）；BerkleyDB单机不存在这个问题。 [janusGraph架构图][^3] JanusGraph是一个图数据库引擎. JanusGraph集中在图的序列化,图的数据模型和高效的查询. 此外JanusGraph依赖hadoop来做图的统计和批量图操作.JanusGraph为数据存储,索引和客户端访问实现了粗粒度的模块接口.JanusGraph的模块架构能和和许多存储,索引,客户端技术集成. 可以简便的扩展新的功能.其中与janusGraph的交互方式有两种:本地交互/远程交互.具体情况如下: (1)内嵌在本地应用的JanusGraph图形数据库,运行Gremlin查询,JanusGraph缓存和事务处理和应用在同一个JVM上.具体实现方式就是调用JanusGraph的接口,然后进行对janusGraph里面的数据进行查询—-OLAP (2)通过提交Gremlin查询, 和本地或者远程的JanusGraph实例交互,由于JanusGraph原生支持Gremlin Server 组件(TinkerPopstack)—就是一个图计算框架.就可以直接在janusGraph的数据当中进行查询.不管是哪种交互都是通过Gremlin查询语言进行的,后台存储主要使用的是Apache HBASE,Apache Cassandra,Oracle Berkeley DB,索引有Elasticsearch,Apache Solr,Apache Lucene, 加快查询和复杂查询.janusGraph的内部结构: 底层是存储和索引层 第二层就是数据库层 第三层就是为了保护数据的安全而提供的一个数据访问的接口层,便于与Gremlin查询进行交互. 第四层就是查询层.(3)补充:OLTP主要是事务处理方面的(操作数据)，而OLAP主要是用于数据分析(分析数据作决策)。一般的数据库通常都是OLTP，因为主要用于在线记录数据，离线进行数据分析 开源QA[种类][^7][yoda-QA][^5] 其中apache的UMIA还提供了一个语义搜索接口,但是ibm的DEEP-QA的UIMA保留了这个搜索功能 YodaQA采用基于UIMA的流水线架构(UIMA就是对非结构化数据/结构化数据作了一个规范化处理,基本单元就是CAS对象)，实行插件化管理，根据需要预先加载相关组件。将整个问答过程划分为若干阶段，每个阶段对应若干插件，将这些插件安装在流水线架构上，完成智能问答过程.YodaQA是一个开源的事实型问答系统，他能够利用即时的信息抽取从数据库和非结构化的文本语料中抽取答案.具体框架：总共有五个过程： 问题分析–>候选答案产生–>候选候选答案分析–>候选答案合并–>各个候选答案评分. 问题分析(核心部分)问题分析：生成词性标注（POS）标签和依存句法分析结果，识别出命名实体；生成若干QA特征：线索、焦点、LAT特征。(从问题文本当中抽取出自然语言特征,并生成基于要素的QA特征) 线索（Clue）：问题内容的中心词，用于查询候选答案。可以是多种成分，如名词短语、名词标记、选择动词、命名实体、问题句主题等，不同成分对应的权重存在差异。如果线索对应于enwiki文章题目或者重定向别名，那么它的权重会被提升。 焦点（Focus）：问题句子的中心点，指示被查询对象。基于依存句法分析的结果，使用简单启发式规则抽取6个要素，按照规定格式构成焦点。 LAT（Lexical AnswerType，词汇答案类型）：描述和问题相匹配的答案类型，它不是预先定义的类别，可以是任意英语名词。 生成LAT方法：将单词映射为名词（从“where”到“location”），将副词归一化（从“hot”到“temperature”） 答案生成 根据问题生成的QA特征(线索,LAT,焦点)，在知识库进行主搜索，具体运行模式如下图:其生成答案的来源有: 结构化文本 搜索结构化数据(RDF三元组),DBpedia,freebase是RDF三元组的知识库,将每个问题的线索作为主语,LAT作为谓语,之后得到三元组的宾语作为候选答案.搜索范围是DBpedia本体和属性命名空间,Freebase的RDF转储库当中. 非结构化文本:(1)搜索文档 对文章正文进行全文的线索搜索,之后选择排名前20的文章,并将他们的标题作为候选答案(看上图). (2)标题分析 对非结构化文本的标题进行处理: 先分析各个文章的标题,得到各个文章的对应的QA特征(焦点,线索,LAT),之后搜索得到跟问题分析最贴合的几篇文章,然后对这几篇文章进行全文分析: (3)全文分析/标题分析+信息提取: 主要是按段落来进行分析—(默认原则,把每段第一句看成是一个中心句),之后对每段进行QA特征的提取.(标题也会进行信息提取 根据线索在每段出现的次数进行权重的设置,从而进行相应的评分,之后对每篇文章的段落评分前三的段落,作为QA特征的提取(4)信息分析 将提取后的信息(组织出来的核心句子)进行分析,通过执行了NLP流水线得到几个候选答案.具体方法就是: 将所有的命名实体,与名词短语都转换为一个个候选答案,此外,对于主语是问题的LAT的句子,其宾语需要转换为一个个候选答案.(针对英文的句法) 上述过程就是将不同数据来源的答案整合成一堆的候选答案 答案分析 生成各种基于细节分析的答案特征,最终的是问题的类型及其约束. 为每个候选答案生成很多特性(例如),目的是为了对问题类型的匹配,具体方法是:对候选答案的类型进行强制转换,检查其是否和问题的LAT相匹配,根据中这个方式来进行候选问题的LAT修正.LAT修正方法如下: A:通过命名实体识别触发器模型生成答案的LAT,可以识别日期、地点、金额、组织、百分比、人名和时间等类型 B:将答案量化为表示LAT的数字,通过统计分析修正LAT C:在 WordNet和实例对中查找得到的答案焦点(句法树根节点)被用于生成LATs D:在DBpedia中查找到的答案焦点,其本体被用于生成LATSE:来自结构化知识库的答案,其属性名被视为LAT为了实现问题和答案LATS之间的类型强制转,通常需要使用WordNet的上位关系分析。例如:科学家”被归纳到“人”长度”被归纳到“数量词. 答案合成( Answer Merge) 合并相同文本对应的候选答案,为了避免过拟合,否则就会造成过于罕见(在问题中比例小于1%,在答案中小于0.1%)的特征出现。 答案评分( Answer Score) 使用机器学习分类器对答案进行评分。每个特征要生成若干二元附加特征,表示特征是否被生成、答案集合是否被归一化(答案中所有特征值均值是0、方差是1)使用LR分类器,将特征向量转换为[0,1]间的分值。使用L2正则化目标函数,按照标准训练数据,可以得到权重向量。为了在准确率和召回率之间达到平衡,正向答案(在答案总数占有比例p=0.03，因为毕竟是少数的)需要进行加权,公式是0.5/p 代码结构 代码模块 cz. brmlab. yodaqa. model:YodaQA中常用数据类型,包括:问题、候选答案、LAT、查询结果、答案命中列表等。(由于UIMA编译机制,将会将问题转换成规范化格式,统称是CAS对象,在不同的阶段,为了便于描述,需要将CAS对象细化) cz. brmlab. yodaqa.flow:用于执行流水线的UIMA后端工具。 cz. brila. yodaqa.io:用于问答交换、调试转储等功能的模块。 cz. brmlab. yodaqa.provider:为各种依赖于UMA注释器的数据源和分析工具提供封装。可以避免重新初始化,保持它们为单例。(问答过程中涉及到各种功能模块组件,具体表现以注释器的形式进行表现) cz. brmlab. yodaqa.analysis.*:具有各种分析工具,主要包括操作数据、读取和生成CAS对象等实际算法的注释器。 cz. brmlab. yodaqa.pipeline:管理流水线不同阶段上的模块,将多个模块综合在一起,形成问答系统所需要的特定功能 OpenEphyra（Java开源）基于Java开发的模块化、可扩展的问答系统、安装简单。从Web和其他来源检索自然语言问题的答案。 open-QA（Java开源）OpenQA是一个开放源码的问题回答框架，它统一了来自多个领域专家的方法。openQA的目的是提供一个共同的平台，通过简单的集成和度量不同的方法来促进进步. Watsonsim（Java开源）开放式问答系统。Watsonsim的工作原理是对问题、候选答案及其支持段落进行一系列操作。在很多方面，它类似于IBM的沃森(Watson)和Petr的YodaQA。它与OpenCog或Wolfram Alpha等基于更多逻辑的系统没有太大的相似之处。","categories":[{"name":"每周报告","slug":"每周报告","permalink":"http://yoursite.com/categories/每周报告/"},{"name":"2018","slug":"每周报告/2018","permalink":"http://yoursite.com/categories/每周报告/2018/"}],"tags":[]},{"title":"知识图谱综述","slug":"知识图谱综述","date":"2018-06-23T06:16:34.000Z","updated":"2018-11-13T03:15:12.000Z","comments":true,"path":"2018/06/23/知识图谱综述/","link":"","permalink":"http://yoursite.com/2018/06/23/知识图谱综述/","excerpt":"该篇是知识图谱的大综述片,文章目录主要分成7个模块,分别是:知识建模,知识获取,知识融合,知识计算,知识应用,最后引入一个质量评估.构成了知识图谱的主要构建流程,每个关键部分都给出相应的构建工具,对于开发知识图谱,有极大的提升.","text":"该篇是知识图谱的大综述片,文章目录主要分成7个模块,分别是:知识建模,知识获取,知识融合,知识计算,知识应用,最后引入一个质量评估.构成了知识图谱的主要构建流程,每个关键部分都给出相应的构建工具,对于开发知识图谱,有极大的提升. 一.通用知识图谱VS行业知识图谱 1.1 区别 1.2 领域知识图谱 1.2.1 挑战 1.2.2 解决方案 1.3 联系 二.关键技术 2.1 知识建模 2.1.1 具体方法 (1)实体抽取与合并 (2) 属性映射与归并 (3) 关系抽取 (4) 实体链接 (5) 动态事件描述 2.1.2 工具 2.1.3 关键技术及其难点 2.2 知识获取 2.2.1 概念 2.2.2 大纲 2.2.3结构化数据 2.2.3.1 实体识别 (1)基于规则与词典的识别 (2)基于统计学习的方法 (3)基于深度学习的方法 (4) 常用的实体识别工具及方法 2.2.4 关系／文本信息抽取 2.2.4.1解决方案 (1)OpenIE：从万维网中抽取关系二元组 (2)DeepDive：关系抽取工具 (3)Distant Supervision：远程监督学习 (5)Ratel语言 (6)总结 2.2.4.2 事件抽取 (1)方法 (2)事件定义： 2.2.4.3事理知识图谱 2.2.4.4 总结 2.2.5 结构化数据 2.2.6 工具 (1)D2R (2)包装器 (3)文本信息抽取 2.2.7 难点 2.3 知识融合 2.3.1 融合内容 2.3.1.1 数据层融合例子 2.3.2 知识融合关键技术与难点 2.4 知识存储 2.4.1 基本数据存储 2.4.2 存储方式 1. JanusGraph 2. RDF 2.1 RDF定义 2.2 基本元素 2.3 三元组 2.4.3 知识存储关键技术与难点 2.5 知识计算 2.5.1 计算方式 2.5.2 图挖掘计算 (2)目的 (3)特点 (4)实现方式 (5)典型框架 2.5.3 本体推理 本体的五元组表示 (3)本体知识推理工具——RDFox 2.5.4基于规则的推理 (1)基于规则推理工具——Drools 2.5.5补充 (1) 基于符号的知识表示 (1.1)方法 (1.2)优缺点 (2) 基于分布式的知识推理 (2.1)基于张量的模型 (2.2)基于翻译的模型：TRANSE (2.3)基于神经网络的模型 优缺点 2.5.6 关键技术与难点 图挖掘计算 本体推理与规则推理 2.6 知识应用 2.6.1 语义搜索 2.6.2 智能问答 难点 (1)基于信息检索的方法 (2)基于语义解析的方法 (3)基于规则的专家系统方法 (4)基于深度学习的方法 (5)最佳实践方法 基于语义解析的自动问答 2.6.3 可视化工具 2.7 质量评估 2.7.1可行方案 *## 一.通用知识图谱VS行业知识图谱### 1.1 区别| | 通用知识图谱 | 行业知识图谱 || :———–: | :——————————: | :———————————————————-: || 广度/深度 | 广度 | 深度 || 知识类型/来源 | 常识性知识, 百科知识，语言学知识 | 专业性知识（内部数据、互联网数据、第三方数据） || 精度 | 低 | 高 || 面向群体 | 普通用户 | 专业用户 || 代表 | 谷歌大脑 | 中医药知识图谱 || 类型 | | 数据类型多:结构化、半结构化、非结构化数据，且后两者越来越多 || 模式 | 数据模型固定 | 数据模式的不确定性:模式在数据出现之后才能确定；数据模式随数据增长不断演变 || 数据量 | | 数据量大：在大数据背景下，行业应用的数据的数量通常都以亿级别计算，通常在TB、PB级别甚至更多 || 获取难度 | 公共数据 | 私有数据 |### 1.2 领域知识图谱#### 1.2.1 挑战1.多源异构数据难以融合2.数据模式动态变迁困难3.非结构化数据计算机难以理解4.分散的数据难以统一消费利用#### 1.2.2 解决方案 • 挑战1：使用知识图谱（本体）对各种类型的数据进行抽象建模，基于可动态变化的“概念—实体—属性—关系”数据模型，实现各类数据的统一建模。 • 挑战2：使用可支持数据模式动态变化的知识图谱的数据存储，实现对大数据及数据模式动态变化的支持。 • 挑战3：利用信息抽取技术，对非结构化数据及半结构化数据进行抽取和转换，形成知识图谱形式的知识。 • 挑战4：在知识融合的基础上，基于语义检索、智能问答、图计算、推理、可视化等技术，提供统一的数据检索、分析和利用平台。### 1.3 联系- 通用知识图谱为行业知识图谱提供基础/体系,细化,则是需要搜寻相应的行业知识- 行业知识图谱能够通过融合到通用知识图谱当中## 二.关键技术 或者这张图(好好感觉)### 2.1 知识建模 就是建立图谱的数据模式,就是对整个知识图谱的结构进行定义,构建- 自顶向下的方法：专家手工编辑形成数据模式- 自底向上的方法： - 基于行业现有的标准进行转换- 从现有的高质量行业数据源（如业务系统数据库表）中进行映射#### 2.1.1 具体方法##### (1)实体抽取与合并​ 以实体为主体目标，实现对不同来源的数据进行映射与合并。（实体抽取与合并）##### (2) 属性映射与归并​ 利用属性来表示不同数据源中针对实体的描述，形成对实体的全方位描述(就是合并不同源的属性)。（属性映射与归并）##### (3) 关系抽取​ 利用关系来描述各类抽象建模成实体的数据之间的关联关系，从而支持关联分析。（关系抽取,不合并,但是会链接）##### (4) 实体链接​ 通过实体链接技术，实现围绕实体的多种类型数据的关联存储。（实体链接）##### (5) 动态事件描述​ 使用事件机制描述客观世界中动态发展，体现事件与实体间的关联；并利用时序描述事件的发展状况。（动态事件描述）#### 2.1.2 工具 主要是采用protege进行本体建模,但是建模工具,才用原生建模,建模速度效率低.对数据的处理量不是很好,容易内存外溢.同时protege支持网页版.有共享开发的功能.客户端是单机构建.#### 2.1.3 关键技术及其难点- 多人在线协同编辑，并且实时更新- 能够导入集成使用现有的（结构化）知识- 支持大数据量- 能够支撑事件、时序等复杂知识表达- 可以与自动算法进行结合，避免全人工操作### 2.2 知识获取#### 2.2.1 概念从原始数据中提取对业务有用的知识·原始数据 ·结构化、半结构化、非结构化·有用的知识 ·实体、关系、事件#### 2.2.2 大纲•非结构化数据 • 实体识别（识别实体） • 关系抽取（识别两个实体之间的关系） • 事件抽取（识别事件中的一组实体间的关系）－－＞（三者就像是不断递进的过程）• 结构化数据 • 映射机制• 半结构化数据 • 包装器#### 2.2.3结构化数据##### 2.2.3.1 实体识别实体识别就是在一个非结构化文本当中找出实体(主语,宾语),然后标注其类型及其位置.###### (1)基于规则与词典的识别 ·专家知识编写 工具：正则表达式，上下文无关文法． 困难：规则覆盖率低，规则冲突###### (2)基于统计学习的方法 前提：大量的标注数据 思想：利用单词的词性、上下文信息等特征进行预测 序列标注算法：HMM, CRF 工具包：StanfordNLP，CRF++，CRFSuite###### (3)基于深度学习的方法 ·深层特征挖掘 思路：利用深度学习生成更复杂的特征##### (4) 常用的实体识别工具及方法１．机器实体识别 • NLTK • Spacy • StanfordNLP • HanNLP • Tensorflow • PyTorch２． 人工实体识别 • 众包机制#### 2.2.4 关系／文本信息抽取| closeIE（面向特定领域抽取信息） | openIE（面向特定领域抽取信息） || :———————————————: | :——————————————————–: || 基于面向特定领域,深度,精度高 | 面向开放领域,基于广度,精度较低，基于语言学模式或者常识进行 || 为了做到高精度,那么需要先定义好需要抽取关系类型 | 关系类型不用事先预设好 || 规模较小 | 规模较大 |##### 2.2.4.1解决方案·OpenIE·规则引擎 ·SystemT：IBM Watson套件使用的组件 ·ANNIE (JAPE)：GATE中目前使用一套规则引擎 ·UIMA Ruta ·Ratel ·正则表达式：TokensRegex·机器学习模型 ·DeepDive###### (1)OpenIE：从万维网中抽取关系二元组• OpenIE的典型代表工具有ReVerb、TextRunner，英文为主• OpenIE工具准确率比较低，在垂直领域知识图谱构建中实用性不高。• 适合做信息的粗粒度筛选，然后在此基础上应用其它的信息抽取方法。###### (2)DeepDive：关系抽取工具•DeepDive用于提取实体之间的复杂关系，并对涉及这些实体的事实进行推断。• 用户只需要关心特征本身，而不是算法• 允许用户使用规则来提升结果的质量，也考虑用户反馈来提高预测的准确度• 性质 • Stanford的研究项目• 状态 • 维护状态，新功能已不在开发###### (3)Distant Supervision：远程监督学习 Distant Supervision利用知识库与非结构化文本对齐来自动构建大量训练数据，进而减少模型对人工标注数据的依赖，增强模型跨领域适应能力。• 两个实体如果在知识库中存在某种关系，则包含该两个实体的非结构化句子均能表示出这种关系。(１)．例子 • Steve Jobs, Apple在 Freebase中存在founder的关系， • Steve Jobs was the co-founder and CEO of Apple and formerly Pixar. 可以作为一个训练正例来训练模型。(２)．优势·Distant Supervision方法减少了模型对人工标注数据的依赖(３)．不足·假设过强，引入大量的噪声数据。·如 “Steven Jobs passed away the day before Apple unveiled iPhone 4s in late 2011”·数据构造过程依赖于 NER 等 NLP 工具，中间过程出错会造成错误传播问题。(４)．方法·引入先验知识作为限制；·利用图模型对数据样例打分，滤除置信度较低的句子；·利用多示例学习方法对测试包打标签；·采用attention机制对不同置信度的句子赋予不同的权值。###### (5)Ratel语言• 高效的机器学习与规则结合的文本挖掘系统• 具有一套完整的声明式语言（DSL）• 具有基于Eclipse的IDE开发环境，支持语法检查、结果查看与规则溯源、规则执行时间统计。• 可接入分词、词性识别、命名实体识别、分类、关键词抽取等机器学习模型。Ratel集成开发环境在ruta(基于规则的文本注释)中:http://uima.apache.org/ruta.html###### (6)总结目前还没有统一的实现各类信息抽取的现成工具。 •分词 实体识别工具： 对现有工具进行集成，依据抽取任务使用不同的工具:进行NLP分词、命名实体识别: • NLPIR、LTP、FudanNLP、Stanford NLP……• 关系抽取工具： • 规则系统 Ratel • DeepDive 对于行业抽取任务，需要针对性的方法来完成 • 通常基于已有的结构化知识进行远程监督学习##### 2.2.4.2 事件抽取事件抽取可以分为预定义事件抽取和开放域事件抽取 • 领域知识图谱中主要为预定义事件抽取###### (1)方法• 采用模式匹配方法，包括三个步骤： • 准备事件触发词表 • 候选事件抽取：寻找含有触发词的句子 • 事件元素识别：根据事件模版抽取相应的元素 (2)事件定义：事件包括：触发器、事件类型、事件参数、参数角色 2.2.4.3事理知识图谱 事理图谱（Event Evolutionary Graph）是一个描述事件之间顺承、因果关系的事理演化逻辑有向图。 2.2.4.4 总结• 实体识别 • 基于规则和词典的方法 • 基于统计学习 • 基于深度学习• 关系抽取 • 规则引擎: Ratel • 远程监督• 事件抽取 • 预定义事件模板 • 触发词 -&gt; 事件类型 -&gt; 参数识别 -&gt; 参数角色识别 2.2.5 结构化数据采用top-down的构建思路： 一种解决方案： • 数据模式整理 • 数据治理 • 数据映射 • 校验规则 2.2.6 工具​ 知识获取主要是从三个方面来进行说明的,依据是不同的处理对象: (1)D2R 结构化数据采用D2R技术-&gt;具体工具就是D2RQ,将关系数据库的数据转换成虚拟的RDF数据集,通过映射机制,之后上传到D2RQ 的服务器中,之后采用SPARQL语言进行查询.数据多的话,查询速度就会很慢 (2)包装器 半结构化数据采用包装器技术进行获取数据,具体形式就是采用信息抽取引擎,首先需要配置好包装器,之后设定好规则,然后开始执行 (3)文本信息抽取 文本信息抽取的任务:关系抽取,概念抽取,实体抽取,事件抽取. 非结构化文本(就是txt,html之类的)采用OPENIE或者CloseIE技术,区别如下: 本人研究的是特定邻域的知识图谱,因此选择使用,closeIE方法,其实现工具就是DEEPDIVE这个关系提取工具.DEEPDIVE运行流程如下: 定义抽取任务–&gt;准备好文件/文本–&gt;进行分词/实体识别(NER)–&gt;生成候选关系对–&gt;特征自动生成–&gt;数据自动标注(可以人工提前标注)+外部已经标注数据–&gt;最宗实现关系预测. 2.2.7 难点 从结构化数据库中获取知识：D2R 难点：复杂表数据的处理 从链接数据中获取知识：图映射 难点：数据对齐 从半结构化（网站）数据中获取知识：使用包装器 难点：方便的包装器定义方法，包装器自动生成、更新与维护 从文本中获取知识：信息抽取 难点：结果的准确率与覆盖率. 2.3 知识融合2.3.1 融合内容 数据模式层融合 数据层融合 概念合并 实体合并 概念上下位关系合并 实体属性融合 概念的属性定义合并 冲突检测与解决 行业知识图谱的融合通常采用自顶向下和自底向上结合的方式，因此基本都经 过人工的校验，保证了可靠性；因此，知识融合的关键任务在数据层的融合。 行业知识图谱的数据模式层通常是由专家人工构建或从可靠的结构化数据中映射得到的，通常在映射时会通过设置融合的规则来确保数据的统一。 2.3.1.1 数据层融合例子(1). 实体合并 在构建行业知识图谱时，实体优先从结构化的数据在获取；对于结构化的数据，通常有对实体进行唯一标识的主键，因此在进行知识抽取时即可设定实体合并的依据。 从非结构化数据中抽取的实体，同样使用设置合并条件的规则来完成实体的合并； 例如： 企业合并：可以通过企业名称直接合并 企业高管合并：人名相同 + 同一企业 （企业高管中同名的概念极低） (２)．实体属性与关系的合并 ​ 具有时态特性的属性（如）：使用新的数据覆盖老的数据 ​ 依据数据源的可靠性进行选取：结构化数据源中的质量通常较高 2.3.2 知识融合关键技术与难点 实现不同来源、不同形态数据的融合 海量数据的高效融合 新增知识的实时整合 多语言的融合 2.4 知识存储2.4.1 基本数据存储知识图谱数据存储需要完成的基本数据存储 • 三元组知识的存储 • 事件信息的存储 • 时态信息的存储 • 使用知识图谱组织的数据的存储 2.4.2 存储方式知识图谱是基于图的数据结构**，其存储方式主要有两种方式： RDF存储（W3C标准，一套完整和复杂的图谱表示框架） 图数据库(Graph Database)（业界实践经验）－－JanusGraph，neo4j 1. JanusGraph架构： JanusGraph可以使用多种非原生的存储系统，包括Hbase, BerkeleyDB, Cassandra, InMemory 2. RDF2.1 RDF定义• Resource Description Framework• Resource: 能被唯一标识的任何对象，如网页、地点• Description: 资源的属性、关系等• Framework: 为资源的描述提供了模型、语法等 2.2 基本元素• Internationalized Resource Identifier (IRI) • 一个符合特定语法的UNICODE字符串，全球唯一 • 如： http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML• 字面值 (literal)－－－＞可以理解为属性值 • 字符串 + 表示数据类型的IRI • 如：”1”^^xs:integer －－＞说明１是整型（分成两部分）• 空节点 (blank node) • 没有IRI的匿名节点（直接字符串） 2.3 三元组 • RDF对资源的陈述(statement)用三元组表示 • (Subject, Predicate, Object) • Subject－－－（IRI, blank node） • Predicate－－－（IRI／属性） • Object－－－（IRI, blank node, literal） 2.4.3 知识存储关键技术与难点 大规模三元组数据的存储 知识图谱组织的大数据的存储 事件与时态信息的存储 快速推理与图计算的支持 2.5 知识计算2.5.1 计算方式 本体推理：使用本体推理进行新知识发现或冲突检测。 基于规则的推理：使用规则引擎，编写相应的业务规则，通过推理辅助业务决策。 图查询：利用图模式匹配算法，对图谱中的数据进行快速访问。 图挖掘计算：基于图论的相关算法，实现对图谱的探索和挖掘 2.5.2 图挖掘计算 图挖掘计算：基于图论的相关算法，实现对图谱的探索和挖掘。 (1)集成实现基本图算法 图遍历：广度优先遍历、深度优先遍历 最短路径查询： Dijkstra（迪杰斯特拉算法）、Floyd（弗洛伊德算法） 路径探寻：给定两个或多个节点，发现它们之间的关联关系 权威节点分析：PageRank算法 族群发现：最大流算法 相似节点发现：基于节点属性、关系的相似度算法 (2)目的解决大规模的图数据分析问题，这里主要是指批量的计算，适合OLAP的应用场景 (3)特点• 随机访问、局部性差• 算法复杂度高 (4)实现方式 图计算引擎将图分布式的存储在内存中，提供简单的编程接口让用户实现复杂且高效的的并行图分析算法。 (5)典型框架 • Google Pregel • Facebook Giraph • Berkeley GraphX • CMU PowerGraph 2.5.3 本体推理 本体推理：使用本体推理进行新知识发现或冲突检测。 本体的五元组表示(1) 本体的五元组表示 O= {C,R,F,A,I} • C – 概念集合，通常以Taxonomy形式组织 • 球星，清华校友 • R – 关系，描述概念或者实例之间语义关系的集合 • subClassOf，birthplace • F – 函数,一组特殊的关系，关系中第n个元素的值由 其他n-1个元素的值确定 • Price-of-a-used-car 由 the car-model, manufacturing data 和 kilometers确定 • A – 公理（规则） • 如果A是B的子女，B是C的子女，则A是C的子孙 • I – 具体个体 • 如:Peter是概念学生的实例 Web Ontology Language = OWL• OWL进一步提供了更多的术语来描述属性和类别 (2)本体推理基本方法 基于表运算及改进的方法：FaCT++、Racer、 Pellet Hermit等 基于一阶查询重写的方法（Ontology based data access，基于本体的数据访问） 基于产生式规则的算法（如rete）：Jena 、Sesame、OWLIM等 基于Datalog转换的方法如KAON、RDFox等。 回答集程序 Answer set programming。 (3)本体知识推理工具——RDFox(3.1)RDFox 特点 支持共享内存并行OWL 2 RL推理 三元组数据可以导出为Turtle文件，规则文件可以导出为RDF数据记录文件；全部数据内容可以导出为二进制文件，可完全恢复数据存储状态。 支持Java、Python多语言APIs访问，并且 RDFox 还支持一种简单的脚本语言与系统的命令行交互。 RDFox完全基于内存，对硬件的要求较高 2.5.4基于规则的推理 基于规则的推理：使用规则引擎，编写相应的业务规则，通过推理辅助业务决策。 (1)基于规则推理工具——Drools1234567891011import com.hiekn.ruleengine.bean.RiskBean;rule highRiskRule1 //高风险企业规则一：企业注册资金每100万加1分，成立的时长每年加1分，每有失信记录减3分，有税务问题每次减少5分。when$risk : RiskBean(eval(true))then $risk.addOrMinusScore (702, 1);$risk.addOrMinusScore (703, 1);$risk.addOrMinusScore (711, -3);$risk.addOrMinusScore (712, -5);#711是类型情况代号 end 2.5.5补充(1) 基于符号的知识表示(1.1)方法 • 一阶谓词逻辑 • 一套灵活且紧凑的对象知识表示方式 • 完善的推理能力和表达能力支持 • Semantic Net • 一个通过语义关系连接的概念网络 • 表达能力受限，直观但是缺乏语义支持 • Frame • 直接表示领域知识模型,支持默认推理 • 表达能力受限，缺乏语义支持 • Script • 表达事件知识，非常受限， 对符合条件的场景非常适合 • 语义网知识表示语言体系 • 面向Web of Data,完善的一整套体系支持 • 有的时候杀鸡用牛刀 (1.2)优缺点 • 可解释、显式推理 • 难以大规模、语义鸿沟 (2) 基于分布式的知识推理(2.1)基于张量的模型 基于重构的张量分解方法：通过矩阵分解获得关系、实体的表示，其实就是类似奇异值分解 • 知识图谱中三元组的结构是（头部实体ℎ，关系ｒ ，尾部实体ｔ ），其中ｒ连接头尾实体。• 以Ｅ., Ｅ0, … , Ｅ2表示知识图谱中的实体，以Ｒ., Ｒ0, … , Ｒ3表示知识图谱中的关系，则可以使用一个三维矩阵 (张量) 表示知识图谱． (2.2)基于翻译的模型：TRANSE 用向量表示实体和关系。关系事实 = (head, relation, tail) 简写为（h, r, t），其中h, r, t分别是一维的向量表示 。 １．势能函数 • 对于positive的三元组(ℎ, ｒ,ｔ) ，要求ℎ + ｒ ≈ ｔ；• 对于negative的三元组则不满足上述等式。d （ℎ, ｒ,ｔ） =∥ℎ + ｒ − ｔ ∥＿２ ｄ（ 中国、首都、北京） &lt; ｄ(中国、首都、杭州) ２．问题 但是知识图谱中关系有“1-1”，“1-N”、“N-1”、“N-N”多种类型 ３解决方案:TransR • TransH、TransR、TransD • 利用投影方式，将不同语义的实体、关系嵌入到同一个语义空间中。 (2.3)基于神经网络的模型 神经网络模型的核心思想：使用神经网络为三元组定义势能函数，在训练目标中，要求正确的三元组具有较高的能量，错误的三元组具有较低的能量。通过惩罚错误的三元组完成学习过程，使得正确的三元组和错误三元组的能量有一个明显的分界线． 优缺点• 可学习、可计算，适合大规模、开放域• 不可解释、隐式推理 2.5.6 关键技术与难点图挖掘计算 本体推理：使用本体推理进行新知识发现或冲突检测。 基于规则的推理：使用规则引擎，编写相应的业务规则，通过推理辅助业务决策。 图查询：利用图模式匹配算法，对图谱中的数据进行快速访问。 图挖掘计算：基于图论的相关算法，实现对图谱的探索和挖掘 本体推理与规则推理 大数据量下的快速推理 对于增量知识和规则的快速加载 2.6 知识应用2.6.1 语义搜索 知识图谱提出的初衷即为解决搜索的准确率问题；传统基于关键词的检索完全不考虑语义信息，主要面临两个难题： 自然语言表达的多样性 自然语言的歧义 解决方案 实体链接 基于知识图谱的语义搜索 2.6.2 智能问答 难点 准确的语义解析 正确理解用户的真实意图 答案确定与排序 (1)基于信息检索的方法 首先利用中文分词、命名实体识别等自然语言处理工具找到问句中所涉及到的实体和关键词，然后去知识资源库中去进行检索。(之前那个基于电影的知识图谱就是.) 优点：实现简单，应用面广，在大部分场景下均可得到 缺点：要求答案中必须至少包含问句中的一个字或词，所以不如语义解析方法精确。 改进：基于知识图谱知识进行语义扩充，提高匹配率；基于知识图谱进行检索时的语义消歧。 (2)基于语义解析的方法 基于语义解析的方法非常符合人们的直觉，它将一个自然语言形式的问句，按照特定语言的语法规则，解析成语义表达式，在得到语义表达式之后，即可非常容易地将其转化为某种数据库的查询语言。 首先自然语言问句的词汇被映射到语义表达式中的词汇，然后按照特定的规则将汇组合起来，进而得到了最终的语义表达式。 改进方法：在特定的领域里边，基于知识图谱的实体、属性、概念等进行词法解析与映射(实体、属性、概念都需要进行映射)， 然后基于图结构进行语法规则匹配。（子图查询匹配）—根据意思来查询 (3)基于规则的专家系统方法 专家系统是一个具有大量的专门知识与经验的程序系统，它应用人工智能技术和计算机技术，根据某领域一个或多个专家提供的知识和经验，进行推理和判断，模拟人类专家的决策过程，以便解决那些需要人类专家处理的复杂问题。 优点：在限定的领域范围内准确度高。缺点：通用性欠缺，不能覆盖很多的应用场景。 (4)基于深度学习的方法 近几年卷积神经网络（CNN）和循环神经网络（RNN）在NLP领域任务中表现出来的语言表示能力，越来越多的研究人员尝试深度学习的方法完成问答领域的关键任务，包括 问题分类（question classification），语义匹配与答案选择（answer selection），答案自动生成（answer generation）；即对用户输入解析、答案查询与检索等环节进行优化。 优点：实现“端到端”的问答：把问题与答案均使用复杂的特征向量表示，使用深度学习来计算问题与答案的相似度。 不足：不支持复杂的查询；需要比较长的训练过程，不适用于现实应用场景中的知识更新后的实时查询。 (5)最佳实践方法 基于语义解析的方法可解释性强，并且能够方便地转换成知识图谱的查询，给 出明确的答案；因此对于用户输入，首先使用基于语义解析的方法进行回答 基于信息检索的方法应用面广，因此当语义解析方法无法给出结果时，则使用 信息检索的方法进行补充回答。 基于语义解析的自动问答 人工配置语义解析模板 知识图谱通用的子图匹配模板 2.6.3 可视化工具D3.js ,ECharts，Cytoscape.js 2.7 质量评估对知识库的质量评估任务 • 与实体对齐任务一起进行的 • 对知识的可信度进行量化，保留置信度较高的，舍弃置信度较低的，有效确保知识的质量 2.7.1可行方案• 利用强业务规则进行筛选和评估• 利用众包进行抽样打分• 在对ReVerb系统的信息抽取质量进行评估时，采用人工标注的方式对1 000个句子中的实体关系三元组进行了标注，并以此作为训练集，使用logistic回归模型计算抽取结果的置信度。• 基于LDIF框架，根据业务需求来定义质量评估函数，或者通过对多种评估方法的综合考评来确定知识的最终质量评分。• Google的Knowledge Vault项目则根据指定数据信息的抽取频率对信息的可信度进行评分，然后利用从可信知识库中得到的先验知识对可信度进行修正。 • 知识图谱 !== 知识","categories":[{"name":"知识图谱","slug":"知识图谱","permalink":"http://yoursite.com/categories/知识图谱/"},{"name":"综述","slug":"知识图谱/综述","permalink":"http://yoursite.com/categories/知识图谱/综述/"}],"tags":[{"name":"综述","slug":"综述","permalink":"http://yoursite.com/tags/综述/"}]},{"title":"语义网技术栈的理解","slug":"语义网技术栈的理解","date":"2018-06-23T06:02:49.000Z","updated":"2018-11-13T03:14:02.000Z","comments":true,"path":"2018/06/23/语义网技术栈的理解/","link":"","permalink":"http://yoursite.com/2018/06/23/语义网技术栈的理解/","excerpt":"本文的讲解主要是为了便于理解后面的知识图谱综述,语义网是以分层的形式来进行阐述的,第一层是资源的存储形式和地址形式,第二层则是资源内部结构是怎样的,第三层主要讲的是资源之间的关系是怎样的,第四层是对第三层的细化增加了资源的属性及其约束,为此形成了一个本体的概念,采用owl语言进行描述,便于区分不同本体之间的界限.最后几层就是验证,加密,应用交互层.","text":"本文的讲解主要是为了便于理解后面的知识图谱综述,语义网是以分层的形式来进行阐述的,第一层是资源的存储形式和地址形式,第二层则是资源内部结构是怎样的,第三层主要讲的是资源之间的关系是怎样的,第四层是对第三层的细化增加了资源的属性及其约束,为此形成了一个本体的概念,采用owl语言进行描述,便于区分不同本体之间的界限.最后几层就是验证,加密,应用交互层. 一.各层概念: 1.1 URI与UNICODE 1.2XML 1.3 RDF 1.3.1 RDF data model 1.3.2 RDF schema 1.3.2 RDF sytanx 1.4 RDF schema 与SPARQL 1.5 OWL ,RIF/SWRL 1. 6 Logic + Proof + Trust 1.7 interaction 1.8 参考 先上图 一.各层概念:阐述之前,先说下资源的概念: 1.可以是任意拥有URI(这是最重要的)的对象,人,物体,概念,文档,文件,音频.等等 2.URI:统一资源标识符 1.1 URI与UNICODE UNICODE:规定了资源的编码形式; URI规定了每个资源的唯一标识符. 1.2XML XML规定了资源的内容及其数据结构(XML tree,标签来描述),最重要的是没有语义的描述能力.(就是不知道资源之间的关系是怎样的). 同时XML的标签可以人为定义,这就导致不能很好地被计算机处理,为了规范化标签,增强可处理性,所以推出了xml schema来规定所拥有的标签,形式为xs:schema&lt;/xs:schema&gt; 但是由于标签的数量减少了,就很容易带来重复,这就引入了了命名空间的概念,xmlns (namespace–ns),在不同的命名空间下,同名的标签不受影响.形式为:xmlns:前缀=地址.具体代码如下: 1234567891011&lt;?xml version=\"1.0\"?&gt;&lt;xs:schema xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"&gt; &lt;xs:element name=\"note\"&gt; &lt;xs:complexType&gt; &lt;xs:sequence&gt; &lt;xs:element name=\"to\" type=\"xs:string\"/&gt; &lt;xs:element name=\"from\" type=\"xs:string\"/&gt; &lt;xs:element name=\"body\" type=\"xs:string\"/&gt; &lt;/xs:sequence&gt; &lt;/xs:complexType&gt;&lt;/xs:element&gt;&lt;/xs:schema&gt; 1.3 RDF RDF: 主要是用来描述资源之间的关系.采用SPO三元组 形式.123subject predict object#subject 与object是节点,节点可以是资源或描述资源的属性值(不是属性),predict是关系:只要能够描述两个节点的关系都可以称之为关系/属性#(资源，属性，属性值/资源) RDF资源描述框架,是一种图模型,数据模型,不是具体的数据类型.主要有三部分构成: 1.3.1 RDF data model 就是单纯描述资源的方式:SPO三元组形式,但是没有语义信息 1.3.2 RDF schema 在RDF data model上增加了语义信息,就是描述资源的关系,方式就是增加资源的属性及其资源所属的类的描述.具体方法如下: 概念 语法形式 描述 Class(类) C rdf:type rdfs:Class C(资源)是一个RDF类 Property(类) P rdf:type rdf:Property P(资源)是一个RDF属性 type(属性) I rdf:type C I(资源)是C(类)的实例 subClassOf(属性) C1 rdfs:subClassOf C2 C1 (类)是C2(类)的子类 subPropertyOf (属性) P1 rdfs:subPropertyOf P2 P1(属性)是P2(属性)的子属性 domain (属性) P rdfs:domain C P(属性)的定义域是C(类) range (属性) P rdfs:range C P(属性)的值域是C(类) 1.3.2 RDF sytanx 就是讲如何将RDF数据存入到计算机当中,也就是我们所说的RDF序列化方法,这就是RDF sytanx的作用.主要有如下集中方式: JSON-LD:“JSON for Linking Data”，用键值对的方式来存储RDF数据,基于json基础扩展的一种语法,以键值对的形式来存储RDF RDFA:将RDF数据嵌入到网页中 RDF/XML:用XML来存储RDF数据,但是由于XML格式冗长,阅读不方便不推荐 N-Triples，即用多个三元组来表示RDF数据集，是最直观的表示方法。在文件中，每一行表示一个三元组，方便机器解析和处理。开放领域知识图谱DBpedia通常是用这种格式来发布数据的。 Turtle, 应该是使用得最多的一种RDF序列化方式了。它比RDF/XML紧凑，且可读性比N-Triples好.turtle语法形式如下:1234567891011121314@prefix person: &lt;http://www.kg.com/person/&gt; .@prefix place: &lt;http://www.kg.com/place/&gt; .@prefix : &lt;http://www.kg.com/ontology/&gt; .person:1 :chineseName \"罗纳尔多·路易斯·纳萨里奥·德·利马\"^^string.person:1 :career \"足球运动员\"^^string.person:1 :fullName \"Ronaldo Luís Nazário de Lima\"^^string.person:1 :birthDate \"1976-09-18\"^^date.person:1 :height \"180\"^^int. person:1 :weight \"98\"^^int.person:1 :nationality \"巴西\"^^string. person:1 :hasBirthPlace place:10086.place:10086 :address \"里约热内卢\"^^string.place:10086 :coordinate \"-22.908333, -43.196389\"^^string. 1.4 RDF schema 与SPARQL RDF schema:主要还是为了增强资源关系的描述能力(上面讲了), SPARQL是一个查询图,RDF是一个存储图,是查询RDF数据的语言. 1.5 OWL ,RIF/SWRL OWL:在RDF schema的基础上,增加了对属性及其类的约束,也就是对资源的属性及其类增加了性质,这样能实现两个功能:分类,推理 实现对概念的分类,因为根据属性及类的约束,容易确定概念的边界 有利于RIF(规则交换)–实现推理,因为某些属性含有某些性质,例如:传递性. 1. 6 Logic + Proof + Trust分别对应逻辑层,验证层,信任层 逻辑层:提供公理及其规则,实现逻辑操作. 验证层:则是对推理结果进行验证 信任层:根据验证层的结果和一些数字签名,建立信任 1.7 interaction​ 就是语义网的交互层,通过实现各种应用来进行交互 1.8 参考https://blog.csdn.net/hohaizx/article/details/80043623 https://zhuanlan.zhihu.com/p/32122644 http://blog.sina.com.cn/s/blog_541caaee0100jjmc.html","categories":[{"name":"知识图谱","slug":"知识图谱","permalink":"http://yoursite.com/categories/知识图谱/"},{"name":"知识点","slug":"知识图谱/知识点","permalink":"http://yoursite.com/categories/知识图谱/知识点/"}],"tags":[{"name":"语义网","slug":"语义网","permalink":"http://yoursite.com/tags/语义网/"}]},{"title":"基于知识图谱的问答系统","slug":"基于知识图谱的问答系统","date":"2018-06-23T05:38:50.000Z","updated":"2018-11-13T02:59:44.000Z","comments":true,"path":"2018/06/23/基于知识图谱的问答系统/","link":"","permalink":"http://yoursite.com/2018/06/23/基于知识图谱的问答系统/","excerpt":"KBQA的工作流程介绍,先是介绍了该系统的必要工作,包括软件安装,语料库的下载,之后使用分类算法以概率的形式来进行模型的匹配.最后是重点描述了分词器的工作流程及其作用,特别是将测试句子转换为词向量的时候特别有用.","text":"KBQA的工作流程介绍,先是介绍了该系统的必要工作,包括软件安装,语料库的下载,之后使用分类算法以概率的形式来进行模型的匹配.最后是重点描述了分词器的工作流程及其作用,特别是将测试句子转换为词向量的时候特别有用. 一.准备工作: 二.设计贝叶斯分类算法 2.1创建进程 2.2创建向量 2.3设置标签 2.4 用List列表存放训练集 2.5训练数据集的格式转换 2.6 模型匹配 三.HANLP分词器的运行流程 3.1分割语句,创建词向量 3.2标注词向量内部元素 3.3提取语句 3.4结果查找 四.NEO4J的语法 一.准备工作: 1.下载好java8,并用mysql创建好数据库–重点在于存储数据 2.spark安装–用来进行提问问题的分类算法的编写 3.进行neo4j–用来存储mysql对应的数据库的关系–重点在于存储关系 4.之后在mysql当中将相应数据库当中的表格进行导出为csv文件,便于neo4j图形数据库的读取.将导出的csv文件放在import文件夹当中.＝ 5.安装hanlp中文分词器—这个主要是用来对 所输入的问题进行词语提取,形成一个词向量,之后使用后面会讲到的贝叶斯分类算法,得到该问题在不同问题模版上的匹配概率,取最大的概率,确定为该问题的匹配模版.第三节会讲到. 二.设计贝叶斯分类算法2.1创建进程 (为了后面并行处理分类问题的计算,提高训练速度与测试速度) 2.2创建向量 (denseVecto与sparseVector)–稠密向量与稀疏向量（Dense Vector and Sparse Vector）。在spark.ml.linalg里有两种vector——DenseVector 和 Sparse Vector，两者都继承于Vectors。区别:1234DenseVector： a value array--(直接把所有的元素都列出来了)`def: Vectors.dense(values: Array[Double])SparseVector : an index and a value array--(存储元素的个数、以及非零元素的编号index和值value)def: Vectors.sparse(size: Int, indices: Array[Int], values: Array[Double]) 2.3设置标签 使用labelpoint()方法为对应的向量来人为指定标签.例如:123456//训练集生成 ，规定数据结构为LabeledPoint == 构建方式:稠密向量模式 ，1.0:类别编号 == 男性 LabeledPoint train_one = new LabeledPoint(1.0,vMale); //(1.0, 0.0, 1.0, 0.0, 1.0, 0.0） //训练集生成 ，规定数据结构为LabeledPoint == 构建方式:稀疏向量模式 ，2.0:类别编号 == 女性 LabeledPoint train_two = new LabeledPoint(2.0,vFemale); //(1.0, 1.0, 1.0, 1.0, 0.0, 1.0） //我们也可以给同一个类别增加多个训练集 LabeledPoint train_three = new LabeledPoint(2.0,Vectors.dense(0,1,1,1,0,1)); 创建向量–一般人为创建的向量并且人为指定标签的叫做有标注的训练数据,测试数据一般采用用户输入语句(先创建一个 词向量,之后使用HANLP,分割语句,形成单词,之后在词汇表当中进行查找,根据存在与否对向量里面的元素进行设置0或者1,然后得到一个查找设置后的向量.之后采用贝叶斯方法进行问题模版匹配,来设置标签) 2.4 用List列表存放训练集便于后面的数据转换 12345//List存放训练集【三个训练样本数据】-- List&lt;LabeledPoint&gt; trains = new ArrayList&lt;&gt;(); trains.add(train_one); trains.add(train_two); trains.add(train_three); 2.5训练数据集的格式转换 将List训练数据集转换成JAVARDD.就是一个转换成RDD的接口,之后再转换成RDD格式的数据,最终是为了便于spark框架对训练数据集进行并行处理,提高并行速度.1234567891011/** * 利用Spark进行数据分析时，数据一般要转化为RDD * JavaRDD转Spark的RDD * SPARK的核心是RDD(弹性分布式数据集) * Spark是Scala写的,JavaRDD就是Spark为Java写的一套API*/ JavaRDD&lt;LabeledPoint&gt; trainingRDD = sc.parallelize(trains);/* 形成一个训练模型*/NaiveBayesModel nb_model = NaiveBayes.train(trainingRDD.rdd()); 2.6 模型匹配 之后利用这个训练模型进行测试数据的模型匹配.123456double [] dTest = &#123;1,0,1,0,0,0&#125;; Vector vTest = Vectors.dense(dTest);//测试对象为单个vector，或者是RDD化后的vector //朴素贝叶斯用法 int modelIndex =(int) nb_model.predict(vTest); System.out.println(\"标签分类编号：\"+modelIndex);// 分类结果 == 返回分类的标签值 三.HANLP分词器的运行流程具体过程如下: 3.1分割语句,创建词向量 使用HANLP分割出输入语句的各个单词,同时设置一个词向量(用来存储该语句在词汇表当中的情况) 3.2标注词向量内部元素 之后分割出来的词(同时也会标注,也可以人为设定)在词汇表当中进行查找,存在的话使得对应的索引为1,不存在则为0,以此类推,这样就可以得到一个词向量. 3.3提取语句 这个词向量在通过贝叶斯分类算法训练样本数据集得到的训练模型进行测试,就会得到这个词向量与各个问题模版(也就是属于哪一类问题,例如是评分问题,还是演员问题,其实就是以恶搞标签)的匹配概率,取最大的概率,作为该语句(词向量)的问题模版: 例如nm 评分问题—所以这就对问题进行了简化:例如: 英雄这部电影的评分是多少–&gt;nm 这部电影的评分是多少–&gt;通过贝叶斯分类算法得到–&gt;nm 评分–&gt;之后将英雄带回到nm中得到:英雄 评分 3.4结果查找 然后在neo4j数据库当中进行查找,查找语句: match(n) where n.name=’英雄’ from movie.genre得到最后的答案.—-得到结果 四.NEO4J的语法 结合表达式： match(n)-[r] -(b) 一般都是先进行查找,之后进行操作(通常)12345678​如果查询节点n,就return n​如果查询关系r,就return r​如果查询节点b,就return b​如果查询节点n和b之间的关系r,就return n，r，b​如果查询带条件,就where n.x = x，r.xx = xx，b.xxx = xxx​如果修改属性就where..... set ....​如果删除属性就where..... remove .....​如果删除节点或关系,就where..... delete n 或者 delete r 或者 delete b 或者 delete n , r , b","categories":[{"name":"知识图谱","slug":"知识图谱","permalink":"http://yoursite.com/categories/知识图谱/"},{"name":"知识点","slug":"知识图谱/知识点","permalink":"http://yoursite.com/categories/知识图谱/知识点/"}],"tags":[{"name":"知识图谱","slug":"知识图谱","permalink":"http://yoursite.com/tags/知识图谱/"},{"name":"综述","slug":"综述","permalink":"http://yoursite.com/tags/综述/"}]},{"title":"tomcat9的安装/配置问题","slug":"tomcat9的安装-配置问题","date":"2018-06-23T05:26:50.000Z","updated":"2018-11-13T02:53:00.000Z","comments":true,"path":"2018/06/23/tomcat9的安装-配置问题/","link":"","permalink":"http://yoursite.com/2018/06/23/tomcat9的安装-配置问题/","excerpt":"tomcat是一个开源免费的服务器,本文主要讲了tomcat的安装,测试,及其在myeclipse中的配置,在此过程之中所带来的问题,并提出了相应的解决方法,并且对tomcat的服务器配置文件的三个端口进行了解析,说明了不同端口的作用.","text":"tomcat是一个开源免费的服务器,本文主要讲了tomcat的安装,测试,及其在myeclipse中的配置,在此过程之中所带来的问题,并提出了相应的解决方法,并且对tomcat的服务器配置文件的三个端口进行了解析,说明了不同端口的作用. 一.流程 1.1下载安装包 1.2设置环境变量 1.3启动服务 1.4安装服务 1.5启动/暂停服务 1.6测试 二.问题 2.1 服务不能安装 2.2 8080端口被占用导致服务不能开启 三.释疑 3.1 server.xml中三个端口的作用 3.1.1监听浏览器请求的端口 3.1.2监听其他服务器转发的请求的端口 3.1.3监听的关闭端口 四.在myeclipse2017配置tomcat 一.流程1.1下载安装包先从官网下载安装包,进行解压缩地址:http://211.81.63.130/cache/3/02/www-us.apache.org/33a3f05043b61c8d9a1bfe7ecfb45b40/apache-tomcat-9.0.8-windows-x64.zip 1.2设置环境变量​ 最好设置在系统变量中,不要在用户变量中: 设置CATALINA_HOME–也就是安装包的目录 设置path: %CATALINA_HOME%\\bin;%CATALINA_HOME%\\lib; 设置CLASSPATH: %CATALINA_HOME%\\lib\\servlet-api.jar; 1.3启动服务两种方法 进入安装目录的bin文件下,打开startup.bat,启动看是否出现异常,没有继续下一步, 或者在cmd之中输入startup 和catalina run 1.4安装服务 安装服务之前先打开service.bat,之后在cmd中输入: install service tomcat9(其中tomcat9代表需要设定服务名)否则会出现报错 1.5启动/暂停服务 1net start/stop tomcat9 1.6测试 在浏览器之中输入:http://localhost:8080并进行回车,成功的话就会得到tomcat的启动页面 二.问题2.1 服务不能安装 install service tomcat9时出现install: cannot stat ‘service’: No such file or directory,如下图: 是因为没有启动tomcat程序(startup.bat),和打开tomcat服务(service.bat)导致的,先后点击即可 2.2 8080端口被占用导致服务不能开启 在安装包的conf文件夹下的server.xml中修改端口号. 用记事本打开,查找Connector port=”8080”中的8080改为其他任意端口号(例如8088)就可以. 之后重复之前的测试:http://localhost:8088 ,成功的话就可以看到tomcat的成功界面 三.释疑3.1 server.xml中三个端口的作用 如果使用多个tomcat 是需要配置这三个 3.1.1监听浏览器请求的端口123&lt;Connector port=\"8080\" protocol=\"HTTP/1.1\" #HTTP/1.1是浏览器的请求协议 connectionTimeout=\"20000\" redirectPort=\"8443\" /&gt; #如果是https请求,则将其转发到8443端口 3.1.2监听其他服务器转发的请求的端口12&lt;Connector port=\"8009\" protocol=\"AJP/1.3\" #AJP/1.3服务器转发协议 redirectPort=\"8443\" /&gt; 3.1.3监听的关闭端口1&lt;Server port=\"8005\" shutdown=\"SHUTDOWN\"&gt; 四.在myeclipse2017配置tomcat 1.打开设置–进入server选项. 2.点击添加之后:注意:JRE最好使用jdk_1.8. 完成之后,设置默认的服务器就是本地的tomcat就可以 3.将项目上传到服务器当中一开始是没有本地服务器的,所以需要进行添加勾选上always use this server when…这项 4.最后点击finish,之后需要先将项目进行发布,点击publish,否则项目会没有上传到服务器. 5.最后才是进行项目的运行","categories":[{"name":"知识图谱","slug":"知识图谱","permalink":"http://yoursite.com/categories/知识图谱/"},{"name":"软件安装","slug":"知识图谱/软件安装","permalink":"http://yoursite.com/categories/知识图谱/软件安装/"}],"tags":[{"name":"tomcat","slug":"tomcat","permalink":"http://yoursite.com/tags/tomcat/"}]},{"title":"spring-boot异常问题","slug":"spring-boot异常问题","date":"2018-06-23T05:22:45.000Z","updated":"2018-11-13T02:52:06.000Z","comments":true,"path":"2018/06/23/spring-boot异常问题/","link":"","permalink":"http://yoursite.com/2018/06/23/spring-boot异常问题/","excerpt":"本文所遇到的问题,是在配置关于电影的KBQA系统时,所出现的问题,目前有用户名及其密码无效,和缺少文件的exception,其实很多时候看bug解释,可以得到很多启发,避免少走很多弯路.","text":"本文所遇到的问题,是在配置关于电影的KBQA系统时,所出现的问题,目前有用户名及其密码无效,和缺少文件的exception,其实很多时候看bug解释,可以得到很多启发,避免少走很多弯路. 1.Invalid username or password. 2.系统找不到指定的文件 1.Invalid username or password.123456 Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is org.springframework.dao.DataAccessResourceFailureException: http://localhost:7474/db/data/transaction/commit; nested exception is org.neo4j.ogm.exception.ConnectionException: http://localhost:7474/db/data/transaction/commit] with root causeorg.apache.http.client.HttpResponseException: Invalid username or password. 从上面的异常描述可以得知.neo4j的数据库不能连接,因为用户名与密码无效. 所以需要在spring-boot的项目的属性中进行配置即application.properties中修改关于neo4j的配置信息(用户名与密码)修改成:123#Neo4j配置spring.data.neo4j.username=自己设置的用户名spring.data.neo4j.password=自己设定的密码 之后进行console框的历史数据清除,重新以 java application地 方式运行. 2.系统找不到指定的文件 如图所示: 说明在对应的位置缺少文件需要进行文件的补充即可.","categories":[{"name":"知识图谱","slug":"知识图谱","permalink":"http://yoursite.com/categories/知识图谱/"},{"name":"异常问题","slug":"知识图谱/异常问题","permalink":"http://yoursite.com/categories/知识图谱/异常问题/"}],"tags":[{"name":"spring-boot","slug":"spring-boot","permalink":"http://yoursite.com/tags/spring-boot/"}]},{"title":"Spark在Windows下的环境搭建","slug":"Spark在Windows下的环境搭建","date":"2018-06-23T05:03:54.000Z","updated":"2018-11-13T02:50:26.000Z","comments":true,"path":"2018/06/23/Spark在Windows下的环境搭建/","link":"","permalink":"http://yoursite.com/2018/06/23/Spark在Windows下的环境搭建/","excerpt":"由于Spark是用Scala来写的，所以Spark对Scala肯定是原生态支持的，因此这里以Scala为主来介绍Spark环境的搭建，主要包括四个步骤，分别是：JDK的安装，Scala的安装，Spark的安装，Hadoop的下载和配置。为了突出”From Scratch”的特点（都是标题没选好的缘故）.","text":"由于Spark是用Scala来写的，所以Spark对Scala肯定是原生态支持的，因此这里以Scala为主来介绍Spark环境的搭建，主要包括四个步骤，分别是：JDK的安装，Scala的安装，Spark的安装，Hadoop的下载和配置。为了突出”From Scratch”的特点（都是标题没选好的缘故）. 一．JDK的安装与环境变量的设置 1.1 JDK的安装 1.2 环境变量的设置 1.3 一些题外话 1.3.1 环境变量、系统变量和用户变量 1.3.2 PATH 1.3.3 CLASSPATH 1.3.4 JAVA_HOME 二. Scala的安装 三. Spark的安装 四．HADOOP下载 五. Python下的PySpark 六. 小结 七. Tips 一．JDK的安装与环境变量的设置1.1 JDK的安装 JDK（全称是JavaTM Platform Standard Edition Development Kit）的安装，下载地址是Java SE Downloads，一般进入页面后，会默认显示一个最新版的JDK，如下图所示，当前最新版本是JDK 8，更为详细具体的地址是Java SE Development Kit 8 Downloads： 上图中两个用红色标记的地方都是可以点击的，点击进去之后可以看到这个最新版本的一些更为详细的信息，如下图所示： 首先，这里主要包含有8u101和8u102这两个版本，Java给出的官方说明是： “Java SE 8u101 includes important security fixes. Oracle strongly recommends that all Java SE 8 users upgrade to this release. Java SE 8u102 is a patch-set update, including all of 8u101 plus additional features (described in the release notes). ” 也就是说Java推荐所有开发人员从以前的版本升级到JDK 8u101，而JDK 8u102则除了包括101的所有特性之外，还有一些其他的特性。对于版本的选择，自行选择就好了，其实对于普通开发人员来说，体现不了太大的区别，我这里就是使用的JDK 8u101版本。 选好8u101版本后，再选择你的对应开发平台，由于我的机器是64位的，所以我这里选择Windows64位的版本。记得在下载之前，必须要接受上方的许可协议，在上图中用红色圈出。 除了下载最新版本的JDK，也可以在Oracle Java Archive下载到历史版本的JDK，但官方建议只做测试用。 JDK在windows下的安装非常简单，按照正常的软件安装思路去双击下载得到的exe文件，然后设定你自己的安装目录（安装目录在设置环境变量的时候需要用到）即可。 1.2 环境变量的设置 接下来设置相应的环境变量，设置方法为：在桌面右击【计算机】－－【属性】－－【高级系统设置】，然后在系统属性里选择【高级】－－【环境变量】，然后在系统变量中找到“Path”变量，并选择“编辑”按钮后出来一个对话框，可以在里面添加上一步中所安装的JDK目录下的bin文件夹路径名，我这里的bin文件夹路径名是：F:\\Program Files\\Java\\jdk1.8.0_101\\bin，所以将这个添加到path路径名下，注意用英文的分号“;”进行分割。这样设置好后，便可以在任意目录下打开的cmd命令行窗口下运行 1java -version1 观察是否能够输出相关java的版本信息，如果能够输出，说明JDK安装这一步便全部结束了。 全部流程如下图所示（后续软件安装的系统变量设置都是这套流程）： 1.3 一些题外话 这里讲两句题外话，各位看官不关心的话可以跳过这里，不影响后续的安装步骤。 在软件安装的时候，相信各位没少遇到过环境变量和系统变量，所以这里就来扒一扒令人头疼的PATH, CLASSPATH和JAVA_HOME等参数的具体含义。 1.3.1 环境变量、系统变量和用户变量 环境变量包括系统变量和用户变量 系统变量的设置针对该操作系统下的所有用户起作用； 用户变量的设置只针对当前用户起作用 如果对这些概念还不是特别熟悉的，建议先看完下面几个点之后，再回过头来看这三句话。 1.3.2 PATH 也就是上一步设置的系统变量，告诉操作系统去哪里找到Java.exe的执行路径，当你在命令行窗口冷不丁的敲上如下命令的时候， 1java -version1 操作系统首先会一惊，What the hell does “java” mean? 不过吐槽归吐槽，活还是得干，于是悠悠的记起来了盖茨爸爸说过的三句话： 当你看不懂命令行窗口中的一个命令的时候，你首先去你所在的当前目录下找找，是否有这个命令的.exe程序？如果有，那就用它来启动执行； 如果没有，千万别放弃，记得要去Path系统变量下的那些目录下去找一找，如果找到了，启动并执行命令； 如果上面两个地方依然还没找到，那你就撒个娇，报个错好了。 所以我们将JDK安装目录下的bin文件夹添加到Path系统变量的目的也就在这里，告诉操作系统：如果在当前目录下找不到java.exe，就去Path系统变量里的那些路径下挨个找一找，直到找到java.exe为止。那为什么要设置bin文件夹，而不是JDK安装的根目录呢？原因就在于根目录下没有java.exe啊，只有bin文件夹下才有啊喂…… 如果只是在命令行窗口下运行一下java的命令，那其实也可以不设置系统变量，只是每次在命令行窗口运行java的命令时，都必须带上一长串路径名，来直接指定java.exe的位置，如下所示。 123C:\\Users\\weizierxu&gt;F:\\Program Files\\Java\\jdk1.8.0_101\\bin\\java.exe -version'F:\\Program' 不是内部或外部命令，也不是可运行的程序或批处理文件。123 注意：这里报错的原因并不是说直接指定java.exe的路径名这种方式有问题，而是命令行下无法解析带有空格的路径名，所以需要用到双引号，如下： 1234C:\\Users\\weizierxu&gt;\"F:\\Program Files\"\\Java\\jdk1.8.0_101\\bin\\java.exe -versionjava version \"1.8.0_101\"Java(TM) SE Runtime Environment (build 1.8.0_101-b13)Java HotSpot(TM) 64-Bit Server VM (build 25.101-b13, mixed mode)1234 1.3.3 CLASSPATH CLASSPATH是在Java执行一个已经编译好的class文件时，告诉Java去哪些目录下找到这个class文件，比如你的程序里用到某个Jar包（Jar包里的都是已经编译好的class文件），那么在执行的时候，Java需要找到这个Jar包才行，去哪找呢？从CLASSPATH指定的目录下，从左至右开始寻找（用分号区分开的那些路径名），直到找到你指定名字的class文件，如果找不到就会报错。这里做一个实验，就能明白具体是什么意思了。 首先，我在F:\\Program Files\\Java目录下，利用Windows自带的记事本写了一个类似于Hello World的程序，保存为testClassPath.java文件（注意后缀名得改成java），内容如下： 12345public class testClassPath&#123; public static void main(String[] args)&#123; System.out.println(\"Hello, this is a test on CLASSPATH!\"); &#125;&#125;12345 然后，我将cmd的当前目录切换到（通过cd命令）F:\\Program Files\\Java目录下，然后用javac命令来对这个.java文件进行编译，如下图所示： 从上图中可以看到，javac命令可以正常使用（没有任何输出的就表明正确编译了），这是因为执行该命令的javac.exe同样存在于JDK安装路径下的bin目录中，而这个目录我们已经添加到Path系统变量中去了，所以cmd能够认识这个命令。这个时候可以看到F:\\Program Files\\Java目录下多了一个testClassPath.class文件。不过运行这个class文件的时候，报错了。这个时候，CLASSPATH就派上用场了，和1.2节中对Path系统变量设置的方法一样，这里在CLASSPATH（如果系统变量的列表中没有CLASSPATH这个选项，那么点击新建，然后添加路径即可）中最后面添加上;.，英文的分号表示和前面已有的路径分割开，后面的小点.表示当前目录的意思。 这个时候记得要另起一个新的cmd窗口，然后利用cd命令切换到testClassPath.class所在目录，然后再去执行，便可以成功得到结果了。12F:\\Program Files\\Java&gt;java testClassPathHello, this is a test on CLASSPATH!12 因此，和Path变量不同的是，Java在执行某个class文件的时候，并不会有默认的先从当前目录找这个文件，而是只去CLASSPATH指定的目录下找这个class文件，如果CLASSPATH指定的目录下有这个class文件，则开始执行，如果没有则报错（这里有去当前目录下找这个class文件，是因为当前路径通过.的方式，已经添加到了CLASSPATH系统变量中）。 上面讲的指定CLASSPATH系统变量的方法，都是直接写死在系统变量中的，为了避免造成干扰（比如多个同名class文件存在于多个路径中，这些路径都有添加到CLASSPATH系统变量中，由于在找class文件的时候，是从左往右扫描CLASSPATH系统变量中的路径的，所以在利用java testClassPath方法执行的时候，运行的便是位置在CLASSPATH系统变量中最左边的那个路径中，对应的class文件，而这显然不是我们想要的结果），因此在诸如Eclipse等等这些IDE中，并不需要人为手动设定CLASSPATH系统变量，而是只设定当前程序的特定的CLASSPATH系统变量，这样便不会影响到其他程序的运行了。 1.3.4 JAVA_HOME JAVA_HOME并不是Java本身所需要的参数，而是其他的一些第三方工具需要这个参数来配置它们自己的参数，它存在的意义无非是告诉那些软件，我的JDK安装在这个目录下，你如果要用到我的Java程序的话，直接来我这个目录下找就好了，而JAVA_HOME就是JDK的安装路径名。比如我的JDK安装在F:\\Program Files\\Java\\jdk1.8.0_101目录下（注意该目录下的bin目录，就是在1.3.2节里Path系统变量中要添加的值），那么JAVA_HOME里要添加的值便是F:\\Program Files\\Java\\jdk1.8.0_101，以后碰到类似HOME的系统变量，都是软件的安装目录。 二. Scala的安装 首先从DOWNLOAD PREVIOUS VERSIONS下载到对应的版本，在这里需要注意的是，Spark的各个版本需要跟相应的Scala版本对应，比如我这里使用的Spark 1.6.2就只能使用Scala 2.10的各个版本，目前最新的Spark 2.0就只能使用Scala 2.11的各个版本，所以下载的时候，需要注意到这种Scala版本与Spark版本相互对应的关系。我这里现在用的是Scala 2.10.6，适配Spark从1.3.0到Spark 1.6.2之间的各个版本。在版本页面DOWNLOAD PREVIOUS VERSIONS选择一个适合自己需要的版本后，会进入到该版本的具体下载页面，如下图所示，记得下载二进制版本的Scala，点击图中箭头所指，下载即可： 下载得到Scala的msi文件后，可以双击执行安装。安装成功后，默认会将Scala的bin目录添加到PATH系统变量中去（如果没有，和JDK安装步骤中类似，将Scala安装目录下的bin目录路径，添加到系统变量PATH中），为了验证是否安装成功，开启一个新的cmd窗口，输入scala然后回车，如果能够正常进入到Scala的交互命令环境则表明安装成功。如下图所示： 如果不能显示版本信息，并且未能进入Scala的交互命令行，通常有两种可能性： Path系统变量中未能正确添加Scala安装目录下的bin文件夹路径名，按照JDK安装中介绍的方法添加即可。 Scala未能够正确安装，重复上面的步骤即可。 三. Spark的安装 Spark的安装非常简单，直接去Download Apache Spark。有两个步骤： 选择好对应Hadoop版本的Spark版本，如下图中所示； 然后点击下图中箭头所指的spark-1.6.2-bin-hadoop2.6.tgz，等待下载结束即可。 这里使用的是Pre-built的版本，意思就是已经编译了好了，下载来直接用就好，Spark也有源码可以下载，但是得自己去手动编译之后才能使用。下载完成后将文件进行解压（可能需要解压两次），最好解压到一个盘的根目录下，并重命名为Spark，简单不易出错。并且需要注意的是，在Spark的文件目录路径名中，不要出现空格，类似于“Program Files”这样的文件夹名是不被允许的。 解压后基本上就差不多可以到cmd命令行下运行了。但这个时候每次运行spark-shell（spark的命令行交互窗口）的时候，都需要先cd到Spark的安装目录下，比较麻烦，因此可以将Spark的bin目录添加到系统变量PATH中。例如我这里的Spark的bin目录路径为D:\\Spark\\bin，那么就把这个路径名添加到系统变量的PATH中即可，方法和JDK安装过程中的环境变量设置一致，设置完系统变量后，在任意目录下的cmd命令行中，直接执行spark-shell命令，即可开启Spark的交互式命令行模式。 四．HADOOP下载 系统变量设置后，就可以在任意当前目录下的cmd中运行spark-shell，但这个时候很有可能会碰到各种错误，这里主要是因为Spark是基于Hadoop的，所以这里也有必要配置一个Hadoop的运行环境。在Hadoop Releases里可以看到Hadoop的各个历史版本，这里由于下载的Spark是基于Hadoop 2.6的（在Spark安装的第一个步骤中，我们选择的是Pre-built for Hadoop 2.6），我这里选择2.6.4版本，选择好相应版本并点击后，进入详细的下载页面，如下图所示，选择图中红色标记进行下载，这里上面的src版本就是源码，需要对Hadoop进行更改或者想自己进行编译的可以下载对应src文件，我这里下载的就是已经编译好的版本，即图中的’hadoop-2.6.4.tar.gz’文件。 下载并解压到指定目录，然后到环境变量部分设置HADOOP_HOME为Hadoop的解压目录，我这里是F:\\Program Files\\hadoop，然后再设置该目录下的bin目录到系统变量的PATH下，我这里也就是F:\\Program Files\\hadoop\\bin，如果已经添加了HADOOP_HOME系统变量，也可以用%HADOOP_HOME%\\bin来指定bin文件夹路径名。这两个系统变量设置好后，开启一个新的cmd，然后直接输入spark-shell命令。 如果出现: 1Unable to load native-hadoop library for your platform... using builtin-java classes where applicabl 是由于hadoop一些本地库里编译时用到的C库与本机上的版本不同造成的，在本机环境下重新编译hadoop即可。不过这个警告对hadoop使用影响不大。 正常情况下是可以运行成功并进入到Spark的命令行环境下的，但是对于有些用户可能会遇到空指针的错误。这个时候，主要是因为Hadoop的bin目录下没有winutils.exe文件的原因造成的。这里的解决办法是： 去 https://github.com/steveloughran/winutils 选择你安装的Hadoop版本号，然后进入到bin目录下，找到winutils.exe文件，下载方法是点击winutils.exe文件，进入之后在页面的右上方部分有一个Download按钮，点击下载即可。 下载好winutils.exe后，将这个文件放入到Hadoop的bin目录下，我这里是F:\\Program Files\\hadoop\\bin。 在打开的cmd中输入F:\\Program Files\\hadoop\\bin\\winutils.exe chmod 777 /tmp/hive这个操作是用来修改权限的。注意前面的F:\\Program Files\\hadoop\\bin部分要对应的替换成实际你所安装的bin目录所在位置。 经过这几个步骤之后，然后再次开启一个新的cmd窗口，如果正常的话，应该就可以通过直接输入spark-shell来运行Spark了。正常的运行界面应该如下图所示： 从图中可以看到，在直接输入spark-shell命令后，Spark开始启动，并且输出了一些日志信息，大多数都可以忽略，需要注意的是两句话： 12Spark context available as sc.SQL context available as sqlContext.12 Spark context和SQL context分别是什么，后续再讲，现在只需要记住，只有看到这两个语句了，才说明Spark真正的成功启动了。 五. Python下的PySpark 针对Python下的Spark，和Scala下的spark-shell类似，也有一个PySpark，它同样也是一个交互式的命令行工具，可以对Spark进行一些简单的调试和测试，和spark-shell的作用类似。对于需要安装Python的来说，这里建议使用Python(x,y)，它的优点就是集合了大多数的工具包，不需要自己再单独去下载而可以直接import来使用，并且还省去了繁琐的环境变量配置，下载地址是Python(x,y) - Downloads，下载完成后，双击运行安装即可。因为本教程主要以Scala为主，关于Python的不做过多讲解。 并且，pyspark的执行文件和spark-shell所在路径一致，按照上述方式解压好spark后，可以直接在cmd的命令行窗口下执行pyspark的命令，启动python的调试环境。 但是如果需要在python中或者在类似于IDEA IntelliJ或者PyCharm等IDE中使用PySpark的话，需要在系统变量中新建一个PYTHONPATH的系统变量，然后 1PATHONPATH=%SPARK_HOME%\\python;%SPARK_HOME%\\python\\lib\\py4j-0.10.4-src.zip1 设置好后，建议使用PyCharm作为IDE（因为IDEA IntelliJ的设置繁琐很多，没耐心了设置一堆参数啦，哈哈哈） 六. 小结 至此，基本的Spark本地调试环境便拥有了，对于初步的Spark学习也是足够的。但是这种模式在实际的Spark开发的时候，依然是不够用的，需要借助于一个比较好用的IDE来辅助开发过程。下一讲就主要讲解ItelliJ IDEA以及Maven的配置过程。 七. Tips 血的教训：永远不要在软件的安装路径中留有任何的空格","categories":[{"name":"知识图谱","slug":"知识图谱","permalink":"http://yoursite.com/categories/知识图谱/"},{"name":"软件安装","slug":"知识图谱/软件安装","permalink":"http://yoursite.com/categories/知识图谱/软件安装/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark启动参数的理解","slug":"spark启动参数的理解","date":"2018-06-23T04:54:06.000Z","updated":"2018-11-13T02:48:42.000Z","comments":true,"path":"2018/06/23/spark启动参数的理解/","link":"","permalink":"http://yoursite.com/2018/06/23/spark启动参数的理解/","excerpt":"本文主要讲述spark启动所需要的参数,从而引伸出spark的部署模式,最终目的还是构建一个spark的线程来启动spark,先配置好然后将其作为一个参数,放到SparkContext之中,采用new方法进行构建spark程序/线程.","text":"本文主要讲述spark启动所需要的参数,从而引伸出spark的部署模式,最终目的还是构建一个spark的线程来启动spark,先配置好然后将其作为一个参数,放到SparkContext之中,采用new方法进行构建spark程序/线程. 一.spark初始化 二.spark部署方式与调度模式 一.spark初始化12val conf = new SparkConf().setMaster(\"master\").setAppName(\"appName\")val sc = new SparkContext(conf) 任何的spark程序都是从SparkContext开始的,就项tensorflow当中的Session一样,都需要先实例化,一般使用SparkContext()方法就可以: 所以在实例化对象之前最好先配置该对象的一些参数,我们可以把它打包在参数conf中, 其中conf对象的产生是使用了SparkConf()之后产生对象之后调用该对象的setMaster()方法来进行设置该spark程序的部署/运行模式,同时也为spark应用程序设置一个名称,使用setAppName()便于区分众多的spark程序- setMaster():设置程序运行模式 - setAppName():为该spark程序设置一个名称 参考:https://www.cnblogs.com/Forever-Road/p/7351245.html 二.spark部署方式与调度模式spark参数主要是从部署方式与调度模式来进行讨论的: 部署方式:有local与cluster(本地和集群)–参数代表为master,区别就是计算机运行的台数,本地上一般只有一台计算机一般主要是用来做测试,训练.集群则是有很多台. 集群的方式引来了另一个问题:调度问题,每台计算机的资源如何调度: 有三种模式: standalone:自己计算,自己调度.— messo:交由messo调度器进行调度—- yarn:就是为了方便与MapReduce在同一集群上的融合,使用yarn调度模式,具体有两种实现方式: yarn cluster:调度和计算都在集群上, yarn client:调度在本机,计算在集群(其实就是资源分配掌握在本地上,但是分配出来的计算资源可以放在集群上) 列表内容 参考下图: 参考链接: https://www.jianshu.com/p/aaac505908dd","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"知识点","slug":"java/知识点","permalink":"http://yoursite.com/categories/java/知识点/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"node4j下载安装与配置及其启动问题","slug":"node4j下载安装与配置及其启动问题","date":"2018-06-23T02:59:55.000Z","updated":"2018-11-13T02:42:54.000Z","comments":true,"path":"2018/06/23/node4j下载安装与配置及其启动问题/","link":"","permalink":"http://yoursite.com/2018/06/23/node4j下载安装与配置及其启动问题/","excerpt":"本文讲的是neo4j的安装，配置，测试，运行，及其在启动的过程中带来的一些问题,安装过程的主要问题就在于下载包的下载,由于neo4j的安装包也实在不太好找,启动问题包括：服务安装不了有时也会启动不了．因此专门撰写这个博客,省去之后的麻烦.","text":"本文讲的是neo4j的安装，配置，测试，运行，及其在启动的过程中带来的一些问题,安装过程的主要问题就在于下载包的下载,由于neo4j的安装包也实在不太好找,启动问题包括：服务安装不了有时也会启动不了．因此专门撰写这个博客,省去之后的麻烦. 一.下载，安装 1.1下载安装包 1.2直接双击安装 二、配置环境变量 三.测试 四、注册NEO4J服务 五、开启NEO4J服务 贰.neo4j 3.4.0 install-service，start 报错的问题 一.下载，安装 由于NEO4J最有效率的功能就是能够快速查询出任意2个节点的最短链接途径，此功能也填补了一些当前主流数据库的空白。NEO4J高效的遍历算法,查询速度快 1.1下载安装包官方下载链接 ：最新版地址:https://neo4j.com/download/other-releases/windows64 ziphttps://neo4j.com/artifact.php?name=neo4j-community-3.1.0-windows.zipwindows64 exehttps://neo4j.com/artifact.php?name=neo4j-community_windows-x64_3_1_0.exelinuxhttps://neo4j.com/artifact.php?name=neo4j-community-3.1.0-unix.tar.gzmachttps://neo4j.com/artifact.php?name=neo4j-community_macos_3_1_0.dmg windows下安装： 想下载哪个版本 修改这个超链接 修改版本号https://neo4j.com/artifact.php?name=neo4j-community_windows-x64_3_1_0.exehttps://neo4j.com/artifact.php?name=neo4j-community_windows-x64_3_3_1.exe 1.2直接双击安装 双击后，这里会让你选择数据库存放位置 然后访问 http://localhost:7474/browser/ 安装好需要登录有默认密码 卧槽 默认密码是多少？猜一下 用户名：neo4j 密码 ：neo4j 后来发现你在官网下载的时候 点击下载正在下载的页面有这么一句话123Your download should begin automatically. Click here if it doesn’t.Default login is username ‘neo4j’ and password ‘neo4j’ (full installation instructions below) 使用默认密码登陆后，需要修改密码 然后就可以在下面界面中愉快的执行命令了 来试一下 1231.插入节点。插入一个Person类别的节点，且这个节点有一个属性name，属性值为AndresCREATE (n:Person &#123;name : '梁川川'&#125;);123 执行完后 出现如下效果 二、配置环境变量 本文参考了http://blog.csdn.net/appleyk/article/details/79091898来配置的。使用的是jdk 1.8 131的版本。 首先，在命令行中输入java -version，查看jvm版本。 然后，配置环境变量，在计算机-属性-高级系统设置-环境变量，添加NEO4J_HOME=安装包所在的路径，可以根据客户端的应用程序找到，如图所示的位置： 接着，复制路径即可， 最后，在配置path路径，path中添加最好直接添加neo4j的bin文件夹的最终路径最好 三.测试 以管理员的身份运行命令行，输入 1neo4j.bat console 命令之后； 如图所示，已经开启了NEO4J数据库, 配置成功后，可以在浏览器中使用http://localhost:7474网址查看数据库，但是前提是得把桌面的应用程序关掉。 记住数据库的用户名和密码，一般默认的是：用户：neo4j, 密码：neo4j. 登陆后重新设置密码： 在浏览器上输入用户：user和密码：admin(这里是我的账户) 四、注册NEO4J服务输入命令： 1neo4j install-service —会出现一一些问题参考 [neo4j 3.4.0 install-service，start 报错的问题]: 本地服务查看： 五、开启NEO4J服务输入命令; 1neo4j start 停止、重启、查询Neo4j服务—会出现 一些问题参考:[贰.neo4j 3.4.0 install-service，start 报错的问题] 接着，进入数据库操作。 可以使用cypher语言对neo4j数据库进行操作了，此处省略。。。。 这部分首先，在neo4j的数据库中已经进行了，测试存储，使用cypher语言，可以把RDF数据形式的数据存储为图形式的数据了，也就是在谓语的边上添加了关系的属性。实验的例子就是本论文中想到的一个小例子。 如果觉得这样存储数据比较慢的话，可以试试直接将大规模数据导入进neo4j数据库中。 贰.neo4j 3.4.0 install-service，start 报错的问题 1，从官网https://neo4j.com/ 及http://www.we-yun.com/index.php/blog/releases-56.html 下载的neo4j 3.4.0 Community 安装及启动服务会报错，其实任何一个命令都报一个相同的错误，导致neo4j install-service，neo4j start 都报错： 2，原因%NEO4J_HOME%\\bin\\Neo4j-Management\\Get-Neo4jPrunsrv.ps1 文件有问题： switch ($PsCmdlet.ParameterSetName) { &quot;ServerInstallInvoke&quot; { $PrunArgs += @(&quot;`&quot;//IS//$($Name)`&quot;&quot;) } &quot;ServerUpdateInvoke&quot; { $PrunArgs += @(&quot;`&quot;//US//$($Name)`&quot;&quot;) } {$_ -in @(&quot;ServerInstallInvoke&quot;, &quot;ServerUpdateInvoke&quot;)} { 3，解决方法：注释如下5行-或者直接删掉也可以 #} #&quot;ServerUpdateInvoke&quot; { #$PrunArgs += @(&quot;`&quot;//US//$($Name)`&quot;&quot;) #} #{$_ -in @(&quot;ServerInstallInvoke&quot;, &quot;ServerUpdateInvoke&quot;)} { 4，之后就正常了： 虽然服务已经安装了但是还是会出现 1Neo4j is not running. Current status is Stopped 那么解决方法就是需要将neo4j.bat console产生的neo4j图数据库程序进行关闭. 如果在已经打开了服务在进行neo4j.bat console,那么就会产生如下错误: 其实主要看最后一行:就可以,就需要将服务先进行关闭,之后打开neoj图数据库 12018-06-06 10:42:28.275+0000 INFO Neo4j Server shutdown initiated by request","categories":[{"name":"知识图谱","slug":"知识图谱","permalink":"http://yoursite.com/categories/知识图谱/"},{"name":"软件安装","slug":"知识图谱/软件安装","permalink":"http://yoursite.com/categories/知识图谱/软件安装/"}],"tags":[{"name":"neo4j","slug":"neo4j","permalink":"http://yoursite.com/tags/neo4j/"},{"name":"异常问题","slug":"异常问题","permalink":"http://yoursite.com/tags/异常问题/"}]},{"title":"Navicat快捷键","slug":"Navicat快捷键","date":"2018-06-23T02:49:35.000Z","updated":"2018-11-13T02:42:20.000Z","comments":true,"path":"2018/06/23/Navicat快捷键/","link":"","permalink":"http://yoursite.com/2018/06/23/Navicat快捷键/","excerpt":"本文主要讲述了一些navicat在操作数据库当中的一些快捷键,借此来提高效率","text":"本文主要讲述了一些navicat在操作数据库当中的一些快捷键,借此来提高效率 一.查询: 二.查找 三.注释 四.运行 五.复制 六.删除 七.命令行窗口 八.窗口 8.1 关闭 8.2切换 8.3放大/缩小 8.4重置窗口大小 九.说明文档 十.查询历史查看 一.查询:ctrl+Q 当选定某表的时候,可以直接进行查询 ctrl+N只是单纯的建立一个查询窗口 二.查找ctrl+F:开启查找 f3:开启查找之后下一个 三.注释ctrl+/ :一般是行注释 四.运行ctrl+R:运行当前窗口sql语句 ctrl+shift+R:运行选定的sql语句 五.复制ctrl+D:(duplicated:复制的含义)–复制粘贴当前行 六.删除ctrl+L :删除当前行 七.命令行窗口F6:就是进入mysql 八.窗口8.1 关闭ctrl+W 8.2切换ctrl+TAB 8.3放大/缩小ctrl+ +/- 或者同时滚轮 8.4重置窗口大小ctrl+0 九.说明文档F1(离线) ctrl+f1在线 十.查询历史查看ctrl+H","categories":[{"name":"快捷方式","slug":"快捷方式","permalink":"http://yoursite.com/categories/快捷方式/"},{"name":"Navicat","slug":"快捷方式/Navicat","permalink":"http://yoursite.com/categories/快捷方式/Navicat/"}],"tags":[{"name":"Navicat","slug":"Navicat","permalink":"http://yoursite.com/tags/Navicat/"}]},{"title":"navicat for mysql的异常问题","slug":"navicat-for-mysql的异常问题","date":"2018-06-23T01:07:31.000Z","updated":"2018-11-13T02:37:54.000Z","comments":true,"path":"2018/06/23/navicat-for-mysql的异常问题/","link":"","permalink":"http://yoursite.com/2018/06/23/navicat-for-mysql的异常问题/","excerpt":"本文主要讲述了navicat在执行操作数据库的时候,所遇到的一些问题,导出问题,合并问题,语法问题.","text":"本文主要讲述了navicat在执行操作数据库的时候,所遇到的一些问题,导出问题,合并问题,语法问题. 1.导出的文件已经存在,就会导致后面SQL语句不能执行. 2.出现sql语句语法问题 3.select的属性形式问题 4.在合并两张表合并失败 5.navicat导入sql文件成功但是没有表 1.导出的文件已经存在,就会导致后面SQL语句不能执行.如图所示:解决办法: 需要导出的数据不是很多的,可以单独运行,选定之后,ctrl+shift+R就可以解决 可以删除之前已经导出的文件,之后整体运行 ctrl+R即可 2.出现sql语句语法问题 解决方法: 去掉when person_name is then person_english_name中的is,因为is常跟null,因此去掉之后就表示person_name存在 3.select的属性形式问题 如果是一个新表(person_),那么在select的时候同时完成了新建属性与选择属性的功能 如果是旧表(person),那么属性名应该是变量名,而是用字符串的形式,否则就会将该字符串直接赋值给对应新表的属性名.错误情况如下 具体情况代码如下: 1234567#导出演员person的信息 == 如果有中文名要中文名，如果没有取英文名 SELECT * INTO OUTFILE 'C:/ProgramData/MySQL/MySQL Server 5.7/Uploads/person.csv' FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"' FROM (select 'pid','birth','death','name','biography','birthplace' union select person_id,person_birth_day,person_death_day,case when person_name is then person_english_name else person_name end as name,person_biography,person_birth_place from person) person_; 4.在合并两张表合并失败 原因有: 可能是所选择的表名有错,就是from table_name 或者旧表的属性名写错了 5.navicat导入sql文件成功但是没有表 有如下原因: sql语句中本身就是有创建数据库的,可能需要创建的与需要导入的数据库的名称不一致,所以只需要全部替换sql语句的数据库名称就可以 之后还是不能导入的话,直接将sql文件中内容复制到新建查询中,通过直接运行创建一个数据库. 12CREATE DATABASE IF NOT EXISTS `movie` /*!40100 DEFAULT CHARACTER SET utf8mb4 */;---说明了数据库的格式USE `movie`; 上面就是显示了很多问题:数据库字符格式:utf-8-bm64 ;还有会自动创建数据库,别忘了:USE movie;这样才会真正导入到创建的数据库之中","categories":[{"name":"知识图谱","slug":"知识图谱","permalink":"http://yoursite.com/categories/知识图谱/"},{"name":"异常问题","slug":"知识图谱/异常问题","permalink":"http://yoursite.com/categories/知识图谱/异常问题/"}],"tags":[{"name":"navicat异常问题","slug":"navicat异常问题","permalink":"http://yoursite.com/tags/navicat异常问题/"}]},{"title":"mysql的启动与文件导出的操作","slug":"mysql的启动与文件导出的操作","date":"2018-06-22T14:42:07.000Z","updated":"2018-11-13T02:37:16.000Z","comments":true,"path":"2018/06/22/mysql的启动与文件导出的操作/","link":"","permalink":"http://yoursite.com/2018/06/22/mysql的启动与文件导出的操作/","excerpt":"本文主要将了mysql在cmd下的启动,之后阐述了利用sql语句进行文件导出的操作.,重点讲述了SELECT 属性名 ,case when then else end as 属性名 FROM 表名和case函数.","text":"本文主要将了mysql在cmd下的启动,之后阐述了利用sql语句进行文件导出的操作.,重点讲述了SELECT 属性名 ,case when then else end as 属性名 FROM 表名和case函数. 一.开启 1.1设置环境变量 1.2开启服务 1.3进入mysql 二.操作 2.1查看MySql的导入与导出的目录 2.2mysql数据导出为csv文件 2.2.1导出顺序 2.2.2 按条件查询 2.2.3 union与union all 2.2.4case函数 2.2.4.1 case可以嵌套 2.2.4.2 case函数注意事项 2.2.5 select as的用法 一.开启1.1设置环境变量 将mysql安装位置对应下的bin文件夹的位置复制到path之中 1.2开启服务1net start mysql57(服务名) 1.3进入mysql1mysql -hlocalhost -uroot -p 回车 其中-h代表的是服务器,-u代表用户,-p代表密码 二.操作2.1查看MySql的导入与导出的目录1show variables like '%secure%'; 2.2mysql数据导出为csv文件导出到哪里;怎么导,从哪里导出 2.2.1导出顺序1SELECT INTO OUTFILE FROM 123456789101112131415161718192021#1---需要导出什么(其实已经在括号设置好了,因为全部数据导出是在对表已经操作的情况下实行的)SELECT * #2---从哪里导出(实际上先对旧表movie进行操作,采用交操作,实现属性名的重新设置(也可以不用更新)新旧属性名要一一对应,之后命名为新表movie_,实际上就是复制了一份,对原先的表没有修改)FROM (select &apos;mid&apos;,&apos;title&apos;,&apos;introduction&apos;,&apos;rating&apos;,&apos;releasedate&apos; union select*from movie) movie_ #movie_就是代表要输出的表(只不过以属性的形式输出)#3---导出到哪里,导出到movie.csv文件中,旧表不能直接导入到movie.csv文件,因为会直接破坏旧表,不利于mysql的数据库的维护.#注意输出的地址斜杠是/,不是windows下的\\,千万注意INTO OUTFILE &apos;C:/ProgramData/MySQL/MySQL Server 5.7/Uploads/movie.csv&apos;#4---设置字符编码--防止乱码一般就是GBK或者utf8character set utf-8 #5----导入规则,下面就是新表中字段导入csv表格当中的规则.# FIELDS之间分隔的标志就是&quot;,&quot;,遇到这个前面的就为一个字段.例如#&quot;我爱你&quot;,&quot;20&quot;,&quot;相貌平常，经常耍流氓！哈哈&quot; mysql数据格式#| id | name | age | description |#+----+----------变为----------------------------+#| 1 | 我爱你 | 20 | 相貌平常，经常耍流氓！哈哈 | csv数据格式FIELDS TERMINATED BY &apos;,&apos; #代有双引号的词组,要先将&quot;&quot;去除,之后放入csv文件中如上面的:#&quot;我爱你&quot;---&gt;| 我爱你 |OPTIONALLY ENCLOSED BY &apos;&quot;&apos; #意思就是双引号可选择关闭 #例如电影当中的简介,一般比较长,遇到换行\\r,那就说明简介完成了,就可以把它设置为一个属性.LINES TERMINATED BY &apos;\\r&apos;; as的用法，起别名，给已知的列起别名 引出一个代码: 12345678910111213SELECT * INTO OUTFILE &apos;C:/ProgramData/MySQL/MySQL Server 5.7/Uploads/person.csv&apos; FIELDS TERMINATED BY &apos;,&apos; OPTIONALLY ENCLOSED BY &apos;&quot;&apos; FROM ( select &apos;pid&apos;,&apos;birth&apos;,&apos;death&apos;,&apos;name&apos;,&apos;biography&apos;,&apos;birthplace&apos; union select person_id,person_birth_day,person_death_day, case when person_name is then person_english_name else person_name end as name,person_biography,person_birth_place from person) person_; 涉及到两个知识点 2.2.2 按条件查询 SELECT 属性名 ,case when then else end as 属性名 FROM 表名 上面当中: 123456select person_id,person_birth_day,person_death_day, #特别注意这个逗号, case when person_name is then person_english_name else person_name end as name,person_biography,person_birth_place from person 旧的属性名(旧表person的) 1234person_id,person_birth_day,person_death_day,#直接选出的name ,person_biography,person_birth_place #其中name是经过case when then else end这个条件选出的属性(用as生成的),#其他的属性person_biography,person_birth_place则是直接选出,这点尤其要注意#两部分的属性都是在旧表person中选择的 新表(person_)的属性 1select &apos;pid&apos;,&apos;birth&apos;,&apos;death&apos;,&apos;name&apos;,&apos;biography&apos;,&apos;birthplace&apos; 并且他们一一对应,–&gt;所以我们一开始的select不是重点所以只需要select * 就可以,主要是为了规范 2.2.3 union与union all 两者都是集合操作,这就要求操作的两个表的结构相同,但union是小并操作(消除重复),union all是大并操作(不消除重复),但两者都是对这两张表的结合(为了便于说明,对如下代码进行了删减进), 1234567SELECT * INTO OUTFILE 'C:/ProgramData/MySQL/MySQL Server 5.7/Uploads/person.csv' FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"' FROM (select 'pid','birth','death' union select person_id,person_birth_day,person_death_day from person) person_; 其数据导出的主要过程如下: 先从旧表person当中选取各种属性,之后在新表(person_)之中建立对应的属性, 然后进行一个小并操作(个人自定义,便于描述),因为新建的表是属性值都为空,加上与旧表的小并操作,,其实就相当于把旧表复制给了新表, 然后新表根据导入规则,将表中的数据进行导入到csv文件当中,这样就实现了mysql数据的导出到csv文件当中的过程 2.2.4case函数2.2.4.1 case可以嵌套 主要是解决不同情况的的不同输出.减少代码量. 选修多门课程的学生，要选择一门课程作为主修，主修flag里面写入 Y。只选择一门课程的学生，主修flag为N(实际上要是写入Y的话，就没有下面的麻烦事了，为了举例子，还请多多包含)。 只选修一门课程的人，返回那门课程的ID 选修多门课程的人，返回所选的主课程ID 12345678910SELECT std_id, CASE WHEN COUNT(*) = 1 --只选择一门课程的学生的情况 THEN MAX(class_id) ELSE MAX(CASE WHEN main_class_flg = &apos;Y&apos; THEN class_id ELSE NULL END ) END AS main_class FROM Studentclass GROUP BY std_id; 运行结果1234567STD_ID MAIN_class ------ ----------100 1 200 3 300 4 400 5 500 6 2.2.4.2 case函数注意事项先看一段代码:(两个易错点)1234CASE col_1 WHEN 1 THEN &apos;Right&apos; WHEN NULL THEN &apos;Wrong&apos; END 其中’Wrong’是错误的,所以永远不会出现Wrong的情况,正确格式为 1WHEN NULL THEN 'Unknown' 也可以写成 12#WHEN col_1=NULL THEN &apos;Unknown&apos; 是错误的WHEN col_1 IS NULL THEN &apos;Unknown&apos; #才是正确的 没有”=”千万要注意,一般col_1 IS就代表不为空 2.2.5 select as的用法 as的用法，起别名，给已知的列起别名 1select id,column1 * column2 as column from table1 1select count(*) as total from table1 1另外注意：select Product as Description, Price, Image from test as的作用范围为“as到它后面的第一个逗号为止”。其后面的“price，image”是select 选取的表中的其他字段。完整的语句： 1select Product as Description, Price, Image from test 查询结果为： 12| product | Price | image || aa | 1.00 | :) | 完整编码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465use movie;#导出电影类型信息select *from(select 'gid','gname' #带''是为了创建新表的属性.用字符串代表unionselect * from genre)genre_#输出地址包括具体的存储文件,不要写错了是csv,不是cvs,即使写错了,改下后缀名同时可以.into outfile 'C:/ProgramData/MySQL/MySQL Server 5.7/Uploads/genre.csv'character set utf8 #设置好编码格式,否则默认是gbkfields terminated by ','optionally enclosed by '\"';#去掉双引号在存储到csv中#导出电影信息select *from(select 'mid','title','introduction','rating','releasedate'unionselect * from movie)movie_into outfile 'C:/ProgramData/MySQL/MySQL Server 5.7/Uploads/movie.csv'character set utf8fields terminated by ','optionally enclosed by '\"'lines terminated by '\\r';#导出电影所属类型信息select *from(select 'mid','gid'unionselect * from movie_to_genre)movie_to_genre_into outfile 'C:/ProgramData/MySQL/MySQL Server 5.7/Uploads/movie_to_genre.csv'character set utf8fields terminated by ','optionally enclosed by '\"';#导出演员信息select *from(select 'pid','birth','death','name','biography','birthplace' unionselect person_id,person_birth_day,person_death_day,casewhen person_name then person_english_name #记住这里千万没有单独 is,只有is nullelse person_nameendas name, #case函数作为选取person_name的条件,其函数结果用name属性表示,实现形式主要是采用as方法person_biography,person_birth_place from person)person_ #新表person_into outfile 'C:/ProgramData/MySQL/MySQL Server 5.7/Uploads/person.csv'character set utf8fields terminated by ','optionally enclosed by '\"'lines terminated by '\\r'; ##导出演员与所演电影关系表select *from(select 'pid','mid'unionselect * from person_to_movie)person_to_movie_into outfile 'C:/ProgramData/MySQL/MySQL Server 5.7/Uploads/person_to_movie.csv'character set utf8fields terminated by ','optionally enclosed by '\"';","categories":[{"name":"命令行","slug":"命令行","permalink":"http://yoursite.com/categories/命令行/"},{"name":"mysql","slug":"命令行/mysql","permalink":"http://yoursite.com/categories/命令行/mysql/"}],"tags":[{"name":"命令行","slug":"命令行","permalink":"http://yoursite.com/tags/命令行/"}]},{"title":"KBQA的工作流程(基于检索的方法)","slug":"KBQA的工作流程(基于检索的方法)","date":"2018-06-22T13:00:43.000Z","updated":"2018-11-13T02:33:38.000Z","comments":true,"path":"2018/06/22/KBQA的工作流程(基于检索的方法)/","link":"","permalink":"http://yoursite.com/2018/06/22/KBQA的工作流程(基于检索的方法)/","excerpt":"基于知识图谱的问答系统，主要分为四个阶段，图谱构建阶段，语义解析阶段，问题分类阶段，问答阶段．其中语义解析难度最大，本文采用的基于信息检索的方法，还可以使用基于语义的方法，最佳实践方法就是：先使用基于语义的方法之后将基于检索的方法作为补充．","text":"基于知识图谱的问答系统，主要分为四个阶段，图谱构建阶段，语义解析阶段，问题分类阶段，问答阶段．其中语义解析难度最大，本文采用的基于信息检索的方法，还可以使用基于语义的方法，最佳实践方法就是：先使用基于语义的方法之后将基于检索的方法作为补充． 一．图谱构建阶段 1.1 数据准备 1.2数据导入 1.3关系构建 二.语义解析阶段 2.1分词器 2.2自定义词典 2.3构建问题模型 2.4构建词汇表 三.问题分类 3.1创建进程. 3.2设置训练集 3.3数据集格式转换 3.4测试集的生成 3.5问题模型匹配 四.查询结果 一．图谱构建阶段注:采用neo4j的方法进行建立图数据库 1.1 数据准备 结构化数据,例如mysql的关系型数据库,进行整理,导出为csv, 半结构化数据,采用包装器技术,进行抽取,之后,进行清洗. 文本数据,采用信息抽取技术,提取其中的关系 一般情况下,neo4j最好只存储结构化数据,否则会有很多高层散乱的节点,因为没有联系.关系型数据如下: 1.2数据导入将导出的csv文件,导入到neo4j之中.导出sql语句:1234567891011121314151617181920212223242526272829303132333435use movie; #CMD命令 查看MySql的导入与导出的目录【其他目录无权限】 # 使用mysql -u root -p 连接mysql # show variables like '%secure%' #MySql导出csv数据，带表头 #导出电影的类型 SELECT * INTO OUTFILE 'C:/ProgramData/MySQL/MySQL Server 5.7/Uploads/genre.csv' FIELDS TERMINATED BY ',' FROM (select 'gid','gname' union select*from genre) genre_; #导出电影的信息 == 如果太多可以只导出前500个，加限制 SELECT * INTO OUTFILE 'C:/ProgramData/MySQL/MySQL Server 5.7/Uploads/movie.csv' FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"' LINES TERMINATED BY '\\r' #电影描述中出现\\r换行字符， FROM (select 'mid','title','introduction','rating','releasedate' union select*from movie) movie_; #导出演员person的信息 == 如果有中文名要中文名，如果没有取英文名 SELECT * INTO OUTFILE 'C:/ProgramData/MySQL/MySQL Server 5.7/Uploads/person.csv' FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"' FROM (select 'pid','birth','death','name','biography','birthplace' union select person_id,person_birth_day,person_death_day,case when person_name then person_english_name else person_name end as name,person_biography,person_birth_place from person) person_; #导出电影ID和电影类别之间的对应 【1对1】 SELECT * INTO OUTFILE 'C:/ProgramData/MySQL/MySQL Server 5.7/Uploads/movie_to_genre.csv' FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"' FROM (select 'mid','gid' union select*from movie_to_genre) movie_to_genre_; #导出演员ID和电影ID之间的对应 【1对多】 SELECT * INTO OUTFILE 'C:/ProgramData/MySQL/MySQL Server 5.7/Uploads/person_to_movie.csv' FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"' FROM (select 'pid','mid' union select*from person_to_movie) person_to_movie_; 如图: 导入就会在neo4j当中看到: 1.3关系构建依据对应数据库的关系表,使用neo4j的cypher语句进行关系构建. 如图: 二.语义解析阶段2.1分词器 典型的有HANLP分词器,自然语言处理工具包,按步骤进行安装(语料库文件,配置文件),驱动文件(jar包)的话使用pom.xml依赖注入就可以,注意版本号.如图: 2.2自定义词典 在分词器的安装目录下,以HANLP工具包为例,在 D:\\HanLp\\data\\dictionary\\custom下,自定义词典,主要是导入原先数据表当中的专有数据,例如电影名,电影类型名,评分等,可以看成是对原先分词器词典的优化.便于正确分词.具体图如下: customDictionary本来就有的,然后根据子集选择进行添加,在genreDict.txt里面 都是些专有名词 2.3构建问题模型 在分词器的question目录下:具体地址为:D:\\HanLp\\data\\question,构建问题模型,在每个问题模型当中,其实就是构建不同的问法.然后确定输入语句的问题模型的时候,比对输入语句与各个问题相似性.如图(两张):具体某个问题模型里面的内容: 之后对问题模型进行索引,就是question_classification.txt如图: 2.4构建词汇表 构建词汇表的目的主要是为了:,经过分词器处理之后的输入语句,会分割成若干个词组,这些词组需要在词汇表中进行查找匹配,用来确定 表示输入语句的词向量中的元素的值.(存在为1,不存在为0).具体情况如图: 注:专有名词(电影名,电影类型,评分等)需要在自定义词典构建,不是在这里构建. 三.问题分类 可以使用spark框架的机器学习算法,来确定输入语句所对应的问题模型 3.1创建进程. 使用JavaSparkContext()来创建进程,通过配置进程来确定运行模式,便于后面的并行处理 12345/** * 本地模式，*表示启用多个线程并行计算 */ SparkConf conf = new SparkConf().setAppName(\"NaiveBayesTest\").setMaster(\"local[*]\"); JavaSparkContext sc = new JavaSparkContext(conf); 3.2设置训练集 主要是通过创建词向量及其标签,然后添加到列表当中,形成一个训练集 可以从外部文件导入(只不过需要分词器处理,匹配),也可以自己在程序中直接设定.,有稀疏向量/密集向量,用labelpoint()添加对应向量的标签.12345678910111213141516171819//稠密向量 == 连续的 Vector vMale = Vectors.dense(1,0,1,0,1,0); //稀疏向量 == 间隔的、指定的，未指定位置的向量值默认 = 0.0 int len = 6; int[] index = new int[]&#123;0,1,2,3,5&#125;; double[] values = new double[]&#123;1,1,1,1,1&#125;; //索引0、1、2、3、5位置上的向量值=1，索引4没给出，默认0 Vector vFemale = Vectors.sparse(len, index, values); //训练集生成 ，规定数据结构为LabeledPoint == 构建方式:稠密向量模式 ，1.0:类别编号 == 男性 LabeledPoint train_one = new LabeledPoint(1.0,vMale); //(1.0, 0.0, 1.0, 0.0, 1.0, 0.0） //训练集生成 ，规定数据结构为LabeledPoint == 构建方式:稀疏向量模式 ，2.0:类别编号 == 女性 LabeledPoint train_two = new LabeledPoint(2.0,vFemale); //(1.0, 1.0, 1.0, 1.0, 0.0, 1.0） //我们也可以给同一个类别增加多个训练集 LabeledPoint train_three = new LabeledPoint(2.0,Vectors.dense(0,1,1,1,0,1)); //List存放训练集【三个训练样本数据】 List&lt;LabeledPoint&gt; trains = new ArrayList&lt;&gt;(); trains.add(train_one); trains.add(train_two); trains.add(train_three); 3.3数据集格式转换 为了加快训练速度,spark需要使用RDD数据集,所以需要将LIst数据集转换成–&gt;RDD数据集,为了spark能够并行处理数据,具体方法如下: 123456JavaRDD&lt;LabeledPoint&gt; trainingRDD = sc.parallelize(trains); /** * 利用Spark进行数据分析时，数据一般要转化为RDD * JavaRDD转Spark的RDD */ NaiveBayesModel nb_model = NaiveBayes.train(trainingRDD.rdd()); 3.4测试集的生成 有两种方法: 直接设置词向量的测试集,然后直接使用训练好的模型进行训练(根据测试集的大小,决定是否也需要进行格式转换) 在外部文件当中设置多个输入语句,之后导入到程序之中,然后使用HANLP分词器(分词,匹配词汇表)转换成词向量,然后进行测试(扩展性强,适应性强) 3.5问题模型匹配 通过训练好的模型,输入对应的测试语句,然后算法模型进行该语句与问题模型相似度计算,选出相似度最大的,作为该语句所对应的问题模型. 以上是问的阶段,下面是答的阶段. 四.查询结果 根据所匹配的问题模型,然后在调用neo4j的接口,在neo4j当中查找问题结果,然后返回,具体过程如下: 输入语句:陈凯歌什么时候出生的,经过机器学习算法分类得到,问题模型的索引编号是13 在问题模型索引/分类表中,如下图: 可以看到是以词典的形式来进行匹配的,结果就是 nnt 出生日期,当中的两个属性: nnt是对应图数据库当中的person.name=陈凯歌,出生日期对应person.birth,为什么会这样对应? 因为在设置索引阶段的时候就通过词典设定不同问题模型对应的属性. 然后将nnt=陈凯歌,出生日期=birth,之后调用neo4j接口,进行具体查找,情况如下:","categories":[{"name":"知识图谱","slug":"知识图谱","permalink":"http://yoursite.com/categories/知识图谱/"},{"name":"知识点","slug":"知识图谱/知识点","permalink":"http://yoursite.com/categories/知识图谱/知识点/"}],"tags":[{"name":"工作流程","slug":"工作流程","permalink":"http://yoursite.com/tags/工作流程/"}]},{"title":"hadoop与spark的区别","slug":"hadoop与spark的区别","date":"2018-06-22T12:49:26.000Z","updated":"2018-11-13T02:22:40.000Z","comments":true,"path":"2018/06/22/hadoop与spark的区别/","link":"","permalink":"http://yoursite.com/2018/06/22/hadoop与spark的区别/","excerpt":"本文主要区分spark与hadoop的联系与区别,其中spark侧重于计算,主要体现在并行处理存储的数据,但前提是需要转换成特定的格式,可以多个请求,单次处理.hadoop侧重于分布式存储,但只能单个请求,单次处理,所以结合分布式存储与分布式计算/处理的性能是最佳的,因此spark常与hadoop结合.","text":"本文主要区分spark与hadoop的联系与区别,其中spark侧重于计算,主要体现在并行处理存储的数据,但前提是需要转换成特定的格式,可以多个请求,单次处理.hadoop侧重于分布式存储,但只能单个请求,单次处理,所以结合分布式存储与分布式计算/处理的性能是最佳的,因此spark常与hadoop结合. 1.hadoop是一个侧重于存储功能的分布式文件系统(HDFS)–是将数据存储到磁盘之中,当然同时也拥有数据处理功能–MapReduce(但是是只能分步处理数据) 2.spark是一个侧重于数据计算(处理)–可以进行批处理可以结合hadoop这类文件存储系统,但不是一定只能和hadoop,因为他们两者结合性能最佳. 3.hadoop中的MapReduce的数据处理速度由于是分步处理,每读取数据一次就只能处理一次,而不能像spark一样进行批量处理数据.所以一般使用spark. 4.在数据恢复方面:由于hadoop的HDFS要求数据存放在磁盘上,因此恢复较为顺畅,但是spark的数据对象(不是数据)一般可以存放在磁盘或者内存当中,但是安全性没有hadoop高,都可以进行数据恢复. 参考:https://blog.csdn.net/forward__/article/details/78770466","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"},{"name":"知识点","slug":"java/知识点","permalink":"http://yoursite.com/categories/java/知识点/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"fuseki的安装及其基本用法","slug":"fuseki的安装及其基本用法","date":"2018-06-22T12:15:44.000Z","updated":"2018-11-13T02:20:22.000Z","comments":true,"path":"2018/06/22/fuseki的安装及其基本用法/","link":"","permalink":"http://yoursite.com/2018/06/22/fuseki的安装及其基本用法/","excerpt":"fuseki就是一个sparql服务,主要是用来查询知识图谱的RDF三元组的,可以把它看成一个查询语言,可以作为一个查询与存储的服务器.","text":"fuseki就是一个sparql服务,主要是用来查询知识图谱的RDF三元组的,可以把它看成一个查询语言,可以作为一个查询与存储的服务器. 一.简介 二、 示例 2.1启动fuseki服务器 2.2浏览器中测试并兴建数据库 2.3上传数据到数据库之中 2.4进行在线测试查询 一.简介 Fuseki，通过HTTP提供RDF数据。Fuseki是一个SPARQL服务器。它通过HTTP提供使用SPARQL协议的REST式SPARQL HTTP更新，SPARQL查询和SPARQL更新。 详细内容请访问下面的网址http://jena.apache.org/documentation/fuseki2/index.html 关于安装和使用下面的一个示例来介绍。 二、 示例 构建一个YAGO的图数据库，并且提供endpoint访问接口。 2.1启动fuseki服务器 （1） 首先，我们在官网上下载Fuseki。最新版本是3.5的，我使用的是3.4版本的为例。下载zip，解压之后,找到相应的文件夹。 例如：D:\\APP\\apache-jena-fuseki-3.4.0。双击fuseki-server.bat脚本文件，出现如图所示的界面。 2.2浏览器中测试并兴建数据库（2）打开，浏览器，访问：http://localhost:3030 回车。出现如图所示的界面。 这样就搭建好了Fuseki，接下来，我们可以创建数据库了。点击addone 注意：这里第二步要选择persistent持久化，以确保数据能够永久保存，而in-memorary是关闭Fuseki之后数据就会消失的，可以当作小练习的时候选择，如果是做项目建议选择persistent。这样就创建好了一个数据库了。 2.3上传数据到数据库之中 (3) 点击uploaddata，可以上传数据。点击select files, 选择一个文件。比如yagoDataTest.ttl单击updata now. 文件上传成功！可以上传多个文件。文件格式可以是多个支持RDF文件的格式。上传到我们后台数据库中了。 再看看命令终端显示Fuseki的信息，也是成功了. 2.4进行在线测试查询 (4) 测试+查询: 点击query: 执行之后: 运行结果如图所示: 好了,开始你自己的查询吧!","categories":[{"name":"知识图谱","slug":"知识图谱","permalink":"http://yoursite.com/categories/知识图谱/"},{"name":"软件安装","slug":"知识图谱/软件安装","permalink":"http://yoursite.com/categories/知识图谱/软件安装/"}],"tags":[{"name":"fuseki","slug":"fuseki","permalink":"http://yoursite.com/tags/fuseki/"}]},{"title":"D2RQ 启动安装时出现的问题","slug":"D2RQ 启动安装时出现的问题","date":"2018-06-22T11:51:44.000Z","updated":"2018-11-13T02:19:22.000Z","comments":true,"path":"2018/06/22/D2RQ 启动安装时出现的问题/","link":"","permalink":"http://yoursite.com/2018/06/22/D2RQ 启动安装时出现的问题/","excerpt":"本文主要讲的是D2RQ的安装及其在安装中需要的注意事项,D2RQ全称可以这样理解:database to RDF数据集 query,就是将关系型数据库通过映射引擎转换成RDF数据,然后上传到D2RQ服务器上,之后然后就可以使用SPARQL语言(RDF数据的查询语言)进行关系的查询(就相当于一个客户端的请求,相对于D2RQ服务器来说),但是这种方法在数据量不是很大的情况下是有效的.因为需要的转换时候可以在承受范围之内.","text":"本文主要讲的是D2RQ的安装及其在安装中需要的注意事项,D2RQ全称可以这样理解:database to RDF数据集 query,就是将关系型数据库通过映射引擎转换成RDF数据,然后上传到D2RQ服务器上,之后然后就可以使用SPARQL语言(RDF数据的查询语言)进行关系的查询(就相当于一个客户端的请求,相对于D2RQ服务器来说),但是这种方法在数据量不是很大的情况下是有效的.因为需要的转换时候可以在承受范围之内. 一.D2RQ之Linked data： 1.1环境准备 二.启动D2R Server 一.D2RQ之Linked data：1.1环境准备 第一步:下载d2r-server-0.7.zip 解压到自己的工作目录下。地址（http://d2rq.org/ 这里是0.8.1了）–最好是最新版的 第二步:我是用的Mysql数据库，这里d2r支持很多数据库，解压包就包含了mysql和PostgreSQL的驱动，创建数据库,例如kg_demo_movie,里面就涉及到在navicat当中如何导入sql语句的问题:连接上mysql之后–&gt;双击对应的数据库,右键就会看到转存储sql文件–&gt;根据自己需求导入结构与或数据 第三步:之后运行脚本,在对应的文件目录当中呼出cmd,来通过命令行运行脚本:1generate-mapping -u root -p root -o kg_demo_movie_mapping.ttl jdbc:mysql:///kg_demo_movie 就相当于创建一个kg_demo_movie_mapping.ttl 映射文件(可以将SPARQL语句转换成SQL语句来查询 关系数据库当中的数据,而不用进行将关系数据库当中的数据转成RDF格式的数据,D2RQ 就是将SPARQL语句虚拟化成SQL语句)–因此只适用于数据比较少的情况下. 其中generate-mapping.bat是一个运行脚本 其中必须要创建一个对应的数据库,否则会报错. /// 代表的是本地localhost的当中的数据 连接成功就会出现如下截图: 1generate-mapping -u root -p root -o kg_demo_movie.nt jdbc:mysql:///kg_demo_movie 就相当于创建一个kg_demo_movie.nt的主谓宾文件 二.启动D2R Server 输入如下命令: 1d2r-server.bat kg_demo_movie_mapping.ttl 正常情况下就会得到: 否则就会出现如下情况: 这主要是因为:1No d2rq:Database defined in the mapping (E1) 数据库没有进行映射,也就是kg_demo_movie_mapping.ttl 为空,那也就是可能mysql没有连接/或者该数据库没有创建,要回滚到第一步linked data","categories":[{"name":"知识图谱","slug":"知识图谱","permalink":"http://yoursite.com/categories/知识图谱/"},{"name":"软件安装","slug":"知识图谱/软件安装","permalink":"http://yoursite.com/categories/知识图谱/软件安装/"}],"tags":[{"name":"D2RQ","slug":"D2RQ","permalink":"http://yoursite.com/tags/D2RQ/"}]},{"title":"重庆与成都的渊源","slug":"重庆历史","date":"2018-06-21T16:00:00.000Z","updated":"2018-11-13T03:16:02.000Z","comments":true,"path":"2018/06/22/重庆历史/","link":"","permalink":"http://yoursite.com/2018/06/22/重庆历史/","excerpt":"重庆在一个历史上非常神奇的存在。这片以巴文化著称的城市，早在200万年以前，重庆的巫山就出现了早期原始人类活动的痕迹，在后来的三峡库区考古中，在巫山，奉节，","text":"重庆在一个历史上非常神奇的存在。这片以巴文化著称的城市，早在200万年以前，重庆的巫山就出现了早期原始人类活动的痕迹，在后来的三峡库区考古中，在巫山，奉节，铜梁等地发现了大批新石器时代古人类文化遗址，在这片以产盐著称的土地上，著名的巴文化开始逐渐形成。 巴人是这片土地早期的主人，一个以尚武著称的民族，他们的活动区域遍布整个三峡地区，重庆地区是其主要的活动中心，形成了与成都地区的蜀文化截然不同的两种文化，虽然我们早已将巴蜀不分家，但是这实际上两种截然不同的文化。 柳叶剑，船棺，悬棺等都是巴文化的标志，盐业是巴人的经济支柱与骄傲，三峡地区凶险的水利环境铸就了巴人高超的造船和驾船技术，长期与强敌楚国的战争形成了巴人尚武的精神，长期的集体劳作让巴人成为了一个能歌善舞的民族。重庆地区是他们的家园，他们在这片土地留下了不可磨灭的巴文化，没有随着民族的消逝而流失在历史的长河中，成为他们给今天的重庆人留下的最宝贵的文化遗产。 但是，在之后非常长一段时间内，重庆扮演了配角了角色，成都是四川的骄傲，这里的蜀绣是中国四大名绣之一，“扬一益二”的荣光是成都的骄傲，天府之国的便利条件铸就了成都的繁华与安逸，中国最早的纸币在商业繁荣的成都产生，诗圣杜甫的草堂永远铭记在了成都的历史中，而此时的重庆，除了那段著名的白帝城托孤，似乎被历史所遗忘，直到十九世纪末，重庆的时代即将到来。 重庆的崛起，某种程度上像及了上海。重庆濒临长江，水路交通发达，在陆路交通极为不便的那个时代，这是个极大的优势，很快，随着帝国主义者侵略中国的加深，通商口岸逐渐从沿海来到内地沿江地区，重庆的朝天门码头变得忙碌起来。 英国人，法国人，德国人，日本人的兵舰商船纷至沓来，传教士，探险者，商人开始在重庆聚集起来，重庆展现了前所未有的新气象。通商口岸的开通使得重庆的商业日益繁荣，并且成为了四川对外交流的一个重要窗口，各种新思想，新的生产方式也从重庆进入四川，这里的风气日益活跃，学术思想新闻传播加快了步伐，把重庆乃至整个四川从一个封闭的内陆盆地变成了一个充满活力的新大陆。所有这一切，直接推动了民国十八年（1929）重庆建市，开启了重庆历史的新纪元。 真正让重庆走上国际舞台的乃是抗战八年的陪都经历。此时的重庆从一个新兴的通商口岸，四川的对外交流窗口，摇身一变，成为了全国的政治中心，成为抗战的总指挥部。 大批沿海地区的难民随国民政府涌入重庆及四川其他地区，这里成为了全国抗战的一个强有力的大后方。一批批工业设备来到这里，在极端艰苦的条件下坚持生产，为战场源源不断提供物质与武器，一批批学术大师和文化名人来到这里，给这里带来了中国当时最先进的文化。重庆，此时，不在是配角，成为了一个物质与人才的集散地，一个政治，经济，文化中心。 抗战八年的物质条件极端艰苦，日本飞机的轰炸一直伴随着这座陪都直到战争结束，可是这八年，重庆人文荟萃，文化交流繁荣，工业发展迅速，并成为了抗战精神的象征，让全世界都刮目相看，重庆，终于可以独当一面，也为日后的直辖市成立奠定了基础。 1949年年底，刘邓大军席卷大西南，重庆解放，西南军政委员会进驻重庆，重庆成为四川乃至整个西南地区的领袖。此时的重庆早已今非昔比，抗战时期大批工业厂矿的迁入，使重庆变成了一座工业基础良好的重工业城市，日后的三线建设更使重庆确立了这一地位，优越的地理位置，便捷的交通，繁荣的科学文化，大批高素质人才队伍的存在，使的重庆越来越独当一面，与成都一起成为四川发展的双引擎。 从一开始，重庆就注定了和四川难以割舍的种种联系，当年的巴人后代早已跟成都的蜀人一样，操着基本相同的口音，吃着相同口味的饮食。当年的巴蜀之争演变成了现在的分分合合，重庆与四川的缘分，一直在如此反复地上演着，虽然他们血脉中有着打不断的联系，但是一个西南地区的超级大都市，注定要与四川渐行渐远。 此时的四川已经是全国第一人口大省，占地面积也在全国前列，省会成都已经难以承担一个超级大省的全面管理，重庆独立设市的呼声越来越高。在97年之前，重庆作为四川经济最为发达的地区，为四川做出了巨大的贡献，大量的税收被用于支援四川其他地区的发展而不是发展本市经济，而重庆作为地级市，在资源政策向省会优先的情况下，发展受到了很大限制。 终于，随着三峡工程的上马，一个巨大的问题摆在世人眼前，如何管理三峡库区一百多万移民，省会成都由于距离问题远水难解近渴，而重庆却是距离库区最近的大城市，理所应当承担起库区管理的重任，此时的四川省已经不堪重负，三峡库区大批的贫困县也需要由其他大城市带动，摆脱贫困，终于，直辖的重庆市在1997年重新出现在世人眼前，这是一个有两千万人口的超级大都市，重庆从此告别了“川B”时代，进入了“渝”时代，开始了独立自主发展的道路，从此川渝分家，开始沿着各自的轨道发展下去，重庆作为四川一部分的历史暂时告一段落。 但是重庆跟四川的缘分并没有就此终结。在重庆，川字开头的汽车牌照随处可见，成渝铁路的列车依旧很繁忙，此时的重庆成为了西南地区经济发展的强力引擎，自主以后的重庆显得更有活力，经济发展的速度随着西部大开发战略的确立日益加快，川渝分家既存进了重庆的发展，也减轻了四川的负担，更为周围地区提供了更多机会，川渝的联系没有因为分家而中断，反而因为各自分工的不同以及商品交流的发达日益紧密，这是一个双赢的结果，一个国际大都市正在这里拔地而起。 相当长一段时间，外地人，特别是北方人都认为重庆跟成都没有太多区别。但是当他们踏上重庆这片土地时，发现自己想错了。重庆是一个跟成都完全不相同的城市，甚至是整个四川地区的一个异类，这也许也是重庆最终脱离四川的一个重要原因。 在成都，可以看到大量的麻将桌火锅店茶馆，这里的人不紧不慢，在麻将，火锅，茶馆当中安逸地过着自己的生活。时间在这里如此缓慢，习惯了快节奏生活的人来到这里仿佛进入了另一个世界。可是到了重庆，一切又是另一幅光景。所有缓慢的节奏到这里一下子突然来了个加速度，这里的节奏不亚于沿海的某些大城市大都市。方便的公交系统和快捷的轨道交通把忙碌人们不断送到他们的目的地，悠闲的生活氛围在这里没有了市场，清闲是整个四川的主旋律，在这里似乎完全跑调了。 曾经作为四川对外开放的一个重要窗口，重庆人更容易接触西方先进文明，四川包括西南地区甚至全国工业上无数个第一都是在这里诞生，这里也更早的进入城市文明，早早建立起一套完善的市政设施，与平静的四川内地相比，这里早在清朝末年就已经开始进入城市的喧嚣之中，与安静的成都相比，这个长江边上的城市注定是一个异类，就像巴文化和蜀文化一开始就是两种不同环境中形成的两种不同文化一样，虽然我们总喜欢把巴蜀文化放在一起。 最能体现重庆性格的地方莫过于朝天门码头。朝天门码头每天都充满着生气与活力，虎虎地朝向长江，无数的船只从这里出发，直通大海。这里有中国面积最大直辖市应有的气度，抗战时期成为了众多西迁人士从朝天门的码头下船，在这里接受庇护，告别了战火的惊扰。大量的外来人口为这座城市注入了活力，从各方面改造了这座城市。如今交通更加便利，朝天门码头已经不是进入重庆唯一的方式，但是这座码头见证了重庆的开埠与崛起，是重庆融入工业文明的象征，是外来人口进入西南内地的象征，是这座城市最好的代言人。 与这座城市的开放形成鲜明对比的，就是四川传统文化中著名的袍哥文化。清朝末年重庆的袍哥文化与成都一样发达，四川辛亥革命的成功很大长度上借助了会党的力量，会党势力之大，影响之深，分布之广，为国内所罕见，会党团结，讲义气，不怕牺牲，这些硬派的作风都深深影响着重庆人的性格。 由于地缘，物产，文化，风俗习惯的不同，重庆与成都在文化和思想观念都有外地人或是北方人察觉不出的不同之处。这些细微差别作用于两个城市之中，呈现出截然不同的两种城市风格。","categories":[{"name":"历史","slug":"历史","permalink":"http://yoursite.com/categories/历史/"},{"name":"重庆","slug":"历史/重庆","permalink":"http://yoursite.com/categories/历史/重庆/"}],"tags":[{"name":"人文","slug":"人文","permalink":"http://yoursite.com/tags/人文/"}]},{"title":"hexo博客构建的全过程","slug":"hexo博客建站全过程","date":"2018-06-21T16:00:00.000Z","updated":"2019-01-01T01:50:55.060Z","comments":true,"path":"2018/06/22/hexo博客建站全过程/","link":"","permalink":"http://yoursite.com/2018/06/22/hexo博客建站全过程/","excerpt":"本文主要是讲了hexo建站的全过程,包括软件安装,域名绑定,主题优化,异常问题这四个模块.","text":"本文主要是讲了hexo建站的全过程,包括软件安装,域名绑定,主题优化,异常问题这四个模块. 一.准备工作 1.1安装node.js,git 1.2配置git 1.2.1 配置用户名及其邮箱 1.2.2 生成密钥对(公钥与私钥) 1.2.3 添加公钥到远程仓库之中 1.2.4 进行测试 二.安装hexo 三.绑定域名 3.1购买,在腾讯云中绑定 3.2在github中绑定 3.3 本地域名绑定 四.优化hexo 4.1 修改hexo配置 4.2 修改icarus主题配置 4.3 增加阅读密码设置 4.4 修改字体设置 4.5 代码区样式编辑 4.6 优化页面布局 4.7 调整侧边栏字体 4.8 增加目录 五.常见问题 5.1 标签与分类异常 5.2 hexo目录解析 一.准备工作1.1安装node.js,git 首先安装的是node.js,直接从官网上下载即可; 之后git也是从网上下载直接安装就可以.1.2配置git1.2.1 配置用户名及其邮箱打开git bash 配置用户名及其邮箱:12git config --global user.name \"xiaopingzhong\"git config --global user.email \"zhongxiaoping@tju.edu.cn\" 1.2.2 生成密钥对(公钥与私钥)输入之前可以先测试下是否已经存在密钥对:12cd ~/.shhdir 如图:结果显示有known_host文件的话,说明已经配对成功. 继续之前的bash窗口,输入: 1ssh-keygen -t rsa -C \"zhongxiaoping@tju.edu.com\" 就会出现 123$ ssh-keygen -t rsa -C \"your_email@youremail.com\"Creates a new ssh key using the provided email # Generating public/private rsa key pair.Enter file in which to save the key (/home/you/.ssh/id_rsa): 直接按Enter就行。然后，会提示你输入密码，用来保存这对密钥的安全 1Enter same passphrase again: [Type passphrase again] 完了之后，大概是这样：12Your public key has been saved in /home/you/.ssh/id_rsa.pub.The key fingerprint is: # 01:0f:f4:3b:ca:85:d6:17:a1:7d:f0:68:9d:f0:a2:db your_email@youremail.com 到此为止，你本地的密钥对就生成了 + 公钥是访问/连接远程仓库数据的; + 私钥是用来当你上传数据到远程仓库时需要的密码 1.2.3 添加公钥到远程仓库之中 将公钥(.pub文件),输入到github的setting–&gt;ssh and GPG key ,截图如下: 打开C:\\Users\\Administrator.ssh上的id_rsa.pub，用记事本打开之后输入到： 最后点击生成就可以． 1.2.4 进行测试然后1ssh -T git@github.com 出现如下情况: 二.安装hexo123456789101112131415161718#1.安装hexo(client客户端):npm install hexo-cli -gnpm install hexo --save#2.初始化博客文件夹(名字叫做blog)hexo i blogcd blog #3.安装依赖包中所有的插件,NPM是随同NodeJS一起安装的包管理工具npm install #4.生成相应的静态文件(打包)hexo g (生成相应文件)#5.之后进行远程部署(先放到github仓库,之后在gitpage上运行)hexo deploy #6.部署服务后台常驻(一般都要先启动)npm install hexo-deployer-git --save#7.最终生成hexo服务器,后台常驻npm install hexo-server --save #8.启动hexo服务器hexo s 最后就会看到如下界面: 上述过程是比较保险的,一般不会出现什么异常情况 三.绑定域名3.1购买,在腾讯云中绑定 现在域名网站那里购买域名,可以在阿里云,腾讯云那里,都可以. 购买域名完成之后进入控制台: 点击云解析: 注意没有www 添加之后,点击右边的解析按钮: 进入页面之后,有两处需要注意: 其中: @对应你的仓库在github服务器上的ip地址具体方法:在cmd中输入ping xiaopingzhong.github.io 就会得到: www对应你仓库的名称:xiaopingzhong.github.io,并且将记录类型转换成CNAME,其实就是:域名映射, 注意:之后别忘了在,本地博客的根目录的source文件夹下创建一个CNAME文件(最好是UTF-8格式),输入你购买的域名即可 其实解析的目的就是实现(通过ping得到域名的ip地址,之后对ip的域名进行域名映射(xiaopingzhong.github.io–&gt;www.zhongxiaoping.cn)3.2在github中绑定进入github设置界面:点击仓库名,之后就会看到找到githubpage项:输入相应的域名,注意下面的两个红框:出现上述情况说明域名绑定成功3.3 本地域名绑定 修改_config.yml 打开本地博客的设置文件(俗称站点配置文件–_config.yml) 在最后的底部输入:1234deploy: type: git repository: https://github.com/xiaopingzhong/xiaopingzhong.github.io.git branch: master 最后进行测试:(hexo的部署服务之前已经安装过,所以现在就不用了)123456#1.先清除缓存hexo clean#2.进行博客的打包,生成静态文件,存在public文件中hexo g#3.将public上的文件上传到仓库之中,之后在githubpage中部署.(因为之前在githubpage中绑定了域名,因此)hexo d 如果出现下图情况，说明，部署成功14.在浏览器输入:www.zhongxiaoping.cn 得到网站: 这样就实现了一个网站的基本建设,接下来就是些美化的过程. 四.优化hexo4.1 修改hexo配置主要直接阅读_config.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128# Hexo Configuration## Docs: 说明文档https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/############################################# Site,站点设置title: iVEGAsubtitle: 一个小确幸的地方description: #description主要用于SEO,用来描述站点,提高排名keywords:author: 初遇常在,吾久深情language: zh-CNtimezone: #时区设置:Asia/Shanghai############################################## URL:网站各个部分地址(首页,文章所在的地址)## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'#网站的网址(可以叫首页)url: http://yoursite.com#网站根目录root: /#文章的永久链接格式,到时候看文章上面的地址就可以理解:例如:https://www.zhongxiaoping.cn/2018/06/22/%E9%87%8D%E5%BA%86%E5%8E%86%E5%8F%B2/permalink: :year/:month/:day/:title/ permalink_defaults:############################################## Directory :对应的目录设置#资源目录source_dir: source#静态文件目录public_dir: public#标签目录tag_dir: tags#归档/历史目录archive_dir: archives#分类目录category_dir: categories#代码目录code_dir: downloads/code#国际化文件夹(适应多种语言)i18n_dir: :langskip_render:############################################## Writing,文章的书写设置#文件名new_post_name: :title.md # File name of new posts#输入hexo new post \"文件名\",当中post可以省略,预设文章的front-matter为post布局default_layout: post#首字母大写,文章中titlecase: false # Transform title into titlecase#新建标签页打开链接external_link: true # Open external links in new tab#文件名转换成大写,针对英文filename_case: 0#显示草稿(渲染草稿)render_drafts: false#启动assert文件夹(方便资源插入)/是否启动资源文件夹post_asset_folder: false#资源(包括文章)的链接可以改为相对位置relative_link: true#未来的文章可以显示future: true#代码设置highlight: enable: true line_number: true #说明脚本会自动检测语言代码高亮 auto_detect: false #也就是使用tab两次,就会得到代码块,默认是true tab_replace:############################################ Home page setting 主页设置# path: Root path for your blogs index page. (default = '')# per_page: Posts displayed per page. (0 = disable pagination)# order_by: Posts order. (Order by date descending by default)#主页设置index_generator:#首页文件的路径,例如:https://www.zhongxiaoping.cn/就是首页了,后面么有后缀 path: '' per_page: 10 order_by: -date # Category &amp; Tag#默认分类default_category: uncategorized#分类别名category_map:#标签别名tag_map:########################################## Date / Time format,时间格式设定,默认采用 Moment.js脚本得到## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DDtime_format: HH:mm:ss######################################### Pagination 分页设置## Set per_page to 0 to disable pagination#每页显示的文章量 (0 = 关闭分页功能)per_page: 10#按页形成一个所有文章的目录pagination_dir: page######################################### Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: icarus########################################### Deployment,部署设置## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repository: https://github.com/xiaopingzhong/xiaopingzhong.github.io.git branch: master ############################################ # Security后期人为添加上去的.##encrypt: enable: true 4.2 修改icarus主题配置 首先是需要对主题进行克隆：1git clone https://github.com/ppoffice/hexo-theme-icarus.git themes/icarus 之后将主题配置文件:_config.yml.example去掉.example即可. 然后,详细修改请看对应的主题配置文件:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123# Menus:首页菜单设置,可以进行人为添加,之后只不过需要在hexo的source文件下新建一个对应文件夹及其文件menu: \"首页\": . \"归档\": archives \"分类\": categories \"标签\": tags \"关于\": about# Customize 自定义设置customize:#网站左边的logo logo: enabled: true width: 40 height: 40 url: images/logo.png #(profile--简介) profile: enabled: true # Whether to show profile bar fixed: true avatar: css/images/avatar.png gravatar: # Gravatar email address, if you enable Gravatar, your avatar config will be overriden author: iVEGA author_title: 一个小确幸的地方 location: 天津大学, 中国 #关注 follow: https://github.com/xiaopingzhong/ highlight: atelier-forest-light # sidebar position, options: left, right or leave it empty(放在右边) sidebar: left # enable posts thumbnail, options: true, false #缩略图功能,每个帖子(很重要,在front-matter设置thumbnail: 图片地址就可以,文章封面设置设置:banner: 图片地址) thumbnail: true # path to favicon,网站缩略图(就是标签页上的图片) favicon: css/images/favicon.png #社交图标(注意格式)--图标名: 实际链接 social_links: github: https://github.com/xiaopingzhong/ weibo: https://weibo.com/u/5124024867 linux: https://blog.csdn.net/fbsxghvudk angellist: http://mail.tju.edu.cn #rss: / #提示图标的文字,#enable the social link tooltip, options: true, false #鼠标移到图标的时候是否有提示 social_link_tooltip: false # Widgets:部件(一侧的展示栏)widgets: - recent_posts - category - archive - tag - tagcloud - links# Searchsearch:# you need to install `hexo-generator-json-content` before using Insight Search(本地搜索需要先安装hexo-generator-json-content,记住)#insight内部 insight: true # enter swiftype install key here swiftype: false baidu: false # you need to disable other search engines to use Baidu search, options: true, false# Comment:评论comment: disqus: # enter disqus shortname here duoshuo: # enter duoshuo shortname here youyan: # enter youyan uid here facebook: # enter true to enable isso: # enter the domain name of your own comment isso server eg. comments.example.com changyan: # please fill in `appid` and `conf` to enable appid: conf: gitment: owner: #your github ID repo: #the repo to store comments #Register an OAuth application, and you will get a client ID and a client secret. client_id: #your client ID client_secret: #your client secret livere: # enter livere uid here #主要使用valine,on的值为true,并且输入对应的id就可以(需重启) valine: # Valine Comment System https://github.com/xCss/Valine # enter true to enable on: false # enter the leancloud application appId here appId: kwmGvMjgOuvYv7WY9Ybwr9Ld-gzGzoHsz # enter the leancloud application appKey here appKey: cpItMh602gazPzBhHy0i5yvt # enter true to enable &lt;Mail notifier&gt; https://github.com/xCss/Valine/wiki/Valine-%E8%AF%84%E8%AE%BA%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E9%82%AE%E4%BB%B6%E6%8F%90%E9%86%92%E8%AE%BE%E7%BD%AE notify: true # enter true to enable &lt;Validation code&gt; verify: false # enter the comment box placeholder placeholder: \"输入问题,咱们共同交流,一起进步\" # Share:设置为None就不能分享share: None # options: jiathis, bdshare, addtoany, default# Pluginsplugins:# options: true, false:可以进入图片模式 lightgallery: true # options: true, false:合理画廊,帖子更好看 justifiedgallery: true google_analytics: # enter the tracking ID for your Google Analytics google_site_verification: # enter Google site verification code baidu_analytics: # enter Baidu Analytics hash key # options: true, false mathjax: true # Miscellaneous,杂项miscellaneous: open_graph: # see http://ogp.me fb_app_id: fb_admins: twitter_id: google_plus: links: \"百度学术\": http://xueshu.baidu.com/ \"小众软件\": https://www.appinn.com/category/windows/ 4.3 增加阅读密码设置 先安装:Hexo-Blog-Encrypt,输入: 1npm install Hexo-Blog-Encrypt 之后在站点配置文件中添加: 123#Securityencrypt: enable: true 最后需要在文章的front-matter处加上:1password: 设定的密码 #有空格的单词最好加上双引号 4.4 修改字体设置icarus主题的字体设置有些特殊: 1.增加字体 现在主题的\\themes\\icarus\\source\\libs中加上对应的字体(最好建文件夹,加上字体与字体样式),如下图:css样式为:123456789@font-face &#123;#'方正宋刻本秀楷简体'是字体名(word的字体设置中可以看到) font-family: '方正水云简体_准'; #相对路径指明字体路径(主要是以上两项) src: url('../fonts/FZShuiYJW_Zhun.TTF'); #字体是否需要加粗 font-weight: normal; font-style: normal;&#125; ２.将字体样式添加到css样式表中 修改:\\themes\\icarus\\layout\\common\\head.ejs 找到如下位置,进行添加: 123456789&lt;%- css('libs/shui-yun/styles') %&gt;&lt;%- css('libs/deng-xian/styles') %&gt;&lt;%- css('libs/Microsoft-JhengHei/styles') %&gt;#上面三项是人为添加的,文件夹最好不用空格&lt;%- css('libs/font-awesome5/css/fontawesome.min') %&gt;&lt;%- css('libs/font-awesome5/css/fa-brands.min') %&gt;&lt;%- css('libs/font-awesome5/css/fa-solid.min') %&gt;&lt;%- css('libs/open-sans/styles') %&gt;&lt;%- css('libs/source-code-pro/styles') %&gt; 3.最后进行字体的引入,设置.在\\themes\\icarus\\source\\css_variables.styl中找到改位置,进行修改就可以,(字体之间的设置,注意空格)123456789// Fontsfont-sans = \"方正水云简体_准\", \"open sans\", \"Helvetica Neue\", \"Microsoft Yahei\", Helvetica, Arial, sans-seriffont-serif = \"方正水云简体_准\", Georgia, \"Times New Roman\", \"Microsoft Yahei\", seriffont-mono = Droid Sans Mono, \"Source Code Pro\", Monaco, Menlo, Consolas, monospacefont-size = 17px#行间距line-height = 2.2em#标题间/标题与正文的行间距line-height-title = 1em 大功告成,最后重新清除缓存输入:123hexo cleanhexo ghexo s #最好先本地测试,之后再部署 得到结果: 4.5 代码区样式编辑代码显示区是在pre标签当中(原文件没有),为此咱们在E:\\Hexo\\ivega\\themes\\icarus\\source\\css\\style.styl下增加:12345678910pre color: #e96900 padding: 0px 0px //大小 font-size: 0.8em //行间距 line-height: 2.0em font-family: Droid Sans Mono //色差用屏幕取色器解决(shareX截屏有) background-color: #3f3f3f 保存后,刷新即可见效,效果如下: 4.6 优化页面布局在E:\\Hexo\\ivega\\themes\\icarus\\source\\css\\_variables.styl下修改:123456789101112// Layoutblock-margin = 40pxarticle-padding = 20pxmobile-nav-width = 280px//原先是7,3,3,4//main-column对应中间正文部分main-column = 8//sidebar-column对应侧边栏部分sidebar-column = 2.6//profile-column--对应个人简介部分profile-column = 2.6sidebar-column-tablet = 4 保存后,刷新即可见效,结果为: 4.7 调整侧边栏字体侧边栏字体,分类目录字体过大在”E:\\Hexo\\ivega\\themes\\icarus\\source\\css_partial\\sidebar.styl”之中修改:12345.category-list-child padding: 10px 0 //由font-size改为16px font-size: 16px border-bottom: 1px solid border-color 保存后,刷新即可见效 4.8 增加目录hexo博客增加目录代码为:npm install hexo-toc --save站点配置文件_config.yml:123456789toc: maxdepth: 3 class: toc slugify: transliteration decodeEntities: false anchor: position: after symbol: '#' style: header-anchor 使用在Markdown文章中加入TOC的占位符. 五.常见问题5.1 标签与分类异常 标签异常的话,本人的搭建的是icarus主题,情况比较特殊,有需要的可以参考下:如果出现 1can't get tags 这种类似情况,直接将icarus主题下的_source子文件夹的三个文件夹复制到hexo的source进行覆盖,之后清除缓存重启,一般都能解决. 5.2 hexo目录解析123456789101112131415|-- _config.yml #对应站点配置文件|-- package.json #对应全局依赖组件,类似java的pom#scaffolds---骨架的含义,模版,主要有三个,post(正式文章),page(new page的时候会生成一个文件夹及其相同的名字的文件,而不是一个文件),#draft草稿是不显示在博客当中,但是可以使用publish让其变为正式文章|-- scaffolds #资源文件,_post主要是放文章的|-- source |-- _posts#主题文件|-- themes#.gitignore”文件，这个文件的作用就是告诉GIT哪些文件不需要添加到版本管理中，比如Android项目中的iml文件及build目录下面编译生成的文件|-- .gitignore#就是hexo的版本信息|-- package.json","categories":[{"name":"博客建站","slug":"博客建站","permalink":"http://yoursite.com/categories/博客建站/"}],"tags":[{"name":"博客","slug":"博客","permalink":"http://yoursite.com/tags/博客/"},{"name":"icarus主题","slug":"icarus主题","permalink":"http://yoursite.com/tags/icarus主题/"},{"name":"域名","slug":"域名","permalink":"http://yoursite.com/tags/域名/"}]}]}