<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2018%2F06%2F17%2F%E8%AF%AD%E4%B9%89%E7%BD%91%E6%8A%80%E6%9C%AF%E6%A0%88%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[语义网技术栈的理解先上图 一.各层概念:阐述之前,先说下资源的概念: ​ 可以是任意拥有URI(这是最重要的)的对象,人,物体,概念,文档,文件,音频.等等 ​ URI:统一资源标识符 1.1 URI与UNICODEUNICODE:规定了资源的编码形式; URI规定了每个资源的唯一标识符. 1.2XML​ XML规定了资源的内容及其数据结构(XML tree,标签来描述),最重要的是没有语义的描述能力.(就是不知道资源之间的关系是怎样的). ​ 同时XML的标签可以人为定义,这就导致不能很好地被计算机处理,为了规范化标签,增强可处理性,所以推出了xml schema来规定所拥有的标签,形式为xs:schema&lt;/xs:schema&gt; ​ 但是由于标签的数量减少了,就很容易带来重复,这就引入了了命名空间的概念,xmlns (namespace–ns),在不同的命名空间下,同名的标签不受影响.形式为:xmlns:前缀=地址.具体代码如下: 1234567891011&lt;?xml version="1.0"?&gt;&lt;xs:schema xmlns:xs="http://www.w3.org/2001/XMLSchema"&gt; &lt;xs:element name="note"&gt; &lt;xs:complexType&gt; &lt;xs:sequence&gt; &lt;xs:element name="to" type="xs:string"/&gt; &lt;xs:element name="from" type="xs:string"/&gt; &lt;xs:element name="body" type="xs:string"/&gt; &lt;/xs:sequence&gt; &lt;/xs:complexType&gt;&lt;/xs:element&gt;&lt;/xs:schema&gt; 1.3 RDFRDF: 主要是用来描述资源之间的关系.采用SPO三元组 形式. 123subject predict object#subject 与object是节点,节点可以是资源或描述资源的属性值(不是属性),predict是关系:只要能够描述两个节点的关系都可以称之为关系/属性#(资源，属性，属性值/资源) ​ RDF资源描述框架,是一种图模型,数据模型,不是具体的数据类型.主要有三部分构成: RDF data model​ 就是单纯描述资源的方式:SPO三元组形式,但是没有语义信息 RDF schema​ 在RDF data model上增加了语义信息,就是描述资源的关系,方式就是增加资源的属性及其资源所属的类的描述.具体方法如下: 概念 语法形式 描述 Class(类) C rdf:type rdfs:Class C(资源)是一个RDF类 Property(类) P rdf:type rdf:Property P(资源)是一个RDF属性 type(属性) I rdf:type C I(资源)是C(类)的实例 subClassOf(属性) C1 rdfs:subClassOf C2 C1 (类)是C2(类)的子类 subPropertyOf (属性) P1 rdfs:subPropertyOf P2 P1(属性)是P2(属性)的子属性 domain (属性) P rdfs:domain C P(属性)的定义域是C(类) range (属性) P rdfs:range C P(属性)的值域是C(类) RDF sytanx 就是讲如何将RDF数据存入到计算机当中,也就是我们所说的RDF序列化方法,这就是RDF sytanx的作用.主要有如下集中方式: JSON-LD:“JSON for Linking Data”，用键值对的方式来存储RDF数据,基于json基础扩展的一种语法,以键值对的形式来存储RDF RDFA:将RDF数据嵌入到网页中 RDF/XML:用XML来存储RDF数据,但是由于XML格式冗长,阅读不方便不推荐 N-Triples，即用多个三元组来表示RDF数据集，是最直观的表示方法。在文件中，每一行表示一个三元组，方便机器解析和处理。开放领域知识图谱DBpedia通常是用这种格式来发布数据的。 Turtle, 应该是使用得最多的一种RDF序列化方式了。它比RDF/XML紧凑，且可读性比N-Triples好. turtle语法形式如下: 1234567891011121314@prefix person: &lt;http://www.kg.com/person/&gt; .@prefix place: &lt;http://www.kg.com/place/&gt; .@prefix : &lt;http://www.kg.com/ontology/&gt; .person:1 :chineseName "罗纳尔多·路易斯·纳萨里奥·德·利马"^^string.person:1 :career "足球运动员"^^string.person:1 :fullName "Ronaldo Luís Nazário de Lima"^^string.person:1 :birthDate "1976-09-18"^^date.person:1 :height "180"^^int. person:1 :weight "98"^^int.person:1 :nationality "巴西"^^string. person:1 :hasBirthPlace place:10086.place:10086 :address "里约热内卢"^^string.place:10086 :coordinate "-22.908333, -43.196389"^^string. 1.4.RDF schema 与SPARQL RDF schema:主要还是为了增强资源关系的描述能力(上面讲了), SPARQL是一个查询图,RDF是一个存储图,是查询RDF数据的语言. 1.5 OWL ,RIF/SWRL​ OWL:在RDF schema的基础上,增加了对属性及其类的约束,也就是对资源的属性及其类增加了性质,这样能实现两个功能:分类,推理 实现对概念的分类,因为根据属性及类的约束,容易确定概念的边界 有利于RIF(规则交换)–实现推理,因为某些属性含有某些性质,例如:传递性. 1. 6 Logic + Proof + Trust分别对应逻辑层,验证层,信任层 逻辑层:提供公理及其规则,实现逻辑操作. 验证层:则是对推理结果进行验证 信任层:根据验证层的结果和一些数字签名,建立信任 1.7 interaction​ 就是语义网的交互层,通过实现各种应用来进行交互 1.8 参考https://blog.csdn.net/hohaizx/article/details/80043623 https://zhuanlan.zhihu.com/p/32122644 http://blog.sina.com.cn/s/blog_541caaee0100jjmc.html]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F06%2F17%2Fhello-world%2F</url>
    <content type="text"><![CDATA[KBQA的工作流程(基于检索的方法)图谱构建阶段注:采用neo4j的方法进行建立图数据库 1.数据准备 结构化数据,例如mysql的关系型数据库,进行整理,导出为csv, 半结构化数据,采用包装器技术,进行抽取,之后,进行清洗. 文本数据,采用信息抽取技术,提取其中的关系 &lt;!-more-&gt; 一般情况下,neo4j最好只存储结构化数据,否则会有很多高层散乱的节点,因为没有联系. 关系型数据如下: 2.数据导入将导出的csv文件,导入到neo4j之中. 导出sql语句: 1234567891011121314151617181920212223242526272829303132333435use movie; #CMD命令 查看MySql的导入与导出的目录【其他目录无权限】 # 使用mysql -u root -p 连接mysql # show variables like &apos;%secure%&apos; #MySql导出csv数据，带表头 #导出电影的类型 SELECT * INTO OUTFILE &apos;C:/ProgramData/MySQL/MySQL Server 5.7/Uploads/genre.csv&apos; FIELDS TERMINATED BY &apos;,&apos; FROM (select &apos;gid&apos;,&apos;gname&apos; union select*from genre) genre_; #导出电影的信息 == 如果太多可以只导出前500个，加限制 SELECT * INTO OUTFILE &apos;C:/ProgramData/MySQL/MySQL Server 5.7/Uploads/movie.csv&apos; FIELDS TERMINATED BY &apos;,&apos; OPTIONALLY ENCLOSED BY &apos;&quot;&apos; LINES TERMINATED BY &apos;\r&apos; #电影描述中出现\r换行字符， FROM (select &apos;mid&apos;,&apos;title&apos;,&apos;introduction&apos;,&apos;rating&apos;,&apos;releasedate&apos; union select*from movie) movie_; #导出演员person的信息 == 如果有中文名要中文名，如果没有取英文名 SELECT * INTO OUTFILE &apos;C:/ProgramData/MySQL/MySQL Server 5.7/Uploads/person.csv&apos; FIELDS TERMINATED BY &apos;,&apos; OPTIONALLY ENCLOSED BY &apos;&quot;&apos; FROM (select &apos;pid&apos;,&apos;birth&apos;,&apos;death&apos;,&apos;name&apos;,&apos;biography&apos;,&apos;birthplace&apos; union select person_id,person_birth_day,person_death_day,case when person_name then person_english_name else person_name end as name,person_biography,person_birth_place from person) person_; #导出电影ID和电影类别之间的对应 【1对1】 SELECT * INTO OUTFILE &apos;C:/ProgramData/MySQL/MySQL Server 5.7/Uploads/movie_to_genre.csv&apos; FIELDS TERMINATED BY &apos;,&apos; OPTIONALLY ENCLOSED BY &apos;&quot;&apos; FROM (select &apos;mid&apos;,&apos;gid&apos; union select*from movie_to_genre) movie_to_genre_; #导出演员ID和电影ID之间的对应 【1对多】 SELECT * INTO OUTFILE &apos;C:/ProgramData/MySQL/MySQL Server 5.7/Uploads/person_to_movie.csv&apos; FIELDS TERMINATED BY &apos;,&apos; OPTIONALLY ENCLOSED BY &apos;&quot;&apos; FROM (select &apos;pid&apos;,&apos;mid&apos; union select*from person_to_movie) person_to_movie_; 如图: 导入就会在neo4j当中看到: 3.关系构建依据对应数据库的关系表,使用neo4j的cypher语句进行关系构建. 如图: 语义解析阶段1.分词器​ 典型的有HANLP分词器,自然语言处理工具包,按步骤进行安装(语料库文件,配置文件),驱动文件(jar包)的话使用pom.xml依赖注入就可以,注意版本号. 如图: 2.自定义词典​ 在分词器的安装目录下,以HANLP工具包为例,在 D:\HanLp\data\dictionary\custom下,自定义词典,主要是导入原先数据表当中的专有数据,例如电影名,电影类型名,评分等,可以看成是对原先分词器词典的优化.便于正确分词.具体图如下: customDictionary本来就有的,然后根据子集选择进行添加,在genreDict.txt里面 都是些专有名词 3.构建问题模型在分词器的question目录下:具体地址为:D:\HanLp\data\question,构建问题模型,在每个问题模型当中,其实就是构建不同的问法.然后确定输入语句的问题模型的时候,比对输入语句与各个问题相似性.如图(两张): 具体某个问题模型里面的内容: 之后对问题模型进行索引,就是question_classification.txt如图: 4.构建词汇表​ 构建词汇表的目的主要是为了:,经过分词器处理之后的输入语句,会分割成若干个词组,这些词组需要在词汇表中进行查找匹配,用来确定 表示输入语句的词向量中的元素的值.(存在为1,不存在为0).具体情况如图: 注:专有名词(电影名,电影类型,评分等)需要在自定义词典构建,不是在这里构建. 问题分类可以使用spark框架的机器学习算法,来确定输入语句所对应的问题模型 1.创建进程.使用JavaSparkContext()来创建进程,通过配置进程来确定运行模式,便于后面的并行处理 12345/** * 本地模式，*表示启用多个线程并行计算 */ SparkConf conf = new SparkConf().setAppName("NaiveBayesTest").setMaster("local[*]"); JavaSparkContext sc = new JavaSparkContext(conf); 2.设置训练集主要是通过创建词向量及其标签,然后添加到列表当中,形成一个训练集 可以从外部文件导入(只不过需要分词器处理,匹配),也可以自己在程序中直接设定.,有稀疏向量/密集向量,用labelpoint()添加对应向量的标签. 12345678910111213141516171819//稠密向量 == 连续的 Vector vMale = Vectors.dense(1,0,1,0,1,0); //稀疏向量 == 间隔的、指定的，未指定位置的向量值默认 = 0.0 int len = 6; int[] index = new int[]&#123;0,1,2,3,5&#125;; double[] values = new double[]&#123;1,1,1,1,1&#125;; //索引0、1、2、3、5位置上的向量值=1，索引4没给出，默认0 Vector vFemale = Vectors.sparse(len, index, values); //训练集生成 ，规定数据结构为LabeledPoint == 构建方式:稠密向量模式 ，1.0:类别编号 == 男性 LabeledPoint train_one = new LabeledPoint(1.0,vMale); //(1.0, 0.0, 1.0, 0.0, 1.0, 0.0） //训练集生成 ，规定数据结构为LabeledPoint == 构建方式:稀疏向量模式 ，2.0:类别编号 == 女性 LabeledPoint train_two = new LabeledPoint(2.0,vFemale); //(1.0, 1.0, 1.0, 1.0, 0.0, 1.0） //我们也可以给同一个类别增加多个训练集 LabeledPoint train_three = new LabeledPoint(2.0,Vectors.dense(0,1,1,1,0,1)); //List存放训练集【三个训练样本数据】 List&lt;LabeledPoint&gt; trains = new ArrayList&lt;&gt;(); trains.add(train_one); trains.add(train_two); trains.add(train_three); 3.数据集格式转换为了加快训练速度,spark需要使用RDD数据集,所以需要将LIst数据集转换成–&gt;RDD数据集,为了spark能够并行处理数据,具体方法如下: 123456JavaRDD&lt;LabeledPoint&gt; trainingRDD = sc.parallelize(trains); /** * 利用Spark进行数据分析时，数据一般要转化为RDD * JavaRDD转Spark的RDD */ NaiveBayesModel nb_model = NaiveBayes.train(trainingRDD.rdd()); 4.测试集的生成 有两种方法: 直接设置词向量的测试集,然后直接使用训练好的模型进行训练(根据测试集的大小,决定是否也需要进行格式转换) 在外部文件当中设置多个输入语句,之后导入到程序之中,然后使用HANLP分词器(分词,匹配词汇表)转换成词向量,然后进行测试(扩展性强,适应性强) 5.问题模型匹配 通过训练好的模型,输入对应的测试语句,然后算法模型进行该语句与问题模型相似度计算,选出相似度最大的,作为该语句所对应的问题模型. 以上是问的阶段,下面是答的阶段. 查询结果根据所匹配的问题模型,然后在调用neo4j的接口,在neo4j当中查找问题结果,然后返回,具体过程如下: 输入语句:陈凯歌什么时候出生的,经过机器学习算法分类得到,问题模型的索引编号是13 在问题模型索引/分类表中,如下图: 可以看到是以词典的形式来进行匹配的,结果就是 nnt 出生日期,当中的两个属性: nnt是对应图数据库当中的person.name=陈凯歌,出生日期对应person.birth,为什么会这样对应? ​ 因为在设置索引阶段的时候就通过词典设定不同问题模型对应的属性. 然后将nnt=陈凯歌,出生日期=birth,之后调用neo4j接口,进行具体查找,情况如下:]]></content>
  </entry>
</search>
